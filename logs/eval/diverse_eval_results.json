[
  {
    "question": "CM3 모델의 Medium과 Large 버전은 각각 몇 개의 파라미터를 가지고 있나요?",
    "answer": "CM3-Medium 모델은 2.7B개의 파라미터를 가지고 있으며, CM3-Large 모델은 13B개의 파라미터를 가지고 있습니다. 이 정보는 [자료 1]과 [자료 4]에서 확인할 수 있습니다.",
    "context": "### [자료 1] (P4)\n3.3 TRAINING We train 4 models; 125M, 800M, 2.7B, and 13B parameters. The purpose of the two smaller models was to establish basic hyper-parameters that are viable for the causally masked language modeling objective and therefore were under-trained. However, all downstream tasks will be evaluated with our 2.7B model (CM3-Medium) and our 13B model (CM3-Large).\n\n### [자료 2] (P10)\nIn-domain Out-of-domain **Model Type** **Method** **AIDA** **MSNBC** **AQUAINT** **ACE2004** **CWEB** **WIKI*** **Avg.** Ganea & Hofmann (2017) 92.2 93.7 88.5 88.5 77.9 77.5 86.4 Guo & Barbosa (2018) 89 92 87 88 77 84.5 86.2 Yang et al. (2018) **95.9** 92.6 89.9 88.5 **81.8** 79.2 88.0 Shahbazi et al. (2019) 93.5 92.3 90.1 88.7 78.4 79.8 87.1 Yang et al. (2019) 93.7 93.8 88.2 90.1 75.6 78.8 86.7 Le & Titov (2019) 89.6 92.2 90.7 88.1 78.2 81.7 86.8 Fang et al. (2019) 94.3 92.8 87.5 91.2 78.5 82.8 87.9 **De Cao et al. (2020)** 93.3 94.3 89.9 90.1 77.3 **87.4** 88.8 _Direct Supervision_    CM3-Medium 93.5 94.2 90.1 90.4 76.5 86.9 88.6 _Direct Supervision_ _{_ CM3-Large 94.8 **94.8** **91.1** **91.4** 78.4 **88.7** **89.8** CM3-Medium 78.0 80.1 75.4 81.4 68.5 76.2 76.6 _Self Supervision (0-Shot)_ _{_ CM3-Large 80.1 80.8 77.7 82.8 72.4 80.2 79.0 Table 4: Aligned with GENRE’s evaluation, we use Micro _F_ 1 (InKB) for the named entity disambiguation task.\n\n### [자료 3] (P19)\n_[arXiv:1904.01310](https://arxiv.org/abs/1904.01310)_, 2019. 18 A APPENDIX A.1 MODEL ARCHITECTURE For model architecture we use the same exact architecture for CM3-Medium and CM3-Large as the dense 2.7B and 13B models described in Artetxe et al. (2021). CM3-Large CM3-Medium –decoder-embed-dim 5120 2560 –decoder-output-dim 5120 2560 –decoder-input-dim 5120 2560 –decoder-ffn-embed-dim 20480 10240 –decoder-layers 40 32 –decoder-normalize-before True True –decoder-attention-heads 40 32 –share-decoder-input-output-embed True True –decoder-learned-pos False False Table 10: FairSeq architecture designation for CM3 models A.2 UNIFORMITY OF VQVAE-GAN TOKENS We plot a histogram of all image tokens in a subset of our data spanning 100k tokens.\n\n### [자료 4] (P2)\nWe train a 2.7 billion and 13 billion causally masked model on this data which we call CM3-Medium and CM3-Large respectively. Extensive experiments demonstrate that these models are able to perform a wide range of zero-shot uni- and cross-modal tasks. We show both qualitatively and quantitatively that CM3 can be prompted for non-trivial image generation, similar to that of DALL-E. We also show that CM3 models are capable of improving over state-of-the-art zero-shot summarization, entity linking, entity disambiguation, highlighting the structure that comes from the hypertext during training. Finally, we show that by fine-tuning CM3 we set the new state-of-the-art for entity linking and entity disambiguation in general.\n\n### [자료 5] (P11)\n(2011) 72.8 65.1 32.6 55.4 46.4 **42.4** **63.1** 0.0 47.2 Steinmetz & Sack (2013) 42.3 30.9 26.5 46.8 18.1 20.5 46.2 46.4 34.7 Moro et al. (2014) 48.5 39.7 29.8 55.9 23.0 29.1 41.9 37.7 38.2 Kolitsas et al. (2018) 82.4 72.4 34.1 35.2 **50.3** 38.2 61.9 52.7 53.4 Broscheit (2020) 79.3 - - - - - - Martins et al. (2019) 81.9 - - - - - - van Hulst et al. (2020) _[†]_ 80.5 72.4 41.1 50.7 49.9 35.0 **63.1** **58.3** 56.4 De Cao et al. (2020) **83.7** 73.7 **54.1** 60.7 46.7 40.3 56.1 50.0 **58.2** _Direct Supervision_    CM3-Medium 71.4 68.5 48.6 58.3 44.9 41.1 61.9 37.7 54.1 _Direct Supervision_ _{_ CM3-Large 79.9 **74.8** 53.2 **62.4** 47.1 **42.8** 61.9 52.7 **59.3** CM3-Medium 20.4 18.6 20.1 35.1 30.6 32.1 36.6 0.0 24.2 _Self Supervision (0-Shot)_ _{_ CM3-Large 24.8 21.4 25.6 39.0 31.1 34.9 37.1 0.0 26.7 Table 5: We report Micro _F_ 1 on our test sets for our entity linking task.\n\n### [자료 6] (P13)\nLevel VisualBert ViLBert CM3-Medium CM3-Large C6: M/W, Career/Family S 1.05 1.14 0.00 0.98 W 0.54 0.51 0.10 0.12 C8: Science/Arts, M/W S 0.86 1.05 -0.09 0.42 W 0.62 0.14 0.08 0.07 C11: M/W, Pleasant/Unpleasant S -0.74 -0.84 0.00 -0.64 W -0.66 -0.31 -0.20 -0.48 Double Bind: M/W, Competent S -0.10 -0.04 0.01 -0.01 W -0.23 0.30 -0.07 -0.27 Double Bind: M/W, Likeable S -0.11 -1.12 -0.24 -0.59 W -0.60 0.09 0.00 0.10 Occupations: M/W, Occupation S 0.98 1.82 0.03 0.62 W 0.91 1.80 0.00 0.58 Total Significant Bias Count - 5 6 0 2 Table 8: Following Ross et al.\n[...]\n(2021) we present the results for all gender bias classes on answering the question: “Do joint embeddings contain biases”? The numbers in our table represent effect sizes, and are underlined if their respective p-values are below 0.05. Each bias type and model are tested three times against Word embeddings (W) and Sentence embeddings (S). We present our empirical results for gender and race bias in Table 8 and Table 9 respectively. Overall, both CM3 have significantly less bias than other competing models, most likely due to our choice to use only Wikipedia and CC-NEWS articles as training sources (and recent CC-NEWS articles at that). Furthermore, we believe the fact that CM3-Medium shows no to very little signs of bias can be an indicator of under-fitting as the large model is, unfortunately, able to show some bias from our training data.\n\n### [자료 7] (P8)\nResults were selected by CLIP from a candidate set of 32 samples. Model FID Zero-shot FID AttnGAN (Xu et al., 2017) 35.49 DM-GAN (Zhu et al., 2019) 32.64 DF-GAN (Tao et al., 2020) 21.42 DM-GAN + CL (Ye et al., 2021) 20.79 XMC-GAN (Zhang et al., 2021) 9.33 LAFITE (Zhou et al., 2021) **8.12** DALL-E (Ramesh et al., 2021) _∼_ 28 LAFITE (Zhou et al., 2021) 26.94 GLIDE (Nichol et al., 2021) **12.24** Unconditional CM3-Medium 40.65 Unconditional CM3-Medium 36.51 Conditional CM3-Medium 36.78 Conditional CM3-Large 29.56 Table 2: We compare FID on MS-COCO 256 _×_ 256. Following Nichol et al."
  },
  {
    "question": "Causally Masked Language Modeling이 양방향 맥락(Bidirectional context)을 제공하는 원리가 무엇인가요?",
    "answer": "Causally Masked Language Modeling이 양방향 맥락(Bidirectional context)을 제공하는 원리는, 일반적인 순차적 언어 모델과 마스킹 언어 모델의 특성을 결합하여 구현된 것입니다. 일반적인 순차적 언어 모델은 토큰을 왼쪽에서 오른쪽으로 생성하며, 이때 맥락은 과거 토큰에만 기반합니다. 반면, 마스킹 언어 모델은 일부 토큰을 미리 마스킹하고, 그 후 전체 토큰 흐름을 기반으로 생성하여 미래 토큰에 대한 정보를 활용합니다.\n\nCausally Masked Language Modeling은 이 두 방식을 혼합하여, 토큰을 왼쪽에서 오른쪽으로 생성하면서도, 특정한 긴 토큰 스피언(장수 토큰 범위)을 마스킹한 후, 그 마스킹된 부분을 문장의 끝에서 생성하는 방식을 사용합니다. 이 과정에서 마스킹된 부분은 생성 시점에 기존 토큰 흐름과 함께, 미래 토큰의 정보를 기반으로 생성되므로, 생성 과정에서 과거와 미래의 맥락을 동시에 활용할 수 있습니다. 이로 인해 모델은 생성 중에 양방향 맥락을 갖게 되며, 이는 텍스트와 이미지의 통합 표현에서 더 자연스럽고 의미 있는 출력을 가능하게 합니다.\n\n즉, Causally Masked Language Modeling은 순차적 생성과 마스킹 생성의 조합을 통해, 마스킹된 토큰이 생성될 때 그 주변 맥락(과거와 미래)을 모두 활용할 수 있게 하여, 양방향 맥락을 제공합니다. 이는 모델이 텍스트와 이미지의 상호작용을 더 잘 이해하고, 다양한 모달리티 간의 연결을 자연스럽게 학습할 수 있게 합니다.",
    "context": "### [자료 1] (P2)\nTo summarize, our contributions include: - We present the first hyper-text language-image model, trained on close to a Terabyte of multi-modal simplified HTML data from the common crawl. - We present the causally masked objective, a hybrid of causal and masked language models that allows for bidirectional context control during generative mask infilling. - We demonstrate consistently strong transfer from CM3 to a range of uni-modal and multimodal tasks at differing supervision levels, including stating state-of-the-art on entity disambiguation and zero-shot summarization.\n\n### [자료 2] (P1)\n(2021) and images Ramesh et al. (2021). Recent work has also shown how to use document structure, e.g., as provided by HTML web markup, to enable more effective zero-shot prompting for text-only tasks (Aghajanyan et al., 2021). In this paper, we show it is possible to learn multi-modal document-structured generative models, to jointly represent formatted hypertext and images as they naturally co-occur within full document contexts. We introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents. Causally masked models generate tokens left to right, just like a causal language model, but also mask out a small number of long token spans, which are then generated at the end of the string instead of their original positions. This provides a new hybrid of causal and masked language models, enabling full generative modeling with bidirectional context.\n\n### [자료 3] (P14)\nFor example, the conditional and unconditional image generation capabilities of our model are most similar in approach to DALL-E, which trains a left-to-right causal model over the concatenation of textual tokens and VQ-VAE visual tokens (Ramesh et al., 2021). At the same time, the use of autoregressive modeling in entity linking and disambiguation was proposed by the GENRE in De Cao et al. (2020). The method of tokenizing non-discrete modalities to use standard sequence modeling approaches have been extensively explored with DALL-E for images, Jukebox for Music (Dhariwal et al., 2020) and vq-wav2vec for Speech (Baevski et al., 2019). 8 CONCLUSION In this paper, we present the CM3 model, a causally masked trained language model that is capable of non-trivial zero-shot performance on a wide range of zero-shot uni- and cross-modal tasks. We first describe a new sequence modeling objective we call causally masked, enabling both full generative modeling with bidirectional context. Through extensive experimentation, we show that as a single model CM3 can be prompted to recover the functionality of many other models being able to do image generation, image captioning, unconditional image generation, and more.\n\n### [자료 4] (P1)\n## CM3: A CAUSAL MASKED MULTIMODAL MODEL OF ### THE INTERNET **Armen Aghajanyan, Bernie Huang** _[∗]_ **, Candace Ross** _[∗]_ **, Vlad Karpukhin** _[∗]_ **, Hu Xu** _[∗]_ **, Naman Goyal,** **Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer** Facebook AI Research _{_ armenag,berniehuang,ccross,vladk,huxu,naman _}_ @fb.com _{_ oxo,mandarj,gghosh,mikelewis,lsz _}_ @fb.com ABSTRACT We introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked languageimage models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multimodal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM (Ramesh et al., 2021; De Cao et al., 2020; Aghajanyan et al., 2021). We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model. 1 INTRODUCTION Recent advancements in large-scale generative sequence modeling have significantly improved zeroshot performance on multiple modalities, including text Brown et al. (2020); Fabbri et al. (2020); Aghajanyan et al.\n\n### [자료 5] (P15)\nAnalysis of named entity recognition and linking for tweets. _Information Processing & Management_, 51(2):32–49, 2015. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding.\n\n### [자료 6] (P6)\n4.1.2 IMAGE IN-FILLING Unlike DALL-E, which leverages left-to-right language modeling objective to model languageimage tokens, CM3 with the proposed causally masked language modeling makes it possible to condition contiguous sections of an image on the surrounding context for image in-filling. Specifically, CM3 can infill images with the following prompt: **Infilling Prompt:** <img src=\" _{_ prefix _}_ <mask:0> _{_ postfix _}_ \"><mask:0> Using the same decoding strategies described in § 4.1.1 we generate unconditional infilled images with only CM3-Large and present qualitative results in Figure 4. Overall we see that CM3-Large is capable of generating semantically coherent infills even without grounding in text. 4.2 TEXT-IMAGE MODALITY 4.2.1 CONDITIONAL IMAGE IN-FILLING Additionally, CM3 can further perform image in-filling condition on the additional text context. This can be achieved by slightly augmenting the prompt as follows: **Conditional Infilling Prompt:** <img alt=\"Photo: _{_ text _}_ \" src=\" _{_ prefix _}_ <mask:0> _{_ postfix _}_ \"><mask:0> We show qualitative results in Figure 4. Immediately we notice the substantial improvement in the generated image when grounded in ground truth text vs. unconditional image-infilling.\n\n### [자료 7] (P13)\n7 RELATED WORK Fundamentally our work is an extension of the HTLM work proposed by Aghajanyan et al. (2021) to using the newly proposed causally masked objective, integrating images through VQVAE-GAN tokens, and scaling up over an order of magnitude.\n\n### [자료 8] (P13)\nOverall we see that both CM3 models are competitive against T5 given the same parameter setting. Furthermore, aligning with the results from Aghajanyan et al. 12 (2021) we see that placing the natural language utterances of the various GLUE tasks into an HTML prompt while fine-tuning non-trivially improves end-finetuning performance. The following experiments show that the causally masked language modeling approach is not detrimental to learning fine-tunable representations, and neither is jointly modeling image tokens. 6 ETHICAL CONSIDERATIONS Prior work has explored the extent to which language models encode harmful gender and racial biases that parallel humans through the Word Embedding Association Test (WEAT) (Caliskan et al., 2017), the Sentence Encoder Association Test (SEAT) (May et al., 2019) and the GroundedWEAT/Grounded-SEAT (Ross et al., 2021) metrics for multimodal language models (Tan & Celis, 2019). Given the generative nature of CM3 in both the language and visual modalities, we used GWEAT/GSEAT to probe our model."
  },
  {
    "question": "CM3 모델이 제로샷(Zero-shot)으로 수행할 수 있는 작업들을 모두 나열해 주세요.",
    "answer": "CM3 모델이 제로샷(Zero-shot)으로 수행할 수 있는 작업들은 다음과 같습니다:\n\n1. **이미지 설명 (Image Captioning)**  \n   - 이미지에 대한 설명을 생성하는 작업. 예를 들어, \"그룹의 사람들이 해변 위에서 육상 스포츠를 하고 있다\"와 같은 문장을 생성할 수 있음.\n\n2. **이름 기반 엔티 해석 (Named Entity Disambiguation)**  \n   - 텍스트 내에서 명명된 엔티(예: '파리', '애플')의 의미를 명확히 구분하여 해석하는 작업. 이는 위키백과의 HTML 데이터에서 자연스럽게 학습된 기반으로 강력한 제로샷 성능을 보임.\n\n3. **엔티 링크링 (Entity Linking)**  \n   - 텍스트 내의 엔티를 특정 지식 베이스(예: 위키백과)와 연결하여 정확한 항목으로 매핑하는 작업. CM3는 이 작업에서 최신 기준(SOTA)을 달성함.\n\n4. **제로샷 요약 (Zero-shot Summarization)**  \n   - 주어진 텍스트를 기반으로 요약을 생성하는 작업. 이는 CM3가 학습한 표현을 통해 제로샷으로 수행 가능함.\n\n5. **이미지 인퍼닝 (Image Infilling)**  \n   - 이미지 내 특정 부분을 보완하는 작업. 예를 들어, 이미지에 있는 벤치나 가구를 보완하는 작업. 이 경우, 텍스트 조건을 사용하거나 사용하지 않는 두 방식(무조건, 텍스트 조건)으로 수행 가능.\n\n이러한 작업들은 모두 CM3 모델이 텍스트 프롬프트를 통해 제로샷으로 수행할 수 있으며, 특히 이미지 기반 작업과 텍스트 기반 작업 모두에서 강력한 성능을 보임.",
    "context": "### [자료 1] (P9)\nWe present our quantitative result in Table 3. CM3-Large is capable of achieving reasonable zero-shot captioning performance on the MS-COCO dataset. Precision Recall F1 CM3-Caption-Beam 0.781 0.789 0.785 CM3-Caption-CLIP 0.863 0.866 0.864 Table 3: BERTScore numbers for zero-shot captioning with CM3.\n\n### [자료 2] (P10)\nWe aim to find a prompt capable of representing the more general end-to-end entity linking task in the CM3 model. From there, a proper sequence scoring of the candidate set will provide us with an approach to zero-shot entity disambiguation.\n\n### [자료 3] (P14)\nEmpirically we improve over state-of-the-art zero-shot summarization, entity linking, entity disambiguation, highlighting the structure from the hypertext during training. We show that representations learned by CM3 are not only useful for zero-shot prompting but for fine-tuning by fine-tuning CM3 and state-of-the-art for entity linking and entity disambiguation in general, all while staying highly competitive with T5 models on the GLUE benchmark.\n\n### [자료 4] (P10)\nWe present our results in Figure 4. Given the strong supervision naturally available in Wikipedia HTML, it is unsurprising that CM3 shows strong, non-trivial zero-shot performance on the named entity disambiguation across a wide array of named entity disambiguation tasks. [3https://github.com/facebookresearch/GENRE](https://github.com/facebookresearch/GENRE) 10 Furthermore, the fine-tuned HTLM-Large model outperforms previous entity linking specific models to achieve a new SOTA over the benchmarked datasets. 4.3.2 ENTITY LINKING We next consider the more general entity linking task.\n\n### [자료 5] (P7)\nthe wooden park benches are painted dark purple. some bread is on a plate with jam, an apple, yogurt and orange juice. a nice looking hotel room with a neatly done bed, coffee table, and a chair. Source Image Masked/Tokenized Image CM3-Infilling-U CM3-Infilling-C Ground Truth Figure 4: We provide qualitative samples for zero-shot image-infilling using the CM3-Large model using the aforementioned prompts. CM3-Infilling-U refers to infilling without conditioning on text while CM3-Infilling-C refers to conditioning on the ground truth text.\n\n### [자료 6] (P5)\n4 ZERO/FEW-SHOT PROMPTING 4.1 IMAGE MODALITY Although we do not train on pure image documents, CM3 can still operate over image tasks. To do so, we must cleverly describe the task through a textual prompt, using the <img> tag.\n\n### [자료 7] (P6)\nIn general CM3 is capable of generating semantically coherent images on-par with modern GANs. Furthermore, our conditional CM3Large model approaches the performance of the DALL-E model while using an order of magnitude fewer data. 4.2.3 CAPTIONING We next look at the dual-task to conditional image generation and image captioning. We can prompt CM3 to do zero-shot image captioning by asking the model to generate either the alt or title 6 group of people windsurfing over the beach and water in the ocean.\n\n### [자료 8] (P2)\nTo summarize, our contributions include: - We present the first hyper-text language-image model, trained on close to a Terabyte of multi-modal simplified HTML data from the common crawl. - We present the causally masked objective, a hybrid of causal and masked language models that allows for bidirectional context control during generative mask infilling. - We demonstrate consistently strong transfer from CM3 to a range of uni-modal and multimodal tasks at differing supervision levels, including stating state-of-the-art on entity disambiguation and zero-shot summarization."
  },
  {
    "question": "Entity Linking 작업에서 CM3 모델은 기존 SOTA와 비교해 어떤 성과를 냈나요?",
    "answer": "CM3 모델은 기존 SOTA(최고 성능)와 비교해 엔티 링크링 작업에서 우수한 성과를 달성했습니다. 특히, CM3이 훈련 과정에서 하이퍼텍스트 구조를 활용하면서 학습한 표현은 단순한 제로샷 프롬프팅 외에도, 엔티 링크링과 엔티 다이스아미게이션 작업에 대해 페인트를 통해 최고 성능을 달성했습니다. 또한, CM3 모델은 T5 모델과 GLUE 벤치마크에서 매우 경쟁력 있는 성능을 보이며, 엔티 링크링 및 엔티 다이스아미게이션 분야에서 기존 모델들을 초월하는 성과를 기록했습니다.",
    "context": "### [자료 1] (P14)\nEmpirically we improve over state-of-the-art zero-shot summarization, entity linking, entity disambiguation, highlighting the structure from the hypertext during training. We show that representations learned by CM3 are not only useful for zero-shot prompting but for fine-tuning by fine-tuning CM3 and state-of-the-art for entity linking and entity disambiguation in general, all while staying highly competitive with T5 models on the GLUE benchmark.\n\n### [자료 2] (P10)\nWe present our results in Figure 4. Given the strong supervision naturally available in Wikipedia HTML, it is unsurprising that CM3 shows strong, non-trivial zero-shot performance on the named entity disambiguation across a wide array of named entity disambiguation tasks. [3https://github.com/facebookresearch/GENRE](https://github.com/facebookresearch/GENRE) 10 Furthermore, the fine-tuned HTLM-Large model outperforms previous entity linking specific models to achieve a new SOTA over the benchmarked datasets. 4.3.2 ENTITY LINKING We next consider the more general entity linking task.\n\n### [자료 3] (P18)\n_[arXiv:1711.10485](https://arxiv.org/abs/1711.10485)_, 2017. Xiyuan Yang, Xiaotao Gu, Sheng Lin, Siliang Tang, Yueting Zhuang, Fei Wu, Zhigang Chen, Guoping Hu, and Xiang Ren. Learning dynamic context augmentation for global entity linking.\n\n### [자료 4] (P18)\n17 Johannes M van Hulst, Faegheh Hasibi, Koen Dercksen, Krisztian Balog, and Arjen P de Vries. Rel: An entity linker standing on the shoulders of giants. In _Proceedings of the 43rd International ACM_ _SIGIR Conference on Research and Development in Information Retrieval_, pp. 2197–2200, 2020. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\n\n### [자료 5] (P16)\nEnd-to-end neural entity linking. _arXiv preprint arXiv:1808.07699_, 2018. Phong Le and Ivan Titov. Improving entity linking by modeling latent relations between mentions. _arXiv preprint arXiv:1804.10637_, 2018. Phong Le and Ivan Titov.\n\n### [자료 6] (P10)\nWe aim to find a prompt capable of representing the more general end-to-end entity linking task in the CM3 model. From there, a proper sequence scoring of the candidate set will provide us with an approach to zero-shot entity disambiguation.\n\n### [자료 7] (P15)\nJoint entity linking with deep reinforcement learning. In _The World Wide Web Conference_, pp. 438–447, 2019. Octavian-Eugen Ganea and Thomas Hofmann. Deep joint entity disambiguation with local neural attention."
  },
  {
    "question": "CM3가 이미지 인필링(Image In-filling)을 수행할 때 사용하는 프롬프트 구조는 어떤 식인가요?",
    "answer": "CM3가 이미지 인필링을 수행할 때 사용하는 프롬프트 구조는 다음과 같습니다.\n\n**1. 무조건적 이미지 인필링 프롬프트 (Unconditional Image In-filling Prompt):**  \n이 프롬프트는 텍스트 조건을 적용하지 않고, 이미지 내 특정 영역을 채우는 데 사용됩니다.  \n구조는 다음과 같습니다:  \n`<img src=\" _{_ prefix _}_ <mask:0> _{_ postfix _}_ \"><mask:0>`  \n\n여기서 `<mask:0>`는 인필링이 이루어질 영역을 나타내며, CM3 모델이 그 영역을 주변 맥락에 기반하여 채우는 방식으로 작동합니다.\n\n**2. 조건적 이미지 인필링 프롬프트 (Conditional Image In-filling Prompt):**  \n이 프롬프트는 추가적인 텍스트 컨텍스트를 기반으로 이미지 인필링을 수행합니다.  \n구조는 다음과 같습니다:  \n`<img alt=\"Photo: _{_ text _}_ \" src=\" _{_ prefix _}_ <mask:0> _{_ postfix _}_ \"><mask:0>`  \n\n이 경우, `alt` 속성에 텍스트 정보가 포함되어 있어, 모델이 그 텍스트를 기반으로 이미지의 특정 영역을 더 자연스럽고 의미 있게 채우게 됩니다.\n\n이 두 프롬프트는 CM3가 이미지 내 연속된 영역을 주변 맥락에 기반하여 채우는 방식을 가능하게 하며, 특히 조건적 인필링은 텍스트 정보를 기반으로 한 결과에서 더 의미 있는, 문맥적으로 일관된 이미지를 생성할 수 있습니다.",
    "context": "### [자료 1] (P7)\nthe wooden park benches are painted dark purple. some bread is on a plate with jam, an apple, yogurt and orange juice. a nice looking hotel room with a neatly done bed, coffee table, and a chair. Source Image Masked/Tokenized Image CM3-Infilling-U CM3-Infilling-C Ground Truth Figure 4: We provide qualitative samples for zero-shot image-infilling using the CM3-Large model using the aforementioned prompts. CM3-Infilling-U refers to infilling without conditioning on text while CM3-Infilling-C refers to conditioning on the ground truth text.\n[...]\nattributes of a properly set <img> tag. Due to attributes always appearing in alphabetical order in order to generate alt attribute (which appears before src), we need to use the masking capabilities of CM3. **Captioning Masked Prompt #1:** <img alt=\"Photo: A photo taken of<mask:0>\" src=\" _{_ image _}_ \"> **Captioning Causal Prompt #1:** <img src=\" _{_ image _}_ \" title=\"Photo: A photo taken of We have two methods of generating captions given the above prompts.\n[...]\nFirst, the relatively inexpensive method involves running beam-search with a beam size of 5 over the proposed prompts. For a single image, we run both available prompts and select the sequence, which minimizes the respective CM3 perplexity. The second method is much more costly and requires sampling 128 captions for every image (we note that this is cheaper than image generation since image generation requires the minimal generation of 256 image tokens while captioning is usually on the order of a dozen tokens).\n\n### [자료 2] (P6)\n4.1.2 IMAGE IN-FILLING Unlike DALL-E, which leverages left-to-right language modeling objective to model languageimage tokens, CM3 with the proposed causally masked language modeling makes it possible to condition contiguous sections of an image on the surrounding context for image in-filling. Specifically, CM3 can infill images with the following prompt: **Infilling Prompt:** <img src=\" _{_ prefix _}_ <mask:0> _{_ postfix _}_ \"><mask:0> Using the same decoding strategies described in § 4.1.1 we generate unconditional infilled images with only CM3-Large and present qualitative results in Figure 4. Overall we see that CM3-Large is capable of generating semantically coherent infills even without grounding in text. 4.2 TEXT-IMAGE MODALITY 4.2.1 CONDITIONAL IMAGE IN-FILLING Additionally, CM3 can further perform image in-filling condition on the additional text context. This can be achieved by slightly augmenting the prompt as follows: **Conditional Infilling Prompt:** <img alt=\"Photo: _{_ text _}_ \" src=\" _{_ prefix _}_ <mask:0> _{_ postfix _}_ \"><mask:0> We show qualitative results in Figure 4. Immediately we notice the substantial improvement in the generated image when grounded in ground truth text vs. unconditional image-infilling.\n[...]\n4.2.2 CONDITIONAL IMAGE GENERATION We can do conditional text generation using CM3 similar to DALL-E by using a proper prompt. Specifically by conditioning using the alt attribute of the img tag. **Conditional Generation Prompt:** <img alt=\" _{_ prompt _}_ We present qualitative conditional image generation results in Figure 5. Specifically, we sample 32 images for every prompt given and re-rank using CLIP to get the top-4 images (Radford et al., 2021). Overall we see that CM3 can generate recognizable images of the input text.\n[...]\nThere are still failure cases, such as the second image in the second prompt, where the model easily generates a landscape but forgets to generate the red car. The third prompt, CM3, is incapable of drawing the face of a sheep while getting the general body and texture correct.\n\n### [자료 3] (P5)\n4 ZERO/FEW-SHOT PROMPTING 4.1 IMAGE MODALITY Although we do not train on pure image documents, CM3 can still operate over image tasks. To do so, we must cleverly describe the task through a textual prompt, using the <img> tag.\n\n### [자료 4] (P5)\nWe present a sample of non-cherry picked examples in Figure 3. <img (a) A mountain of olive trees on the way to Cabo de la Vela (b) Spain Europa Amenacer Winter (c) blog TIGI Bed Head Tie Dye Spray Hair Spray Hairspray ml (d) birthday invitation printable christmas gift for birthday party Printable Template <img src=\" Figure 3: Four samples for two of the prompts we proposed for unconditional image generation for CM3-Large."
  }
]