--- PAGE 1 ---
CM3: A CAUSAL MASKED MULTIMODAL MODEL OF THE INTERNET Armen Aghajanyan, Bernie Huang∗, Candace Ross∗, Vlad Karpukhin∗, Hu Xu∗, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer Facebook AI Research {armenag,berniehuang,ccross,vladk,huxu,naman}@fb.com {oxo,mandarj,gghosh,mikelewis,lsz}@fb.com ABSTRACT We introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking ob- ject provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language- image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi- modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM (Ramesh et al., 2021; De Cao et al., 2020; Aghajanyan et al., 2021). We set the new state-of-the-art in zero-shot summa- rization, entity linking, and entity disambiguation while maintaining competitive performance in the ﬁne-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model. 1 INTRODUCTION arXiv:2201.07520v1 [cs.CL] 19 Jan 2022 Recent advancements in large-scale generative sequence modeling have signiﬁcantly improved zero- shot performance on multiple modalities, including text Brown et al. (2020); Fabbri et al. (2020); Aghajanyan et al. (2021) and images Ramesh et al. (2021). Recent work has also shown how to use document structure, e.g., as provided by HTML web markup, to enable more effective zero-shot prompting for text-only tasks (Aghajanyan et al., 2021). In this paper, we show it is possible to learn multi-modal document-structured generative models, to jointly represent formatted hypertext and images as they naturally co-occur within full document contexts. We introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents. Causally masked models generate tokens left to right, just like a causal language model, but also mask out a small number of long token spans, which are then generated at the end of the string instead of their original positions. This provides a new hybrid of causal and masked language models, enabling full generative modeling with bidirectional context. For example, it can also be used in our setting to inﬁll complete images or larger structured text sections, conditioned on the rest of the document. We train CM3 models on close to a terabyte of web-based data following Aghajanyan et al. (2021), extended to include images through VQVAE-GAN tokens (Esser et al., 2021) and additional hyper- text link structure. This data is in strong contrast to previous methods that were either uni-modal or ∗Equal Contribution for Second Author 1

--- PAGE 2 ---
carefully curated the image-text alignment (e.g., for image captioning Radford et al. (2021); Ramesh et al. (2021)). We train a 2.7 billion and 13 billion causally masked model on this data which we call CM3-Medium and CM3-Large respectively. Extensive experiments demonstrate that these models are able to perform a wide range of zero-shot uni- and cross-modal tasks. We show both qualitatively and quantitatively that CM3 can be prompted for non-trivial image generation, similar to that of DALL-E. We also show that CM3 models are ca- pable of improving over state-of-the-art zero-shot summarization, entity linking, entity disambigua- tion, highlighting the structure that comes from the hypertext during training. Finally, we show that by ﬁne-tuning CM3 we set the new state-of-the-art for entity linking and entity disambiguation in general. To summarize, our contributions include: • We present the ﬁrst hyper-text language-image model, trained on close to a Terabyte of multi-modal simpliﬁed HTML data from the common crawl. • We present the causally masked objective, a hybrid of causal and masked language models that allows for bidirectional context control during generative mask inﬁlling. • We demonstrate consistently strong transfer from CM3 to a range of uni-modal and multi- modal tasks at differing supervision levels, including stating state-of-the-art on entity dis- ambiguation and zero-shot summarization. • We release all code and models to support future CM3 research. 2 CAUSALLY MASKED OBJECTIVE Traditional approaches to pre-training have focused on mixing the architectural choices (i.e., encoder-only, decoder-only, encoder-decoder) with objective choices (i.e., masking, causal language modeling). For example, masked encoder-only models such as BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) excel in non-generative ﬁne-tuning tasks. Masked encoder-decoder models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2019) excel in both discriminative and generative ﬁne-tuning. Brown et al. (2020) on the other hand, showed that causal language mod- els (de-facto, decoder only) are capable of non-trivial performance without the need of ﬁne-tuning by simply prompting with appropriate string to control the generated outputs Radford et al. (2019); Brown et al. (2020); Artetxe et al. (2021). There are pros and cons to both masked and causal language modeling in the context of prompting. Masking offers the critical ability to encode bi-directionality within the prompts at the cost of only decoding roughly 15% of the tokens of the input sequence during training (Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019). Conversely, decoder-only causal language models decode every token in the input sequence in the training but are typically limited to left-only contexts. Empirically, more work has also been done on scaling causal decoder-only rather than their counterparts. In an effort to get most of the best of both worlds, we introduce a novel objective that combines the beneﬁt of per-token generation with optional bi-directionality speciﬁcally tailored to prompting. For a document of size s we select n ∼Clamp(Poisson(1), 1, 16) masks and for each of those masks we select span m ∼(Uniform(0, s), Uniform(0, s)) which does not intersect with any other m. These values are chosen to, on average, select relatively few relatively long spans, which we expect will allow the model to learn to inﬁll long spans. We then order these masks by the order that they appear in the source document, replace the span of the mask in the source document with an enumerated mask token (i.e., <mask:0>, <mask:1>), and move the masked spans to the end of the document followed by a unique end of document token. Figure 1 shows the complete process. We also augment the standard cross-entropy loss to weigh the loss of predicting mask tokens as 0, as they are placed at random locations which carry no information to the underlying sequence modeling objective. The complete array of beneﬁts will become more apparent when designing prompts for uni/cross- modal tasks in § 4. However, at the core, the causally masked objective can do causal language modeling while optionally allowing for bidirectionality when needed. 2

--- PAGE 3 ---
Causally Monte Melkonian was left-wing a nationalist militant . <mask:0> <mask:0> <a href= Armenian _nationalism > Masked Language Model <a href= Armenian _nationalism > Monte Melkonian was left-wing a nationalist militant . Masked Language <mask> Model Monte Melkonian was <a left-wing a href= Armenian _nationalism > nationalist militant . Language Model Figure 1: A visual representation of various language modeling objectives as well as our proposed causal language modeling objective with a single mask (n = 1). Given the left-to-right nature of causal language models (bottom row) we would not be able to generate the Wikipedia entity link highlighted in orange. 3 CM3 Aghajanyan et al. (2021) used structured documents for text-only pre-training with strong zero-shot performance. Causally-Masked Multimodal Modeling (CM3) extends this work by modeling full document structure including images and hypertext links. Furthermore, we move away from the BART-like objective of Aghajanyan et al. (2021) to use our new causally masked objective with decoder-only models. 3.1 DATA Following Aghajanyan et al. (2021) we aim to implement a transform over HTML documents to extract out to minimal-HTML, i.e., the minimal set of text that is semantically relevant for end tasks. Birhane et al. (2021) gave in-depth criticisms of Common Crawl based multi-modal datasets and showed the existence of highly problematic examples (i.e., explicit images and text pairs of rape, pornography, and ethnic slurs). Given these severe ethical concerns, we opt-out of processing all of Common Crawl and instead opt into using a subset of the Common Crawl News (CC-NEWS) dataset and all of English Wikipedia. Given a valid HTML DOM1 per document, we run several passes to strip down the DOM to the elements of maximal semantic value. We ﬁrst remove all elements which do not contain textual ele- ments. We also ﬁlter out all headers, footers, copyrights, forms, dialog boxes and iFrames. We fold consecutive <div> elements into a singular <div> element with merged attributes. Furthermore we strip all the attributes from every element which are not derived from structured graphs such as OpenGraph, Schema and Twitter. For every <img> tag in the document with a valid src attribute URL, we download the image, resize to 256x256 pixels with random cropping and then tokenize it with VQVAE-GAN from Esser et al. (2021). This amounts to 256 tokens for every image. We then insert the string value of the tokens joined with a space back into the src attribute. We do not place any restrictions on the number of images or their locations. We present a set of high-level statistics in Table 1. Documents (Million) Size (GB) Unique Images (Million) Tokens (Billion) CC-NEWS 45 460 18 121 En-Wikipedia 16 383 7 102 Total 61 843 25 223 Table 1: High level statistics of the data used to train CM3. 1The DOM or Document Object Model is an interface that treats an HTML document as a tree structure wherein each node is an object representing a part of the document. 3

--- PAGE 4 ---
For experimentation, we create two test sets from each data source with 10,000 unique documents for each. We de-duplicated our test sets to ensure no overlap between test and train sets to the best of our abilities. 3.2 SIZE HINTS Aghajanyan et al. (2021) introduced the concept of size hints which allows the user to guide the model during sample generation through token conditioning. Speciﬁcally, HTLM inserts a prob- abilistic estimate of the size of the mask as a token post the mask token (e.g., <mask>12 for a probabilistic size of 12). For CM3, we noticed that size-hints degraded not only end-perplexity but also the zero-shot performance on a signiﬁcant set of evaluation tests. We also note that we can implicitly give a size hint during mask generation for a single mask by asking the model to generate causally max sequence length - size hint tokens before placing the secondary <mask:0> token. 3.3 TRAINING We train 4 models; 125M, 800M, 2.7B, and 13B parameters. The purpose of the two smaller models was to establish basic hyper-parameters that are viable for the causally masked language modeling objective and therefore were under-trained. However, all downstream tasks will be evaluated with our 2.7B model (CM3-Medium) and our 13B model (CM3-Large). HTLM-Medium was trained on 240 V100 GPU for 28 days, while HTLM-Large was trained on 384 A100 GPU for 24 days. Our implementation was in PyTorch (Paszke et al., 2019) using fairseq (Ott et al., 2019) and fairscale (Baines et al., 2021). For every model, our per GPU batch size was 8, with a maximum token sequence length of 2048. We use the polynomial decay learning rate scheduler available in Paszke et al. (2019) with 1500 warmup updates. We clipped the gradient norms to 1.0 and used the Adam optimizer with β1 = 0.9, β2 = 0.98 (Kingma & Ba, 2014). We defer our model architecture description to § A.1. 3.4 SCALING LAWS Our training setting has a couple of new parameters that can impact the traditional scaling laws of causal language models. The multi-modal nature of our proposed model breaks the standard assumptions of token distributionality. Traditionally language tokens are said to follow a Zipﬁan distribution (Piantadosi, 2014), while image tokens are strictly uniform (see § A.2). Furthermore, the unrestricted locations of the images and text introduce unpredictable complexity. Lastly, although we are still computing the joint probability of the document, we do so in a roundabout way through shufﬂing of the document via the causally masked objective. These fundamental differences warrant a quick look into the scaling laws of CM3. Perplexity Based Scaling Laws for CM3 100 100 100 Model CM3-XSmall CM3-Small CM3-Medium CM3-Large Validation Perplexity 0 50000 100000 150000 200000 0 1 2 3 4 Number of Documents 1e8 Number of Updates 0 1 2 3 Training Time (Seconds) 1e6 Figure 2: Basic perplexity based scaling laws for the proposed CM3 objective and training set-up. We present the various perplexity curves for the four models of varying sizes we trained. Given that our models were trained on various hardware set-ups, we normalize the training time by linearly scaling each experiment timing to 256 GPU. Most importantly, we see healthy scaling, similar to 4

--- PAGE 5 ---
Kaplan et al. (2020) without any pathological cases, implying there is still a good amount of gains to be achieved with further scaling. An in-depth analysis of the scaling laws of the causally masked objective is outside this current work’s scope and will be considered for future work. 4 ZERO/FEW-SHOT PROMPTING 4.1 IMAGE MODALITY Although we do not train on pure image documents, CM3 can still operate over image tasks. To do so, we must cleverly describe the task through a textual prompt, using the <img> tag. 4.1.1 UNCONDITIONAL IMAGE GENERATION To sample from the distribution of images available to CM3, we can simply ask the model to produce the next set of tokens after the following prompt: <img. Interestingly enough, CM3 prefers to ﬁrst generate a short description of the image through the alt attribute and then generate the image tokens via the src attribute. We can force the model to directly generate image tokens without ﬁrst giving a description with the following prompt: <img src=". We consider both prompts to test unconditional image generation since we do not condition the image generation but rather the model self-conditions. We sample according to the distribution of the model without altering the temperature. We present a sample of non-cherry picked examples in Figure 3. <img (b) Spain Europa Amenacer Winter (a) A mountain of olive trees on the way to Cabo de la Vela (c) blog TIGI Bed Head Tie Dye Spray Hair Spray Hairspray ml (d) birthday in- vitation printable christmas gift for birthday party Printable Template <img src=" Figure 3: Four samples for two of the prompts we proposed for unconditional image generation for CM3-Large. For the self-captioned images we place the respective caption under the image. Results were selected at random, with no cherry picking. The model is more than capable of generating coherent images. We note that via this prompting, we can recover the full functionality of the DALL-E model proposed in Ramesh et al. (2021). Interest- ingly enough, we see qualitative improvements with allowing the model to free generate a caption prior to generating. We continue by doing an empirical study of the unconditional generation of CM3, by generating 30k samples without textual conditioning and calculating the Fr´echet Inception Distance (FID, Heusel et al. (2017)) over MS-COCO, following the methodology proposed in Nichol et al. (2021) (Lin et al., 2014). We present our results in the uniﬁed table showing FID calculations in Table 2. With- out any textual conditioning and without explicitly optimizing for either MS-COCO or generation 5

