import os
import torch
# Streamlitê³¼ Torch ê°„ì˜ ì¶©ëŒ í•´ê²° íŒ¨ì¹˜
torch.classes.__path__ = []

import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

st.title("ğŸ“„ RAG Chatbot with Ollama LLM")

# PDF ì—…ë¡œë“œ ë° ì²˜ë¦¬
uploaded_file = st.file_uploader("Upload your PDF file here", type="pdf")
if uploaded_file:
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.getvalue())
    st.write("âœ… PDF íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    loader = PyPDFLoader("temp.pdf")
    docs = loader.load()
    st.write(f"ğŸ“„ ë¡œë“œëœ ë¬¸ì„œ(í˜ì´ì§€) ìˆ˜: {len(docs)}")
    if not docs:
        st.error("âŒ PDFì—ì„œ ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    embedder = HuggingFaceEmbeddings(model_name="intfloat/e5-base-v2")
    text_splitter = SemanticChunker(embedder)
    documents = text_splitter.split_documents(docs)
    st.write(f"ğŸ“‘ ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(documents)}")

    vector = FAISS.from_documents(documents, embedder)
    st.write("ğŸ—„ï¸ FAISS ë²¡í„° ì €ì¥ì†Œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    retriever = vector.as_retriever(search_type="similarity", search_kwargs={"k": 3})
    st.write("ğŸ” ê²€ìƒ‰ê¸°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    llm = OllamaLLM(model="deepseek-r1:14b")
    st.write("ğŸ¤– LLMì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    prompt = """
    Use the following context to answer the question.
    Context: {context}
    Question: {question}
    Answer:"""
    QA_PROMPT = PromptTemplate.from_template(prompt)

    combine_documents_chain = create_stuff_documents_chain(llm, QA_PROMPT)
    qa = create_retrieval_chain(retriever, combine_documents_chain)

    # ëŒ€í™” ì´ë ¥ì„ ì €ì¥ (ìµœì´ˆ ì ‘ì† ì‹œ ê¸°ë³¸ ì¸ì‚¬ ë©”ì‹œì§€ ì¶”ê°€)
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! PDF ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."}]

    # ì €ì¥ëœ ëŒ€í™” ë©”ì‹œì§€ë¥¼ ì±— ì¸í„°í˜ì´ìŠ¤ë¡œ ì¶œë ¥
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ ì±—ë´‡ ëŒ€í™” ì§„í–‰
    user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    if user_input:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° ì¶œë ¥
        st.session_state.messages.append({"role": "user", "content": user_input})
        st.chat_message("user").write(user_input)

        # RAG ì²´ì¸ì„ ì´ìš©í•´ ë‹µë³€ ìƒì„±
        response = qa.invoke({"input": user_input, "question": user_input})
        answer = response["answer"]

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥ ë° ì¶œë ¥
        st.session_state.messages.append({"role": "assistant", "content": answer})
        st.chat_message("assistant").write(answer)
