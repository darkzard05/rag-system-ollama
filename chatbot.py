import os
import torch
torch.classes.__path__ = []
import tempfile
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
import logging
from concurrent.futures import ThreadPoolExecutor, TimeoutError
import config  # ì„¤ì • íŒŒì¼ ë¡œë“œ

st.set_page_config(page_title="RAG Chatbot", layout="wide")  # í˜ì´ì§€ ì„¤ì •

st.title("ğŸ“„ RAG Chatbot with Ollama LLM")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)

# ì´ˆê¸° ì¶œë ¥ ë©”ì‹œì§€ë¥¼ ìœ„í•œ ê³µê°„ ì˜ˆì•½
initial_output = st.container()

# ì‚¬ì´ë“œë°”ì— íŒŒì¼ ì—…ë¡œë“œ ë°°ì¹˜
with st.sidebar:
    st.header("íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("Upload your PDF file here", type="pdf")

# ë©”ì¸ í˜ì´ì§€ì— ì±„íŒ… ê¸°ë¡ ë°°ì¹˜
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# PDF ë¡œë”©ì„ ìºì‹±: íŒŒì¼ ë°”ì´íŠ¸ë¥¼ ë°›ì•„ ë¬¸ì„œë¥¼ ì¶”ì¶œ
@st.cache_data(show_spinner=False)
def load_pdf_docs(file_bytes):
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=config.PDF_TEMP_SUFFIX) as tmp_file:
            tmp_file.write(file_bytes)
            temp_path = tmp_file.name
        loader = PyPDFLoader(temp_path)
        docs = loader.load()
        os.remove(temp_path)  # íŒŒì¼ì„ ìˆ˜ë™ìœ¼ë¡œ ì‚­ì œ
        return docs
    except Exception as e:
        logging.error(f"PDF ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# HuggingFaceEmbeddings ê°ì²´ ìƒì„± í•¨ìˆ˜
def create_embedder():
    return HuggingFaceEmbeddings(model_name=config.EMBEDDINGS_MODEL_NAME,
                                 model_kwargs=config.EMBEDDINGS_MODEL_KWARGS)

# ë¬¸ì„œ ë¶„í•  ìºì‹±: ë¡œë“œëœ ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
@st.cache_data(show_spinner=False)
def split_documents(_docs):
    try:
        embedder = create_embedder()
        chunker = SemanticChunker(embedder)
        return chunker.split_documents(_docs)
    except Exception as e:
        logging.error(f"ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# ë²¡í„° ì €ì¥ì†Œ ìƒì„± ìºì‹±: ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ FAISS ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
@st.cache_resource(show_spinner=False)
def create_vector_store(_documents):
    try:
        embedder = create_embedder()
        return FAISS.from_documents(_documents, embedder)
    except Exception as e:
        logging.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# LLM ì´ˆê¸°í™” ìºì‹±: LLM ê°ì²´ëŠ” í•œ ë²ˆë§Œ ìƒì„±í•˜ë„ë¡ í•¨
@st.cache_resource(show_spinner=False)
def init_llm():
    try:
        llm = OllamaLLM(model=config.LLM_MODEL, device=config.LLM_DEVICE)
        logging.debug("LLM ì´ˆê¸°í™” ì„±ê³µ")
        return llm
    except Exception as e:
        logging.error(f"LLM ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# ë©”ì‹œì§€ ì¶”ê°€ í•¨ìˆ˜
def add_message(role, content):
    st.session_state.messages.append({"role": role, "content": content})
    st.chat_message(role).write(content)

if uploaded_file:
    file_bytes = uploaded_file.getvalue()
    docs = load_pdf_docs(file_bytes)
    initial_output.write(f"ğŸ“„ ë¡œë“œëœ ë¬¸ì„œ(í˜ì´ì§€) ìˆ˜: {len(docs)}")
    if not docs:
        st.error("âŒ PDFì—ì„œ ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    with ThreadPoolExecutor() as executor:
        future_split = executor.submit(split_documents, docs)
        try:
            documents = future_split.result(timeout=config.PDF_LOAD_TIMEOUT)
        except TimeoutError:
            st.error("âŒ ë¬¸ì„œ ë¶„í• ì´ ì‹œê°„ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.stop()
        except Exception as e:
            st.error(f"âŒ ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            st.stop()
    
    initial_output.write(f"ğŸ“‘ ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(documents)}")

    vector_store = create_vector_store(documents)
    if vector_store is None:
        st.error("âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        st.stop()
    
    initial_output.write("ğŸ—„ï¸ FAISS ë²¡í„° ì €ì¥ì†Œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    retriever = vector_store.as_retriever(search_type=config.RETRIEVER_SEARCH_TYPE,
                                          search_kwargs=config.RETRIEVER_SEARCH_KWARGS)
    initial_output.write("ğŸ” ê²€ìƒ‰ê¸°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    llm = init_llm()
    if llm is None:
        st.error("âŒ LLM ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        st.stop()
    
    initial_output.write("ğŸ¤– LLMì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    prompt = """
    ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì „ë¬¸ ì¡°ë ¥ìì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì„¸ìš”:

    [ê·œì¹™]
    1. ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸({context})ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
    2. ê°„ê²°í•˜ê³  ëª…ë£Œí•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    3. ì „ë¬¸ì ì´ê³  ê¹”ë”í•œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    4. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
    
    [ì»¨í…ìŠ¤íŠ¸]
    {context}

    [ì‹¤ì œ ì§ˆë¬¸]
    ì§ˆë¬¸: {input}
    ë‹µë³€:
    """
    QA_PROMPT = PromptTemplate.from_template(prompt)
    combine_chain = create_stuff_documents_chain(llm, QA_PROMPT)
    qa_chain = create_retrieval_chain(retriever, combine_chain)

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    if user_input:
        add_message("user", user_input)
        try:
            response = qa_chain.invoke({"input": user_input, "context": documents})
            answer = response["answer"]
        except Exception as e:
            answer = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        add_message("assistant", answer)