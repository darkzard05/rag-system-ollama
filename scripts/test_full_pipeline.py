import os
import sys
import asyncio
import json
import time
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
ROOT_DIR = Path(__file__).parent.parent.absolute()
sys.path.append(str(ROOT_DIR / "src"))

from core.rag_core import RAGSystem
from core.model_loader import load_llm, load_embedding_model
from common.config import DEFAULT_OLLAMA_MODEL, OLLAMA_BASE_URL
from common.logging_config import setup_logging

async def run_evaluation(data_points: list[dict]):
    """Ragasë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤."""
    print("\n" + "=" * 50)
    print("[Ragas] Starting Automatic Quality Evaluation...")
    
    try:
        from ragas import evaluate, EvaluationDataset
        from ragas.metrics import Faithfulness, AnswerRelevancy
        from ragas.llms import LangchainLLMWrapper
        from ragas.embeddings import LangchainEmbeddingsWrapper
        from langchain_ollama import ChatOllama
        from ragas import RunConfig

        # 1. ë°ì´í„°ì…‹ êµ¬ì„±
        eval_data = []
        for d in data_points:
            eval_data.append({
                "user_input": d["query"],
                "response": d["response"],
                "retrieved_contexts": [d["context"]] if d["context"] else ["(No context)"]
            })
        
        dataset = EvaluationDataset.from_list(eval_data)

        # 2. í‰ê°€ê¸° ì„¤ì • (ë¡œì»¬ Ollama ì‚¬ìš©, JSON ì¶œë ¥ ë° ê²°ì •ë¡ ì  ì¶”ë¡  ê°•í™”)
        llm = ChatOllama(
            model=DEFAULT_OLLAMA_MODEL, 
            base_url=OLLAMA_BASE_URL,
            timeout=600,
            format="json", # [í•´ê²°ì±… 2] Ollama JSON ëª¨ë“œ ê°•ì œ
            temperature=0,  # [í•´ê²°ì±… 3] ì¶œë ¥ ì•ˆì •ì„± ê·¹ëŒ€í™”
            num_ctx=8192
        )
        evaluator_llm = LangchainLLMWrapper(llm)
        
        embedder = load_embedding_model()
        evaluator_embeddings = LangchainEmbeddingsWrapper(embedder)

        # [ì°¸ê³ ] RagasëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì§€ë§Œ, 
        # format="json" ì„¤ì •ì´ ì ìš©ëœ OllamaëŠ” LLMì´ í…ìŠ¤íŠ¸ë¥¼ ì„ì–´ ì“°ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.

        metrics = [
            Faithfulness(llm=evaluator_llm),
            AnswerRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)
        ]

        # 3. í‰ê°€ ì‹¤í–‰
        print(f"[Ragas] Starting evaluation for {len(dataset)} cases...")
        print("[Ragas] Running metrics (Faithfulness, AnswerRelevancy)...")
        
        start_eval_time = time.time()
        run_config = RunConfig(timeout=300, max_workers=1) 
        
        # [ìµœì í™”] ê²°ê³¼ ì§ì ‘ í™•ì¸ì„ ìœ„í•´ evaluate í˜¸ì¶œë¶€ ê°ì‹¸ê¸°
        results = evaluate(dataset=dataset, metrics=metrics, run_config=run_config)
        
        eval_duration = time.time() - start_eval_time
        print(f"[Ragas] Evaluation finished in {eval_duration:.2f}s")

        # 4. ë¦¬í¬íŠ¸ ì €ì¥
        report_dir = ROOT_DIR / "reports"
        report_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = report_dir / f"e2e_eval_report_{timestamp}.md"

        summary = results.to_pandas().mean(numeric_only=True).to_dict()
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(f"# E2E Pipeline Evaluation Report ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n\n")
            f.write(f"**Evaluator Model:** {DEFAULT_OLLAMA_MODEL}\n\n")
            f.write("## ğŸ“Š Summary Scores\n\n")
            for m, s in summary.items():
                f.write(f"- **{m}:** {s:.4f}\n")
            f.write("\n## ğŸ” Detailed Analysis\n\n")
            f.write(results.to_pandas().to_markdown(index=False))

        print(f"[Ragas] Evaluation complete. Scores: {summary}")
        print(f"[Ragas] Detailed report saved to: {report_path}")
        print("=" * 50)

    except Exception as e:
        print(f"[Ragas] Evaluation failed: {e}")
        import traceback
        traceback.print_exc()

async def run_full_pipeline_test():
    # ë¡œê¹… ì„¤ì • ì´ˆê¸°í™”
    setup_logging(log_level="INFO", log_file=ROOT_DIR / "logs" / "test_e2e.log")
    
    print("[E2E] RAG Pipeline Integration Test Started")
    
    session_id = f"test-session-{int(datetime.now().timestamp())}"
    rag = RAGSystem(session_id=session_id)
    
    print("1. Loading Models...")
    try:
        embedder = load_embedding_model()
        llm = load_llm(DEFAULT_OLLAMA_MODEL)
    except Exception as e:
        print(f"Error loading models: {e}")
        return

    test_pdf = str(ROOT_DIR / "tests" / "data" / "2201.07520v1.pdf")
    print(f"2. Indexing Document: {os.path.basename(test_pdf)}")
    
    start_time = asyncio.get_event_loop().time()
    msg, cache_used = await rag.load_document(test_pdf, "2201.07520v1.pdf", embedder)
    load_time = asyncio.get_event_loop().time() - start_time
    print(f"   Result: {msg} (Cache: {cache_used}) | Time: {load_time:.2f}s")

    # í‰ê°€ë¥¼ ìœ„í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì„¸íŠ¸ (ë¹ ë¥¸ ê²€ì¦ì„ ìœ„í•´ 1ê°œë¡œ ì œí•œ)
    test_cases = [
        "CM3 ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•˜ëŠ” êµ¬ì²´ì ì¸ ì›ë¦¬ì™€ í† í°í™” ë°©ì‹ì€ ë­ì•¼? ê¸°ì¡´ DALL-Eì™€ëŠ” ì–´ë–¤ ì°¨ì´ê°€ ìˆì–´?"
    ]
    
    captured_data = []

    print("\n3. Running Test Queries & Collecting Data...")
    for i, test_query in enumerate(test_cases):
        print(f"   [{i+1}/{len(test_cases)}] Querying: '{test_query[:50]}...'")
        
        start_t = asyncio.get_event_loop().time()
        result = await rag.aquery(test_query, llm=llm)
        q_time = asyncio.get_event_loop().time() - start_t
        
        print(f"   -> Done ({q_time:.2f}s)")
        
        captured_data.append({
            "query": test_query,
            "response": result.get("response", ""),
            "context": result.get("context", "")
        })

    print(f"\n4. Pipeline Test Finished (Total Queries: {len(test_cases)})")
    
    # 5. [ì¶”ê°€] ì¦‰ì‹œ í‰ê°€ ì‹¤í–‰
    await run_evaluation(captured_data)

if __name__ == "__main__":
    asyncio.run(run_full_pipeline_test())