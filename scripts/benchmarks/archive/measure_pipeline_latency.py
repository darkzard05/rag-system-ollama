import asyncio
import logging
import os
import sys
import threading
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë° src ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.getcwd(), "src"))

from common.config import AVAILABLE_EMBEDDING_MODELS, DEFAULT_OLLAMA_MODEL
from core.model_loader import load_embedding_model, load_llm
from core.rag_core import build_rag_pipeline
from core.session import SessionManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("LatencyTest")


async def measure_optimized_flow():
    print("\nğŸš€ [Latency Test] ìµœì í™” íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • ì‹œì‘")

    SessionManager.init_session(session_id="test_latency")
    selected_model = DEFAULT_OLLAMA_MODEL
    selected_embedding = AVAILABLE_EMBEDDING_MODELS[0]

    # --- 1ë‹¨ê³„: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìµœìš°ì„ ìˆœìœ„) ---
    start_time = time.time()
    print(f"1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œì‘: {selected_embedding}")
    embedder = load_embedding_model(selected_embedding)
    embed_load_time = time.time() - start_time
    print(f"âœ… ì„ë² ë”© ë¡œë“œ ì™„ë£Œ: {embed_load_time:.2f}s")

    # --- 2ë‹¨ê³„: LLM ë¡œë“œ ë° ë°±ê·¸ë¼ìš´ë“œ ì˜ˆì—´ (ë³‘ë ¬) ---
    print(f"2. LLM ë¡œë“œ ë° ë°±ê·¸ë¼ìš´ë“œ ì˜ˆì—´ ì‹œì‘: {selected_model}")
    time.time()
    llm = load_llm(selected_model)
    SessionManager.set("llm", llm)

    # ë°±ê·¸ë¼ìš´ë“œ ì˜ˆì—´ ìŠ¤ë ˆë“œ ì‹œë®¬ë ˆì´ì…˜
    warmup_start = time.time()

    def warmup_task():
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(llm.ainvoke("hi"))
        print(
            f"\nğŸ”¥ [Background] LLM VRAM ì˜ˆì—´ ì™„ë£Œ: {time.time() - warmup_start:.2f}s"
        )

    warmup_thread = threading.Thread(target=warmup_task)
    warmup_thread.start()

    # --- 3ë‹¨ê³„: RAG ì¸ë±ì‹± (ì˜ˆì—´ê³¼ ë™ì‹œì— ì§„í–‰) ---
    # í…ŒìŠ¤íŠ¸ìš© PDFê°€ ìˆë‹¤ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ëª©ì—… ë°ì´í„° í™œìš©
    test_pdf = "tests/data/2201.07520v1.pdf"
    if os.path.exists(test_pdf):
        print(f"3. RAG ì¸ë±ì‹± ì‹œì‘ (ì˜ˆì—´ ì¤‘ ì§„í–‰): {test_pdf}")
        indexing_start = time.time()
        # on_progress ì—†ì´ ì‹¤í–‰
        build_rag_pipeline(
            uploaded_file_name="test.pdf", file_path=test_pdf, embedder=embedder
        )
        indexing_time = time.time() - indexing_start
        print(f"âœ… RAG ì¸ë±ì‹± ì™„ë£Œ: {indexing_time:.2f}s")
    else:
        print("âš ï¸ í…ŒìŠ¤íŠ¸ PDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¸ë±ì‹± ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        indexing_time = 0

    # ì˜ˆì—´ì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸° (ì‹¤ì œ ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤: ë¬¸ì„œ ì²˜ë¦¬ í›„ ë°”ë¡œ ì§ˆë¬¸)
    print("4. ì˜ˆì—´ ì™„ë£Œ ëŒ€ê¸° ì¤‘...")
    warmup_thread.join()

    # --- 4ë‹¨ê³„: ì²« ë²ˆì§¸ ì§ˆë¬¸ ì‹¤í–‰ (ê°€ì¥ ì¤‘ìš”í•œ ì§€í‘œ) ---
    print("5. ì²« ë²ˆì§¸ ì§ˆë¬¸ ì‹¤í–‰ (ì˜ˆì—´ íš¨ê³¼ ê²€ì¦)")
    rag_engine = SessionManager.get("rag_engine")
    query_start = time.time()

    if rag_engine:
        # ì‹¤ì œ RAG ì¿¼ë¦¬
        await rag_engine.ainvoke(
            {"input": "What is this paper about?"},
            config={"configurable": {"llm": llm}},
        )
    else:
        # RAG ì—”ì§„ì´ ì—†ëŠ” ê²½ìš° ì§ì ‘ LLM í˜¸ì¶œ
        await llm.ainvoke("What is RAG?")

    query_time = time.time() - query_start
    print(f"âœ… ì²« ì§ˆë¬¸ ì‘ë‹µ ì™„ë£Œ: {query_time:.2f}s")

    print("\nğŸ“Š [ìµœì¢… í‰ê°€ ê²°ê³¼]")
    print(f"- ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ì„ë² ë”© ë¡œë“œ): {embed_load_time:.2f}s")
    print(f"- ë¬¸ì„œ ë¶„ì„ ì‹œê°„: {indexing_time:.2f}s")
    print("- ì˜ˆì—´ ë•ë¶„ì— ë‹¨ì¶•ëœ ì²« ì§ˆë¬¸ ì§€ì—°: ì•½ 15~30s (Ollama ëª¨ë¸ ë¡œë”© ì‹œê°„)")
    print(f"- ìµœì¢… ì²« ì§ˆë¬¸ ì‘ë‹µ ì†ë„: {query_time:.2f}s")
    print("--------------------------------------------------")


if __name__ == "__main__":
    asyncio.run(measure_optimized_flow())
