# RAG System with Ollama & LangGraph

This project is a Streamlit web application that allows you to chat with your PDF documents using a local Large Language Model (LLM) powered by Ollama. It leverages the RAG (Retrieval-Augmented Generation) pattern, LangChain, and LangGraph to provide contextual answers based on the content of your uploaded files.

## Key Features

-   **Chat with Your PDFs:** Upload a PDF file and ask questions about its content.
-   **Powered by Local LLMs:** Uses Ollama to run LLMs locally on your machine, ensuring privacy and control.
-   **Selectable Embedding Models:** Choose from a list of different sentence-transformer models for document embedding.
-   **Hybrid Search:** Combines dense vector search (FAISS) and keyword-based search (BM25) for robust and accurate document retrieval.
-   **Built with LangGraph:** The RAG pipeline is orchestrated as a graph, making the logic clear and extensible.
-   **Interactive UI:** A user-friendly interface built with Streamlit, including a PDF viewer.
-   **Caching:** Caches processed documents (vector stores) to ensure fast re-loading of previously analyzed files.
-   **Semantic Chunking:** Supports advanced semantic chunking based on embedding similarity for better context preservation.

## Setup and Installation

### 1. Prerequisites: Install Ollama

You must have Ollama installed and running on your system.

1.  Download and install Ollama from the [official website](https://ollama.com/).
2.  Pull the LLM model you intend to use. The default model in this project is `gemma3:8b` (configurable).
    ```sh
    ollama pull gemma3:8b
    ```

### 2. Clone the Repository

```sh
git clone <repository-url>
cd rag-system-ollama
```

### 3. Create a Virtual Environment (Recommended)

```sh
# For Windows
python -m venv venv
.\venv\Scripts\activate

# For macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### 4. Install Dependencies

Install all the required Python packages from `requirements.txt`.

```sh
pip install -r requirements.txt
```

## How to Run

Once the setup is complete, you can run the Streamlit application with the following command:

```sh
streamlit run src/main.py
```

The application will open in your default web browser.

## Project Structure

```
.
â”œâ”€â”€ .env.example
â”œâ”€â”€ config.yml
â”œâ”€â”€ readme.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ src/
    â”œâ”€â”€ config.py          # Loads settings from config.yml
    â”œâ”€â”€ graph_builder.py   # Defines the RAG workflow using LangGraph
    â”œâ”€â”€ main.py            # Main entry point for the Streamlit app
    â”œâ”€â”€ model_loader.py    # Handles loading LLMs and embedding models
    â”œâ”€â”€ rag_core.py        # Core RAG logic (document loading, splitting, embedding)
    â”œâ”€â”€ schemas.py         # Data schemas (e.g., GraphState)
    â”œâ”€â”€ semantic_chunker.py# Custom semantic chunking logic
    â”œâ”€â”€ session.py         # Manages the Streamlit session state
    â”œâ”€â”€ ui.py              # Contains all Streamlit UI rendering functions
    â””â”€â”€ utils.py           # Utility functions (e.g., decorators)
```

## Configuration

The main behavior of the application can be configured in the `config.yml` file:

-   **`models`**: Set the default Ollama model, preferred embedding models, etc.
-   **`rag`**: Configure the RAG pipeline, such as chunk size, retriever settings, semantic chunking parameters, and ensemble weights.
-   **`ui`**: Change UI messages and container heights.

## License
This project is distributed under the MIT License. See the `LICENSE` file for more details.

---
<a name="korean"></a>
# LangGraph & Ollama ê¸°ë°˜ RAG ì±—ë´‡ (í•œêµ­ì–´)

**LangGraph, ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„(BM25 & FAISS), Ollama, Streamlit ê¸°ë°˜ì˜ ê³ ë„í™”ëœ PDF ì±—ë´‡**

ì´ í”„ë¡œì íŠ¸ëŠ” ì •êµí•œ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ë©°, í•µì‹¬ íŒŒì´í”„ë¼ì¸ì´ **LangGraph**ë¥¼ ì‚¬ìš©í•œ ìƒíƒœ ë¨¸ì‹ ìœ¼ë¡œ êµ¬ì¡°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìëŠ” PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  Ollamaë¥¼ í†µí•´ ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” LLMê³¼ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![RAG Chatbot Preview](image/image1.png)

## ğŸ”‘ ì£¼ìš” ê¸°ëŠ¥

- **LangGraph ê¸°ë°˜ ì•„í‚¤í…ì²˜**: RAG íŒŒì´í”„ë¼ì¸(ê²€ìƒ‰ -> ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± -> ë‹µë³€ ìƒì„±)ì„ ê·¸ë˜í”„ë¡œ êµ¬ì¶•í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ë¥¼ íˆ¬ëª…í•˜ê³  ìˆ˜ì •í•˜ê¸° ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤.
- **ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰(BM25)ê³¼ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰(FAISS)ì„ ê²°í•©í•œ **ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•˜ê³  ë¬¸ë§¥ì— ë§ëŠ” ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- **ì˜ë¯¸ë¡ ì  ì²­í‚¹(Semantic Chunking)**: ë‹¨ìˆœ ê¸¸ì´ ê¸°ë°˜ì´ ì•„ë‹Œ, ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ë§¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆì„ ë†’ì…ë‹ˆë‹¤.
- **ê°•ë ¥í•œ ë¡œì»¬ LLM (Ollama)**: ê°œì¸ ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•´ Ollamaë¡œ ë¡œì»¬ì—ì„œ LLMì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
- **íš¨ìœ¨ì ì¸ ìºì‹±**: ë²¡í„° ì €ì¥ì†Œì™€ ëª¨ë¸ì„ ìºì‹±í•˜ì—¬, ì•±ì„ ì¬ì‹¤í–‰í•˜ê±°ë‚˜ ë™ì¼í•œ íŒŒì¼ì„ ë‹¤ì‹œ ë¡œë“œí•  ë•Œ ì†ë„ê°€ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤.
- **ëŒ€í™”í˜• UI**: ì†ì‰¬ìš´ ë¬¸ì„œ ì—…ë¡œë“œ, ëª¨ë¸ ì„ íƒ, ì±„íŒ… ë° ë‚˜ë€íˆ ë³´ëŠ” PDF ë·°ì–´ë¥¼ ìœ„í•´ Streamlitìœ¼ë¡œ êµ¬ì¶•ëœ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.

---

## âš¡ ë¹ ë¥¸ ì‹œì‘

### ğŸ“‹ ì‚¬ì „ ì¤€ë¹„ ì‚¬í•­
- **Python**: 3.10 ì´ìƒ
- **Ollama**: ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ, Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³  ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
  - ì„¤ì¹˜ëŠ” [Ollama ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://ollama.com)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
- **ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤**: ë¡œì»¬ ëª¨ë¸(7B ì´ìƒ) êµ¬ë™ì„ ìœ„í•œ ì¶©ë¶„í•œ RAM/VRAMì´ ê¶Œì¥ë©ë‹ˆë‹¤.

---

### ğŸ’» ì„¤ì¹˜ ë° ì‹¤í–‰

1.  **ì €ì¥ì†Œ í´ë¡ **
    ```bash
    git clone <repository-url>
    cd rag-system-ollama
    ```

2.  **(ê¶Œì¥) ê°€ìƒ í™˜ê²½ ìƒì„± ë° í™œì„±í™”**
    ```bash
    python -m venv venv
    
    # Windows
    venv\Scripts\activate
    
    # macOS/Linux
    source venv/bin/activate
    ```

3.  **í•„ìš”í•œ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**
    ê¸°ë³¸ ì„¤ì • ëª¨ë¸ì¸ `gemma3:8b`ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜, ì›í•˜ëŠ” ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.
    ```bash
    ollama pull gemma3:8b
    ```
    - ì´ ëª…ë ¹ì„ ì‹¤í–‰í•˜ê¸° ì „ì— Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.
    - `config.yml`ì˜ `default_ollama` ì„¤ì •ì„ ì›í•˜ëŠ” ëª¨ë¸ë¡œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

5.  **Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰**
    ```bash
    streamlit run src/main.py
    ```

6.  ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:8501`ë¡œ ì ‘ì†í•˜ì—¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ êµ¬ì¡°
```
.
â”œâ”€â”€ config.yml        # ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • íŒŒì¼
â”œâ”€â”€ requirements.txt  # ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ëª©ë¡
â””â”€â”€ src/
    â”œâ”€â”€ main.py            # ì•± ì§„ì…ì 
    â”œâ”€â”€ ui.py              # UI ë Œë”ë§ ë¡œì§
    â”œâ”€â”€ session.py         # ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
    â”œâ”€â”€ rag_core.py        # RAG í•µì‹¬ ë¡œì§ (ë¬¸ì„œ ì²˜ë¦¬, ê²€ìƒ‰)
    â”œâ”€â”€ graph_builder.py   # LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜
    â”œâ”€â”€ semantic_chunker.py# ì˜ë¯¸ë¡ ì  ì²­í‚¹ êµ¬í˜„ì²´
    â”œâ”€â”€ model_loader.py    # ëª¨ë¸ ë¡œë”© ë° ìºì‹±
    â”œâ”€â”€ schemas.py         # ë°ì´í„° ìŠ¤í‚¤ë§ˆ
    â”œâ”€â”€ config.py          # ì„¤ì • ë¡œë”
    â””â”€â”€ utils.py           # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
```

## âš™ï¸ ì„¤ì •

- **ëª¨ë¸ ë° íŒŒë¼ë¯¸í„°**: `config.yml` íŒŒì¼ì—ì„œ ë‹¤ìŒ í•­ëª©ë“¤ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - `models`: ê¸°ë³¸ LLM ë° ì„ë² ë”© ëª¨ë¸
  - `rag`: ì²­í‚¹ ì „ëµ(Semantic Chunking ë“±), ë¦¬íŠ¸ë¦¬ë²„ ê°€ì¤‘ì¹˜, ê²€ìƒ‰ íŒŒë¼ë¯¸í„°(k)
  - `ui`: UI ë©”ì‹œì§€ ë° ë ˆì´ì•„ì›ƒ ì„¤ì •

## ğŸ“„ ë¼ì´ì„ ìŠ¤
ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `LICENSE` íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.