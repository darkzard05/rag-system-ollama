# RAG System with Ollama & LangGraph

This project is a Streamlit web application that allows you to chat with your PDF documents using a local Large Language Model (LLM) powered by Ollama. It leverages the RAG (Retrieval-Augmented Generation) pattern, LangChain, and LangGraph to provide contextual answers based on the content of your uploaded files.

## Key Features

-   **Chat with Your PDFs:** Upload a PDF file and ask questions about its content.
-   **Powered by Local LLMs:** Uses Ollama to run LLMs locally on your machine, ensuring privacy and control.
-   **Selectable Embedding Models:** Choose from a list of different sentence-transformer models for document embedding.
-   **Hybrid Search:** Combines dense vector search (FAISS) and keyword-based search (BM25) for robust and accurate document retrieval.
-   **Built with LangGraph:** The RAG pipeline is orchestrated as a graph, making the logic clear and extensible.
-   **Interactive UI:** A user-friendly interface built with Streamlit, including a PDF viewer.
-   **Caching:** Caches processed documents (vector stores) to ensure fast re-loading of previously analyzed files.

## Setup and Installation

### 1. Prerequisites: Install Ollama

You must have Ollama installed and running on your system.

1.  Download and install Ollama from the [official website](https://ollama.com/).
2.  Pull the LLM model you intend to use. The default model in this project is `qwen2:1.5b`.
    ```sh
    ollama pull qwen2:1.5b
    ```

### 2. Clone the Repository

```sh
git clone <repository-url>
cd rag-system-ollama
```

### 3. Create a Virtual Environment (Recommended)

```sh
# For Windows
python -m venv venv
.\venv\Scripts\activate

# For macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### 4. Install Dependencies

Install all the required Python packages from `requirements.txt`.

```sh
pip install -r requirements.txt
```

## How to Run

Once the setup is complete, you can run the Streamlit application with the following command:

```sh
streamlit run src/main.py
```

The application will open in your default web browser.

## Project Structure

```
.
â”œâ”€â”€ .env.example
â”œâ”€â”€ config.yml
â”œâ”€â”€ readme.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ src/
    â”œâ”€â”€ config.py          # Loads settings from config.yml
    â”œâ”€â”€ graph_builder.py   # Defines the RAG workflow using LangGraph
    â”œâ”€â”€ main.py            # Main entry point for the Streamlit app
    â”œâ”€â”€ model_loader.py    # Handles loading LLMs and embedding models
    â”œâ”€â”€ rag_core.py        # Core RAG logic (document loading, splitting, embedding)
    â”œâ”€â”€ schemas.py         # Data schemas (e.g., GraphState)
    â”œâ”€â”€ session.py         # Manages the Streamlit session state
    â”œâ”€â”€ ui.py              # Contains all Streamlit UI rendering functions
    â””â”€â”€ utils.py           # Utility functions (e.g., decorators)
```

## Configuration

The main behavior of the application can be configured in the `config.yml` file:

-   **`models`**: Set the default Ollama model, preferred embedding models, etc.
-   **`rag`**: Configure the RAG pipeline, such as chunk size, retriever settings, and ensemble weights.
-   **`ui`**: Change UI messages and container heights.


---
<a name="english"></a>
# RAG Chatbot with LangGraph, Ollama & Gemini (English)

**An advanced PDF-based Chatbot powered by LangGraph, an Ensemble Retriever (BM25 & FAISS), Ollama, Gemini, and Streamlit.**

This project implements a sophisticated RAG (Retrieval-Augmented Generation) system where the core pipeline is structured as a state machine using **LangGraph**. This approach enhances modularity, observability, and scalability.

![RAG Chatbot Preview](image/image1.png)

## ğŸ”‘ Key Features

- **LangGraph-based Architecture**: The RAG pipeline (Retrieve -> Format Context -> Generate Response) is built as a graph, making the workflow transparent and easy to modify.
- **Workflow Visualization**: The Streamlit UI includes a "Workflow" tab that visually represents the RAG pipeline structure using Mermaid diagrams, offering clear insight into the operational flow.
- **Advanced Hybrid Search**: Utilizes an **Ensemble Retriever** that combines keyword-based search (BM25) and semantic search (FAISS) to deliver more accurate and contextually relevant results.
- **Flexible LLM Selection (Ollama & Gemini)**: Choose between running large language models locally with Ollama for privacy, or using powerful Gemini models via API for high performance.
- **Efficient Caching**: The system caches the entire FAISS vector store, not just document splits. This significantly speeds up initialization by avoiding the need to re-calculate embeddings for previously processed documents.
- **View LLM's Thinking Process**: An expander in the UI shows the internal "thought" process of the LLM before it generates a final answer, providing transparency into its reasoning.
- **Interactive UI**: A user-friendly interface built with Streamlit for easy document upload, model selection, chatting, and side-by-side PDF viewing.

---

## âš¡ Quick Start

### ğŸ“‹ Prerequisites
- **Python**: 3.10 or higher
- **Ollama (Optional)**: If using local models, Ollama must be installed and the server running.
  - Refer to the [Ollama Official Website](https://ollama.com) for installation.
- **Gemini API Key (Optional)**: If using Gemini models, you need to set up your API key.
  - Create a `.env` file in the project's root directory.
  - Add your API key to the file: `GEMINI_API_KEY="YOUR_API_KEY"`
  - You can get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).
- **System Resources**: For local models, sufficient RAM (e.g., 16GB+ for 7B models) is recommended.

---

### ğŸ’» Installation & Run

1.  **Clone the repository**
    ```bash
    git clone https://github.com/darkzard05/rag-system-ollama.git
    cd rag-system-ollama
    ```

2.  **(Recommended) Create and activate a virtual environment**
    ```bash
    python -m venv venv
    
    # On Windows
    venv\Scripts\activate
    
    # On macOS/Linux
    source venv/bin/activate
    ```

3.  **Install required Python packages**
    ```bash
    pip install -r requirements.txt
    ```

4.  **(For Ollama Users) Pull a model**
    We recommend a general-purpose model to start. You can find more models at the [Ollama Library](https://ollama.com/library).
    ```bash
    ollama pull llama3:8b
    ```
    - Ensure the Ollama server is running before this step.
    - Make sure the `default_ollama` model name in your `config.yml` matches a model you have pulled.

5.  **Run the Streamlit application**
    ```bash
    streamlit run src/main.py
    ```

6.  Open your web browser and go to `http://localhost:8501` to use the application.

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ ...
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ image/
â”‚   â””â”€â”€ image1.png
â””â”€â”€ src/
    â”œâ”€â”€ main.py
    â”œâ”€â”€ ui.py
    â”œâ”€â”€ session.py
    â”œâ”€â”€ rag_core.py
    â”œâ”€â”€ graph_builder.py  <-- New: Defines the LangGraph structure
    â”œâ”€â”€ schemas.py        <-- New: Defines data structures (e.g., GraphState)
    â”œâ”€â”€ config.py
    â””â”€â”€ config.yml
```
- **`main.py`**: Entry point of the Streamlit application.
- **`ui.py`**: Renders the Streamlit UI, including the chat interface and workflow visualization.
- **`session.py`**: Manages the application's session state.
- **`rag_core.py`**: Handles core RAG logic like document processing, embedding, and retriever creation.
- **`graph_builder.py`**: Constructs the RAG workflow using LangGraph.
- **`schemas.py`**: Defines the state object used throughout the graph.
- **`config.py` & `config.yml`**: Manage application configurations.

## âœ¨ Key Components

- **LangGraph**: Orchestrates the RAG pipeline as a stateful graph.
- **PDF Loader (PyMuPDF)**: Loads and extracts text from PDF files.
- **Text Splitter (Langchain)**: Divides text into smaller, manageable chunks.
- **Embedding Model (Sentence Transformers)**: Converts text chunks into vector embeddings.
- **Vector Store (FAISS)**: Stores embeddings for efficient similarity search.
- **Retriever (Ensemble)**: Combines **BM25** (keyword) and **FAISS** (semantic) search.
- **LLM (Ollama & Gemini)**: Generates answers based on user queries and retrieved context.
- **Streamlit UI**: Provides the interactive web interface.

## âš™ï¸ Configuration

- **API Keys**: Set your `GEMINI_API_KEY` in a `.env` file.
- **Models and Parameters**: Adjust models, retriever weights, text splitter settings, and **Ollama's response token limit (`ollama_num_predict`)** in `config.yml`. The default `ollama_num_predict` is set to `2048` to balance detailed responses with performance.

## ğŸ“„ License
This project is distributed under the MIT License. See the `LICENSE` file for more details.

---
<a name="korean"></a>
# LangGraph, Ollama & Gemini ê¸°ë°˜ RAG ì±—ë´‡ (í•œêµ­ì–´)

**LangGraph, ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„(BM25 & FAISS), Ollama, Gemini, Streamlit ê¸°ë°˜ì˜ ê³ ë„í™”ëœ PDF ì±—ë´‡**

ì´ í”„ë¡œì íŠ¸ëŠ” ì •êµí•œ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ë©°, í•µì‹¬ íŒŒì´í”„ë¼ì¸ì´ **LangGraph**ë¥¼ ì‚¬ìš©í•œ ìƒíƒœ ë¨¸ì‹ ìœ¼ë¡œ êµ¬ì¡°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ëª¨ë“ˆì„±, ê´€ì°° ê°€ëŠ¥ì„± ë° í™•ì¥ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

![RAG Chatbot Preview](image/image1.png)

## ğŸ”‘ ì£¼ìš” ê¸°ëŠ¥

- **LangGraph ê¸°ë°˜ ì•„í‚¤í…ì²˜**: RAG íŒŒì´í”„ë¼ì¸(ê²€ìƒ‰ -> ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± -> ë‹µë³€ ìƒì„±)ì„ ê·¸ë˜í”„ë¡œ êµ¬ì¶•í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ë¥¼ íˆ¬ëª…í•˜ê³  ìˆ˜ì •í•˜ê¸° ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤.
- **ì›Œí¬í”Œë¡œìš° ì‹œê°í™”**: Streamlit UIì— RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¥¼ Mermaid ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” "ì›Œí¬í”Œë¡œìš°" íƒ­ì„ í¬í•¨í•˜ì—¬, ì‘ë™ íë¦„ì— ëŒ€í•œ ëª…í™•í•œ í†µì°°ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
- **ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰(BM25)ê³¼ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰(FAISS)ì„ ê²°í•©í•œ **ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•˜ê³  ë¬¸ë§¥ì— ë§ëŠ” ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- **ìœ ì—°í•œ LLM ì„ íƒ (Ollama & Gemini)**: ê°œì¸ ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•´ Ollamaë¡œ ë¡œì»¬ì—ì„œ LLMì„ ì‹¤í–‰í•˜ê±°ë‚˜, ê³ ì„±ëŠ¥ì„ ìœ„í•´ APIë¥¼ í†µí•´ ê°•ë ¥í•œ Gemini ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ ì¤‘ì—ì„œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **íš¨ìœ¨ì ì¸ ìºì‹±**: ì‹œìŠ¤í…œì€ ë¬¸ì„œ ì¡°ê°ë¿ë§Œ ì•„ë‹ˆë¼ ì „ì²´ FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìºì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì´ì „ì— ì²˜ë¦¬ëœ ë¬¸ì„œì— ëŒ€í•œ ì„ë² ë”©ì„ ë‹¤ì‹œ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ì–´ ì´ˆê¸°í™” ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.
- **LLMì˜ ì‚¬ê³  ê³¼ì • í™•ì¸**: UIì˜ í™•ì¥ íŒ¨ë„ì„ í†µí•´ LLMì´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ê¸° ì „ì˜ ë‚´ë¶€ "ì‚¬ê³ " ê³¼ì •ì„ ë³´ì—¬ì£¼ì–´ ì¶”ë¡  ê³¼ì •ì˜ íˆ¬ëª…ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
- **ëŒ€í™”í˜• UI**: ì†ì‰¬ìš´ ë¬¸ì„œ ì—…ë¡œë“œ, ëª¨ë¸ ì„ íƒ, ì±„íŒ… ë° ë‚˜ë€íˆ ë³´ëŠ” PDF ë·°ì–´ë¥¼ ìœ„í•´ Streamlitìœ¼ë¡œ êµ¬ì¶•ëœ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.

---

## âš¡ ë¹ ë¥¸ ì‹œì‘

### ğŸ“‹ ì‚¬ì „ ì¤€ë¹„ ì‚¬í•­
- **Python**: 3.10 ì´ìƒ
- **Ollama (ì„ íƒ ì‚¬í•­)**: ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ, Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³  ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
  - ì„¤ì¹˜ëŠ” [Ollama ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://ollama.com)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
- **Gemini API í‚¤ (ì„ íƒ ì‚¬í•­)**: Gemini ëª¨ë¸ ì‚¬ìš© ì‹œ, API í‚¤ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
  - í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬ì— `.env` íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.
  - íŒŒì¼ì— `GEMINI_API_KEY="YOUR_API_KEY"` í˜•ì‹ìœ¼ë¡œ API í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
  - [Google AI Studio](https://aistudio.google.com/app/apikey)ì—ì„œ í‚¤ë¥¼ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤**: ë¡œì»¬ ëª¨ë¸ì˜ ê²½ìš°, ì¶©ë¶„í•œ RAM(ì˜ˆ: 7B ëª¨ë¸ì˜ ê²½ìš° 16GB ì´ìƒ)ì´ ê¶Œì¥ë©ë‹ˆë‹¤.

---

### ğŸ’» ì„¤ì¹˜ ë° ì‹¤í–‰

1.  **ì €ì¥ì†Œ í´ë¡ **
    ```bash
    git clone https://github.com/darkzard05/rag-system-ollama.git
    cd rag-system-ollama
    ```

2.  **(ê¶Œì¥) ê°€ìƒ í™˜ê²½ ìƒì„± ë° í™œì„±í™”**
    ```bash
    python -m venv venv
    
    # Windows
    venv\Scripts\activate
    
    # macOS/Linux
    source venv/bin/activate
    ```

3.  **í•„ìš”í•œ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜**
    ```bash
    pip install -r requirements.txt
    ```

4.  **(Ollama ì‚¬ìš©ì) ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**
    ì‹œì‘ì„ ìœ„í•´ ë²”ìš© ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë” ë§ì€ ëª¨ë¸ì€ [Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬](https://ollama.com/library)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ```bash
    ollama pull llama3:8b
    ```
    - ì´ ëª…ë ¹ì„ ì‹¤í–‰í•˜ê¸° ì „ì— Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.
    - `config.yml`ì˜ `default_ollama` ëª¨ë¸ ì´ë¦„ì´ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

5.  **Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰**
    ```bash
    streamlit run src/main.py
    ```

6.  ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:8501`ë¡œ ì ‘ì†í•˜ì—¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ êµ¬ì¡°
```
.
â”œâ”€â”€ ...
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ image/
â”‚   â””â”€â”€ image1.png
â””â”€â”€ src/
    â”œâ”€â”€ main.py
    â”œâ”€â”€ ui.py
    â”œâ”€â”€ session.py
    â”œâ”€â”€ rag_core.py
    â”œâ”€â”€ graph_builder.py  <-- ì‹ ê·œ: LangGraph êµ¬ì¡° ì •ì˜
    â”œâ”€â”€ schemas.py        <-- ì‹ ê·œ: ë°ì´í„° êµ¬ì¡°(ì˜ˆ: GraphState) ì •ì˜
    â”œâ”€â”€ config.py
    â””â”€â”€ config.yml
```
- **`main.py`**: Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì§„ì…ì ì…ë‹ˆë‹¤.
- **`ui.py`**: ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ë° ì›Œí¬í”Œë¡œìš° ì‹œê°í™”ë¥¼ í¬í•¨í•œ Streamlit UIë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
- **`session.py`**: ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„¸ì…˜ ìƒíƒœë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
- **`rag_core.py`**: ë¬¸ì„œ ì²˜ë¦¬, ì„ë² ë”©, ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±ê³¼ ê°™ì€ í•µì‹¬ RAG ë¡œì§ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- **`graph_builder.py`**: LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
- **`schemas.py`**: ê·¸ë˜í”„ ì „ì²´ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìƒíƒœ ê°ì²´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
- **`config.py` & `config.yml`**: ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

## âœ¨ ì£¼ìš” êµ¬ì„± ìš”ì†Œ

- **LangGraph**: RAG íŒŒì´í”„ë¼ì¸ì„ ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ë¡œ ì¡°ìœ¨í•©ë‹ˆë‹¤.
- **PDF ë¡œë” (PyMuPDF)**: PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤.
- **í…ìŠ¤íŠ¸ ë¶„í• ê¸° (Langchain)**: í…ìŠ¤íŠ¸ë¥¼ ê´€ë¦¬í•˜ê¸° ì‰¬ìš´ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
- **ì„ë² ë”© ëª¨ë¸ (Sentence Transformers)**: í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- **ë²¡í„° ì €ì¥ì†Œ (FAISS)**: íš¨ìœ¨ì ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•´ ì„ë² ë”©ì„ ì €ì¥í•©ë‹ˆë‹¤.
- **ë¦¬íŠ¸ë¦¬ë²„ (Ensemble)**: **BM25**(í‚¤ì›Œë“œ)ì™€ **FAISS**(ì˜ë¯¸) ê²€ìƒ‰ì„ ê²°í•©í•©ë‹ˆë‹¤.
- **LLM (Ollama & Gemini)**: ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
- **Streamlit UI**: ëŒ€í™”í˜• ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## âš™ï¸ ì„¤ì •

- **API í‚¤**: `.env` íŒŒì¼ì— `GEMINI_API_KEY`ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
- **ëª¨ë¸ ë° íŒŒë¼ë¯¸í„°**: `config.yml` íŒŒì¼ì—ì„œ ëª¨ë¸, ë¦¬íŠ¸ë¦¬ë²„ ê°€ì¤‘ì¹˜, í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • ë“±ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“„ ë¼ì´ì„ ìŠ¤
ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `LICENSE` íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.