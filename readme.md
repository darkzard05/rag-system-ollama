# RAG Chatbot with Ollama LLM  
**PDF-based Local Chatbot powered by Ollama and Streamlit**  
**PDF ê¸°ë°˜ ë¡œì»¬ ì±—ë´‡ (Ollama + Streamlit)**

![RAG Chatbot Preview](image/image1.png)

## ğŸ”‘ Key Features / ì£¼ìš” ê¸°ëŠ¥

- **PDF-based Q&A**  
  Upload your PDF documents and get answers to your questions based on their content.  
  PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  í•´ë‹¹ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **Local LLM Processing with Ollama**  
  Utilizes Ollama to run large language models locally, ensuring your data remains private and secure on your machine.  
  Ollamaë¥¼ í™œìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì‹¤í–‰í•˜ë¯€ë¡œ ë°ì´í„°ê°€ ì‚¬ìš©ì ê¸°ê¸° ë‚´ì—ì„œ ì•ˆì „í•˜ê²Œ ë¹„ê³µê°œë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.

- **Streamlit-based Web Interface**  
  A user-friendly and interactive web interface built with Streamlit for easy document upload, chatting, and PDF viewing.  
  Streamlitìœ¼ë¡œ êµ¬ì¶•ëœ ì‚¬ìš©ì ì¹œí™”ì ì´ê³  ëŒ€í™”í˜• ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ì†ì‰½ê²Œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³ , ì±„íŒ…í•˜ë©°, PDFë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **View LLM's Thinking Process**
  Option to see the thought process of the LLM before it generates an answer, providing transparency.
  LLMì´ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ì „ì˜ ì‚¬ê³  ê³¼ì •ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì˜µì…˜ì„ ì œê³µí•˜ì—¬ íˆ¬ëª…ì„±ì„ ë†’ì…ë‹ˆë‹¤.

- **Customizable Experience**
  Adjust settings like the LLM model, text chunk size, and retrieval parameters through the UI.
  UIë¥¼ í†µí•´ LLM ëª¨ë¸, í…ìŠ¤íŠ¸ ì²­í¬ í¬ê¸°, ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜ ë“±ì˜ ì„¤ì •ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## âš¡ Quick Start / ë¹ ë¥¸ ì‹œì‘

### ğŸ“‹ Prerequisites / ì‚¬ì „ ì¤€ë¹„ ì‚¬í•­
- **Python:** 3.10 or higher / Python 3.10 ì´ìƒ
- **Ollama:** Installed and the Ollama server must be running. / Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³  Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
  - Refer to the [Ollama Official Website](https://ollama.com) for installation instructions. / ì„¤ì¹˜ ì•ˆë‚´ëŠ” [Ollama ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://ollama.com)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
- **System Resources:** Sufficient RAM (e.g., 8GB+ for smaller models like `qwen3:4b`, 16GB+ for larger models) is recommended for running Ollama and the models smoothly. / Ollama ë° ëª¨ë¸ì„ ì›í™œí•˜ê²Œ ì‹¤í–‰í•˜ë ¤ë©´ ì¶©ë¶„í•œ RAM(ì˜ˆ: `qwen3:4b`ì™€ ê°™ì€ ì†Œí˜• ëª¨ë¸ì˜ ê²½ìš° 8GB ì´ìƒ, ëŒ€í˜• ëª¨ë¸ì˜ ê²½ìš° 16GB ì´ìƒ)ì´ ê¶Œì¥ë©ë‹ˆë‹¤.

---

### ğŸ’» Installation & Run / ì„¤ì¹˜ ë° ì‹¤í–‰

1. **Clone the repository / ì €ì¥ì†Œ í´ë¡ **
   ```bash
   git clone https://github.com/darkzard05/rag-system-ollama.git
   cd rag-system-ollama

2. **(Recommended) Create and activate a virtual environment / (ê¶Œì¥) ê°€ìƒ í™˜ê²½ ìƒì„± ë° í™œì„±í™”**
   ```bash
   python -m venv venv
   # Windows
   venv\Scripts\activate
   # macOS/Linux
   source venv/bin/activate
   ```

3. **Install required Python packages / í•„ìš”í•œ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜**
   ```bash
   pip install -r requirements.txt
   ```

3. Ollamaë¥¼ ì„¤ì¹˜í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤:
   - [Ollama ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://ollama.com)ì—ì„œ ì„¤ì¹˜ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì„¤ì¹˜í•˜ì„¸ìš”.
   - ì„¤ì¹˜ í›„, `ollama list` ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ í™•ì¸í•©ë‹ˆë‹¤.

4. ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:
   ```bash
   ollama pull qwen3:4b
   ```
   - `qwen3:4b` ëª¨ë¸ì€ ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ê¶Œì¥ë˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.

5. Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤:
   ```bash
   streamlit run src/main.py
   ```

6. ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:8501`ë¡œ ì ‘ì†í•˜ì—¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

## ğŸ“ Project Structure / íŒŒì¼ êµ¬ì¡°
```
readme.md
requirements.txt
image/
    image1.png
    image2.png
    image3.png
src/
    main.py
    utils.py
```
- **readme.md**: Project description file / í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì„¤ëª… íŒŒì¼ì…ë‹ˆë‹¤.
- **requirements.txt**: List of required Python packages / í•„ìš”í•œ Python íŒ¨í‚¤ì§€ ëª©ë¡ì…ë‹ˆë‹¤.
- **image/**: Folder containing project images / í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ì´ ì €ì¥ëœ í´ë”ì…ë‹ˆë‹¤.
- **src/**: Folder containing main application source code / ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì£¼ìš” ì†ŒìŠ¤ ì½”ë“œê°€ í¬í•¨ëœ í´ë”ì…ë‹ˆë‹¤.
  - **main.py**: Entry point of Streamlit application / Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì§„ì…ì ì…ë‹ˆë‹¤.
    - Handles the user interface, chat logic, session state management, and orchestrates the RAG pipeline.
    - ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤, ì±„íŒ… ë¡œì§, ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ ë° RAG íŒŒì´í”„ë¼ì¸ ì¡°ì •ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
  - **utils.py**: PDF processing and utility functions / PDF ì²˜ë¦¬ ë° ê¸°íƒ€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ê°€ í¬í•¨ëœ íŒŒì¼ì…ë‹ˆë‹¤.
    - Contains functions for loading and parsing PDF documents, splitting text into chunks, generating embeddings, managing the vector store (FAISS), and creating the QA chain with Ollama.
    - PDF ë¬¸ì„œ ë¡œë“œ ë° íŒŒì‹±, í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• , ì„ë² ë”© ìƒì„±, ë²¡í„° ì €ì¥ì†Œ(FAISS) ê´€ë¦¬, Ollamaë¥¼ ì‚¬ìš©í•œ QA ì²´ì¸ ìƒì„± ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.

## âœ¨ Key Components / ì£¼ìš” êµ¬ì„± ìš”ì†Œ

- **PDF Loader (PyMuPDF):** Loads and extracts text content from uploaded PDF files. / ì—…ë¡œë“œëœ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë¡œë“œí•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤.
- **Text Splitter (Langchain):** Divides the extracted text into smaller, manageable chunks for processing. / ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê¸° ì‰¬ìš´ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
- **Embedding Model (Sentence Transformers):** Converts text chunks into numerical vector embeddings. This project uses `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` by default (supports multiple languages). / í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ìˆ«ì ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (ë‹¤êµ­ì–´ ì§€ì›).
- **Vector Store (FAISS):** Stores these embeddings and allows for efficient similarity searches to find relevant chunks based on a user's query. / ì´ëŸ¬í•œ ì„ë² ë”©ì„ ì €ì¥í•˜ê³  ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ì²­í¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
- **Retriever:** Fetches the most relevant text chunks from the vector store based on the query. / ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„° ì €ì¥ì†Œì—ì„œ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
- **Ollama LLM:** The selected local large language model (e.g., `qwen3:4b`) generates an answer using the user's query and the retrieved context. / ì„ íƒëœ ë¡œì»¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(ì˜ˆ: `qwen3:4b`)ì´ ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
- **Streamlit UI:** Provides the interactive web interface for all user interactions. / ëª¨ë“  ì‚¬ìš©ì ìƒí˜¸ ì‘ìš©ì„ ìœ„í•œ ëŒ€í™”í˜• ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ How to Use / ì‚¬ìš© ë°©ë²•
1. Upload a PDF file in the sidebar / ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
2. (Optional) Select your preferred Ollama model from the dropdown menu in the sidebar. / (ì„ íƒ ì‚¬í•­) ì‚¬ì´ë“œë°”ì˜ ë“œë¡­ë‹¤ìš´ ë©”ë‰´ì—ì„œ ì„ í˜¸í•˜ëŠ” Ollama ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
3. Wait for the PDF to be processed. A notification will appear. / PDFê°€ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤. ì•Œë¦¼ì´ í‘œì‹œë©ë‹ˆë‹¤.
4. Enter your questions about the document content in the chat input field. / ì±„íŒ… ì…ë ¥ì°½ì— ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•©ë‹ˆë‹¤.
5. The chatbot will provide answers. You can expand the "ğŸ¤” ìƒê° ê³¼ì •" (Thinking Process) section below each answer to see the LLM's reasoning steps. / ì±—ë´‡ì´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. ê° ë‹µë³€ ì•„ë˜ì˜ "ğŸ¤” ìƒê° ê³¼ì •" ì„¹ì…˜ì„ í™•ì¥í•˜ì—¬ LLMì˜ ì¶”ë¡  ë‹¨ê³„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## âš™ï¸ Configuration / ì„¤ì •

- **LLM Model:** Selectable via the UI from models downloaded in Ollama. Default is `qwen3:4b`. / Ollamaì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ ì¤‘ UIë¥¼ í†µí•´ ì„ íƒ ê°€ëŠ¥. ê¸°ë³¸ê°’ì€ `qwen3:4b`ì…ë‹ˆë‹¤.
- **Embedding Model:** Currently set to `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (supports multiple languages) in `src/utils.py`. This constant can be changed in the code if a different embedding model is preferred. / í˜„ì¬ `src/utils.py`ì—ì„œ `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`(ë‹¤êµ­ì–´ ì§€ì›)ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì„ë² ë”© ëª¨ë¸ì„ ì„ í˜¸í•˜ëŠ” ê²½ìš° ì½”ë“œì—ì„œ ì´ ìƒìˆ˜ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ› ï¸ Technical Stack / ê¸°ìˆ  ìŠ¤íƒ

- **Programming Language:** Python 3.10+
- **LLM Orchestration:** Langchain
- **Local LLM Server:** Ollama
- **Web Framework:** Streamlit
- **Embedding Models:** Sentence Transformers (Hugging Face)
- **PDF Processing:** PyMuPDF
- **Vector Store:** FAISS (Facebook AI Similarity Search)
- **Core ML/DL:** PyTorch

## ğŸš‘ Troubleshooting / ë¬¸ì œ í•´ê²°

- **Ollama Connection Issues / Ollama ì—°ê²° ë¬¸ì œ:**
  - Ensure the Ollama application/server is running. You can test this by running `ollama list` in your terminal. / Ollama ì• í”Œë¦¬ì¼€ì´ì…˜/ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”. í„°ë¯¸ë„ì—ì„œ `ollama list`ë¥¼ ì‹¤í–‰í•˜ì—¬ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - If using Docker for Ollama, ensure the container is running and ports are correctly mapped. / Ollamaì— Dockerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì´ê³  í¬íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ë§¤í•‘ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
- **Model Not Found / ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ:**
  - Make sure you have pulled the desired model using `ollama pull <model_name>` (e.g., `ollama pull qwen3:4b`). / `ollama pull <model_name>` (ì˜ˆ: `ollama pull qwen3:4b`)ì„ ì‚¬ìš©í•˜ì—¬ ì›í•˜ëŠ” ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
  - The model name selected in the UI must match a model available in your local Ollama instance. / UIì—ì„œ ì„ íƒí•œ ëª¨ë¸ ì´ë¦„ì´ ë¡œì»¬ Ollama ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
- **Slow Performance / ëŠë¦° ì„±ëŠ¥:**
  - Processing large PDFs or using very large/complex LLM models can be resource-intensive. / ëŒ€ìš©ëŸ‰ PDFë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ ë§¤ìš° í¬ê³  ë³µì¡í•œ LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë¦¬ì†ŒìŠ¤ë¥¼ ë§ì´ ì†Œëª¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - Ensure your system meets Ollama's recommended specifications (especially RAM). / ì‹œìŠ¤í…œì´ Ollamaì˜ ê¶Œì¥ ì‚¬ì–‘(íŠ¹íˆ RAM)ì„ ì¶©ì¡±í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
  - Consider using smaller/quantized LLM models available through Ollama. / Ollamaë¥¼ í†µí•´ ì‚¬ìš© ê°€ëŠ¥í•œ ë” ì‘ê±°ë‚˜ ì–‘ìí™”ëœ LLM ëª¨ë¸ ì‚¬ìš©ì„ ê³ ë ¤í•´ ë³´ì„¸ìš”.
  - Adjusting chunk size/overlap or reducing the 'K' value for retrieval might help for very large documents. / ë§¤ìš° í° ë¬¸ì„œì˜ ê²½ìš° ì²­í¬ í¬ê¸°/ì¤‘ë³µì„ ì¡°ì •í•˜ê±°ë‚˜ ê²€ìƒ‰ì„ ìœ„í•œ 'K' ê°’ì„ ì¤„ì´ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **`torch.classes.__path__ = []` in `main.py`:**
  - This line is a workaround for a known issue with certain PyTorch and TorchVision versions. If you encounter import errors related to `torchvision.models`, this line might be the cause or solution depending on your environment.
  - ì´ ë¼ì¸ì€ íŠ¹ì • PyTorch ë° TorchVision ë²„ì „ì˜ ì•Œë ¤ì§„ ë¬¸ì œì— ëŒ€í•œ í•´ê²° ë°©ë²•ì…ë‹ˆë‹¤. `torchvision.models`ì™€ ê´€ë ¨ëœ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²½ìš°, í™˜ê²½ì— ë”°ë¼ ì´ ë¼ì¸ì´ ì›ì¸ì´ê±°ë‚˜ í•´ê²°ì±…ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¤ Contributing / ê¸°ì—¬
Contributions are welcome! If you find a bug or want to suggest a new feature, please use the issue tracker in this repository.  
ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤! ë²„ê·¸ë¥¼ ë°œê²¬í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì œì•ˆí•˜ë ¤ë©´ ì´ ì €ì¥ì†Œì˜ ì´ìŠˆ íŠ¸ë˜ì»¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

## ğŸ“„ License / ë¼ì´ì„ ìŠ¤
This project is distributed under the MIT License. See the `LICENSE` file for more details.  
ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `LICENSE` íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.
