"""
Streamlit UI ì»´í¬ë„ŒíŠ¸ ë Œë”ë§ í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ íŒŒì¼.
Clean & Minimal Version: ë¶€ê°€ ìš”ì†Œ ì œê±°, ì§ê´€ì ì¸ ë¡œë”© ë° ìŠ¤íŠ¸ë¦¬ë°.
"""

from __future__ import annotations

import logging
import os
import time
from collections.abc import Callable
from contextlib import aclosing
from typing import Any

import streamlit as st

from api.streaming_handler import get_adaptive_controller, get_streaming_handler
from common.config import (
    AVAILABLE_EMBEDDING_MODELS,
    MSG_CHAT_GUIDE,
    MSG_CHAT_INPUT_PLACEHOLDER,
    MSG_CHAT_NO_QA_SYSTEM,
    MSG_PDF_VIEWER_NO_FILE,
    UI_CONTAINER_HEIGHT,
)
from core.session import SessionManager

logger = logging.getLogger(__name__)


async def _stream_chat_response(
    rag_engine, user_query: str, chat_container
) -> dict[str, Any]:
    """
    ì ì‘í˜• ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ê³  ê³¼ì •ê³¼ ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë Œë”ë§í•©ë‹ˆë‹¤.
    """

    state: dict[str, Any] = {
        "full_response": "",
        "full_thought": "",
        "retrieved_docs": [],
        "performance": None,  # ì¶”ê°€ëœ í•„ë“œ
        "start_time": time.time(),
        "thinking_start_time": None,
        "thinking_end_time": None,
    }

    current_llm = SessionManager.get("llm")
    if not current_llm:
        return {
            "response": "âŒ ì˜¤ë¥˜: ì¶”ë¡  ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "thought": "",
            "documents": [],
        }

    run_config = {"configurable": {"llm": current_llm}}
    SessionManager.set("is_generating_answer", True)

    # í•¸ë“¤ëŸ¬ ë° ì œì–´ê¸° íšë“
    handler = get_streaming_handler()
    controller = get_adaptive_controller()

    # UI ë””ë°”ìš´ì‹± ì„¤ì •
    last_render_time = 0.0
    render_interval = 0.05

    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë Œë”ë§
    try:
        with chat_container, st.chat_message("assistant", avatar="ğŸ¤–"):
            # íŒŒì´í”„ë¼ì¸ ìƒíƒœ í‘œì‹œ (ë©”ì‹œì§€ ìµœìƒë‹¨ì— í…ìŠ¤íŠ¸ ë¡œê·¸ë¡œ ëˆ„ì )
            status_container = st.empty()
            pipeline_logs = []

            def update_pipeline_display(new_log: str):
                pipeline_logs.append(f"â””â”€ `PROCESS` {new_log}")
                status_container.markdown("  \n".join(pipeline_logs))

            # ì‚¬ê³  ê³¼ì • ë° ë‹µë³€ í‘œì‹œìš© ì»¨í…Œì´ë„ˆ
            thought_area = st.container()
            answer_area = st.empty()

            # ì ì‘í˜• ìŠ¤íŠ¸ë¦¬ë° ì ìš©
            event_generator = rag_engine.astream_events(
                {"input": user_query}, config=run_config, version="v2"
            )

            async with aclosing(
                handler.stream_graph_events(
                    event_generator, adaptive_controller=controller
                )
            ) as stream:
                async for chunk in stream:
                    # A. ìƒíƒœ ì—…ë°ì´íŠ¸ ì²˜ë¦¬ (ëˆ„ì  ë¡œê·¸ ë°©ì‹)
                    if chunk.status:
                        update_pipeline_display(chunk.status)
                        SessionManager.add_status_log(chunk.status)

                    # B. ë©”íƒ€ë°ì´í„°(ë¬¸ì„œ) ì²˜ë¦¬
                    if chunk.metadata and "documents" in chunk.metadata:
                        state["retrieved_docs"] = chunk.metadata["documents"]
                        doc_count = len(state["retrieved_docs"])
                        update_pipeline_display(f"ê´€ë ¨ ì§€ì‹ {doc_count}ê°œ í™•ë³´ ì™„ë£Œ")

                    # [ì¶”ê°€] í†µí•© ì„±ëŠ¥ ì§€í‘œ ì²˜ë¦¬
                    if chunk.performance:
                        state["performance"] = chunk.performance

                    # C. ì‚¬ê³  ê³¼ì • ì²˜ë¦¬
                    if chunk.thought:
                        # ì‚¬ê³  ê³¼ì • ì‹œì‘ ì‹œ íƒ€ì´ë° ê¸°ë¡
                        if not state["full_thought"]:
                            state["thinking_start_time"] = time.time()
                            with thought_area:
                                st.caption("AIì˜ ì‚¬ê³  íë¦„:")
                                thought_display = st.empty()

                        state["full_thought"] += chunk.thought

                        current_time = time.time()
                        if current_time - last_render_time > render_interval:
                            thought_display.markdown(f"*{state['full_thought']}*")
                            last_render_time = current_time

                    # D. ë‹µë³€ ë³¸ë¬¸ ì²˜ë¦¬
                    if chunk.content:
                        # ì²« í† í° ìˆ˜ì‹  ì‹œ íŒŒì´í”„ë¼ì¸ ë¡œê·¸ ì •ë¦¬ ë° ë‹µë³€ ì‹œì‘
                        if not state["full_response"]:
                            status_container.empty()  # ì§„í–‰ ë¡œê·¸ ì œê±° (ë‹µë³€ ì§‘ì¤‘)
                            state["thinking_end_time"] = time.time()

                            # ì‚¬ê³  ê³¼ì •ì´ ìˆì—ˆë‹¤ë©´ ì˜ˆì˜ê²Œ ë§ˆë¬´ë¦¬
                            if state["full_thought"]:
                                thinking_dur = (
                                    state["thinking_end_time"]
                                    - state["thinking_start_time"]
                                )
                                with thought_area:
                                    with st.expander(
                                        f"ğŸ’­ ì‚¬ê³  ì™„ë£Œ ({thinking_dur:.1f}ì´ˆ)",
                                        expanded=False,
                                    ):
                                        st.markdown(state["full_thought"])
                                    if "thought_display" in locals():
                                        thought_display.empty()

                        state["full_response"] += chunk.content

                        current_time = time.time()
                        if (
                            current_time - last_render_time > render_interval
                            or chunk.is_final
                        ):
                            render_start = current_time
                            answer_area.markdown(state["full_response"] + "â–Œ")

                            # ë Œë”ë§ ì„±ëŠ¥ í”¼ë“œë°±
                            render_latency = (time.time() - render_start) * 1000
                            controller.record_latency(render_latency)
                            last_render_time = time.time()

            # 2. ìµœì¢… ì •ëˆ (ì¸ìš©êµ¬, í”¼ë“œë°± ë“±)
            _finalize_ui_rendering(thought_area, answer_area, state)

            final_result = {
                "response": state["full_response"],
                "thought": state["full_thought"],
                "documents": state["retrieved_docs"],
                "performance": state["performance"], # None ëŒ€ì‹  ì‹¤ì œ ë°ì´í„° ë°˜í™˜
            }

        return final_result

    except Exception as e:
        logger.error(f"UI ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}", exc_info=True)
        from common.utils import format_error_message

        friendly_msg = format_error_message(e)

        # [ìµœì í™”] ìƒíƒœì°½ì— ì—ëŸ¬ ë©”ì‹œì§€ ì¦‰ì‹œ ë°˜ì˜
        SessionManager.add_status_log(friendly_msg)
        return {"response": friendly_msg, "thought": "", "documents": []}
    finally:
        SessionManager.set("is_generating_answer", False)


def _finalize_ui_rendering(thought_container, answer_display, state):
    """ë‹µë³€ ìƒì„±ì´ ëë‚œ í›„ UIë¥¼ ìµœì¢… ìƒíƒœë¡œ ì •ë¦¬í•©ë‹ˆë‹¤."""
    # 1. ì‚¬ê³  ê³¼ì • ì •ë¦¬
    if state["full_thought"]:
        with thought_container:
            if state.get("thinking_start_time") and state.get("thinking_end_time"):
                dur = state["thinking_end_time"] - state["thinking_start_time"]
                label = f"ğŸ§  ì‚¬ê³  ì™„ë£Œ ({dur:.1f}ì´ˆ)"
            else:
                label = f"ğŸ§  ì‚¬ê³  ì™„ë£Œ ({len(state['full_thought'].split())} tokens)"

            with st.expander(label, expanded=False):
                st.markdown(
                    f'<div class="thought-container">{state["full_thought"]}</div>',
                    unsafe_allow_html=True,
                )
    else:
        thought_container.empty()

    # 2. ë‹µë³€ ë³¸ë¬¸ ìµœì¢… ë Œë”ë§
    if state["full_response"]:
        from common.utils import apply_tooltips_to_response

        if state["retrieved_docs"]:
            final_html = apply_tooltips_to_response(
                state["full_response"], state["retrieved_docs"]
            )
            answer_display.markdown(final_html, unsafe_allow_html=True)
        else:
            answer_display.markdown(state["full_response"], unsafe_allow_html=True)

        # 2. í”¼ë“œë°± ë° ë©”íŠ¸ë¦­
        st.divider()
        c1, c2 = st.columns([0.85, 0.15])

        with c2:
            # í”¼ë“œë°± ìœ„ì ¯
            st.feedback("thumbs", key=f"fb_{int(state['start_time'])}")

        with c1:
            # í•˜ë‹¨ ë©”íŠ¸ë¦­ ìº¡ì…˜ (í†µí•© ì§€í‘œ ì‚¬ìš©)
            perf = state.get("performance")
            if perf:
                st.caption(
                    f"â±ï¸ {perf.get('total_time', 0):.1f}s (TTFT: {perf.get('ttft', 0):.2f}s) | "
                    f"ğŸš€ {perf.get('tps', 0):.1f} t/s | "
                    f"ğŸ“„ {perf.get('doc_count', 0)} refs | "
                    f"ğŸ¤– {perf.get('model_name', 'Unknown')}"
                )
            else:
                # í´ë°± (ì§€í‘œ íšë“ ì‹¤íŒ¨ ì‹œ)
                total_dur = time.time() - state["start_time"]
                st.caption(f"â±ï¸ {total_dur:.1f}s | ğŸš€ ë¶„ì„ ì™„ë£Œ")
    else:
        answer_display.error("âš ï¸ ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


def render_pdf_viewer():
    _pdf_viewer_fragment()


@st.fragment
def _pdf_viewer_fragment():
    import fitz  # PyMuPDF
    from streamlit_pdf_viewer import pdf_viewer

    # 1. ì´ë¯¸ ê³„ì‚°ëœ ë†’ì´ ê°€ì ¸ì˜¤ê¸° (í´ë°± 800)
    win_h = st.session_state.get("last_valid_height", 800)
    container_h = max(400, win_h - 250)

    # [UI ëŒ€ì¹­ì„±] ì±„íŒ…ì°½ê³¼ ë™ì¼í•˜ê²Œ í…Œë‘ë¦¬ê°€ ìˆëŠ” ì»¨í…Œì´ë„ˆ ìƒì„±
    viewer_container = st.container(height=container_h, border=True)

    # [ìˆ˜ì •] ì„¸ì…˜ ì´ˆê¸°í™” ì „ì—ë„ ì•ˆì „í•˜ë„ë¡ ê¸°ë³¸ê°’ None ì œê³µ ë° ëª…ì‹œì  ì²´í¬
    pdf_path_raw = SessionManager.get("pdf_file_path", None)

    if not pdf_path_raw:
        with viewer_container:
            st.info(MSG_PDF_VIEWER_NO_FILE)
        return

    # [ìˆ˜ì •] ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜í•˜ì—¬ ì •í™•í•œ íŒŒì¼ ì°¸ì¡° ë³´ì¥
    pdf_path = os.path.abspath(pdf_path_raw)

    if not os.path.exists(pdf_path):
        with viewer_container:
            st.error(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return

    try:
        with fitz.open(pdf_path) as doc:
            total_pages = len(doc)

            # [ìˆ˜ì •] SessionManagerì˜ ìƒíƒœë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ë°˜ì˜ (ë„¤ë¹„ê²Œì´ì…˜ ì—°ë™)
            current_page = SessionManager.get("current_page", 1)
            st.session_state.current_page = current_page

            is_generating = SessionManager.get("is_generating_answer", False) or False

            # 1. PDF ë·°ì–´ ë©”ì¸ ì˜ì—­
            with viewer_container:
                pdf_viewer(
                    input=pdf_path,
                    height=UI_CONTAINER_HEIGHT,
                    pages_to_render=[current_page],
                )

            # 2. ì„¸ë ¨ëœ ë²„íŠ¼ ê·¸ë£¹í˜• íƒìƒ‰ íˆ´ë°”
            # ë¹„ìœ¨ ì¡°ì •: [ì´ì „|ë‹¤ìŒ | í˜ì´ì§€ì •ë³´ | ìŠ¬ë¼ì´ë”]
            # [ìˆ˜ì •] ìƒë‹¨ ì—¬ë°± ì œê±°í•˜ì—¬ ì±„íŒ…ì°½ ë°”ë‹¥ê³¼ ë†’ì´ ì •ë ¬
            st.markdown("<div style='margin-top: -10px;'></div>", unsafe_allow_html=True)
            c1, c2, c3, c4 = st.columns([4.0, 1.2, 0.4, 0.4])

            with c1:
                # ìš°ì¸¡ì˜ ë„“ì€ ê³µê°„ì„ ì°¨ì§€í•˜ëŠ” ìŠ¬ë¼ì´ë”
                # [ìˆ˜ì •] keyê°€ ìˆìœ¼ë¯€ë¡œ value ì¸ìëŠ” ì œê±° (ì¤‘ë³µ ì„¤ì • ë°©ì§€)
                new_page = st.slider(
                    "page_nav_wide",
                    min_value=1,
                    max_value=total_pages,
                    key="pdf_nav_slider_wide",
                    disabled=is_generating,
                    label_visibility="collapsed",
                )
                # ìŠ¬ë¼ì´ë” ì¡°ì‘ ì‹œ current_page ë™ê¸°í™”
                if new_page != st.session_state.current_page:
                    st.session_state.current_page = new_page
                    st.rerun()

            with c2:
                # í˜ì´ì§€ ì •ë³´ë¥¼ ë²„íŠ¼ ë°”ë¡œ ì˜†ì— ë°°ì¹˜
                st.markdown(
                    f"<div style='text-align: center; line-height: 2.3rem; font-family: monospace; font-size: 0.95rem; color: #888;'>"
                    f"<span style='color: #0068c9; font-weight: bold;'>{st.session_state.current_page}</span> / {total_pages}"
                    f"</div>",
                    unsafe_allow_html=True,
                )

            with c3:
                if st.button(
                    "â€¹",
                    use_container_width=True,
                    disabled=(st.session_state.current_page <= 1 or is_generating),
                    key="btn_pdf_prev_grp",
                    help="ì´ì „ í˜ì´ì§€",
                ):
                    st.session_state.current_page -= 1
                    # ìŠ¬ë¼ì´ë” ìƒíƒœë„ í•¨ê»˜ ì—…ë°ì´íŠ¸
                    st.session_state.pdf_nav_slider_wide = st.session_state.current_page
                    st.rerun()

            with c4:
                if st.button(
                    "â€º",
                    use_container_width=True,
                    disabled=(
                        st.session_state.current_page >= total_pages or is_generating
                    ),
                    key="btn_pdf_next_grp",
                    help="ë‹¤ìŒ í˜ì´ì§€",
                ):
                    st.session_state.current_page += 1
                    # ìŠ¬ë¼ì´ë” ìƒíƒœë„ í•¨ê»˜ ì—…ë°ì´íŠ¸
                    st.session_state.pdf_nav_slider_wide = st.session_state.current_page
                    st.rerun()

    except Exception as e:
        with viewer_container:
            st.error(f"PDF ì˜¤ë¥˜: {e}")


def inject_custom_css():
    """ì•± ì „ë°˜ì— ê±¸ì¹œ ìµœì†Œí•œì˜ ì»¤ìŠ¤í…€ CSSë§Œ ì£¼ì…í•©ë‹ˆë‹¤."""
    st.markdown(
        """
    <style>
    /* 1. ì „ì²´ ì•± í™”ë©´ ê³ ì • ë° ì „ì—­ ìŠ¤í¬ë¡¤ ì°¨ë‹¨ */
    [data-testid="stAppViewContainer"] {
        height: 100vh !important;
        overflow: hidden !important;
    }
    
    /* 2. ë©”ì¸ ì˜ì—­ ë° ì‚¬ì´ë“œë°” íŒ¨ë”© ë° ë†’ì´ ìµœì í™” */
    [data-testid="stMainBlockContainer"] {
        height: 100vh !important;
        padding-top: 2rem !important; /* ìƒë‹¨ ì—¬ë°± í†µì¼ */
        padding-bottom: 0rem !important;
        overflow: hidden !important;
    }

    [data-testid="stSidebarContent"] {
        padding-top: 2rem !important; /* ìƒë‹¨ ì—¬ë°± í†µì¼ */
    }

    /* 3. ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ìŠ¤í¬ë¡¤ í™œì„±í™” */
    [data-testid="stVerticalBlockBorderWrapper"] > div:nth-child(1) {
        height: 100% !important;
        overflow-y: auto !important;
    }

    /* 4. JS ì¸¡ì •ê¸° ë“± ì»¤ìŠ¤í…€ ì»´í¬ë„ŒíŠ¸ì˜ ìœ ë ¹ ê³µê°„ ì œê±° */
    div[data-testid="stHtml"] iframe, 
    div.element-container:has(iframe[title="streamlit_javascript.st_javascript"]),
    div.stMarkdown:has(iframe[title="streamlit_javascript.st_javascript"]) {
        position: absolute !important;
        top: -9999px !important;
        left: -9999px !important;
        width: 0 !important;
        height: 0 !important;
        margin: 0 !important;
        padding: 0 !important;
        overflow: hidden !important;
        visibility: hidden !important;
        display: none !important; /* ë¸Œë¼ìš°ì €ì— ë”°ë¼ ì‹¤í–‰ì´ ì•ˆë  ìˆ˜ ìˆìœ¼ë‚˜ ë¨¼ì € ì‹œë„ */
    }
    
    /* 5. ìƒë‹¨ ì„œë¸Œí—¤ë” ë° ì‚¬ì´ë“œë°” ì œëª© ì •ë ¬ */
    h3 {
        height: auto !important;
        line-height: 1.5 !important;
        margin-bottom: 1.2rem !important;
        padding-top: 0.2rem !important; /* ìƒë‹¨ ì—¬ë°± ì†Œí­ ì¡°ì • */
        margin-top: 0rem !important;
    }
    
    [data-testid="stSidebar"] h1 {
        font-size: 1.8rem !important;
        margin-top: 0rem !important;
        padding-top: 0rem !important;
        margin-bottom: 0.5rem !important;
    }

    [data-testid="stSidebar"] [data-testid="stVerticalBlock"] {
        gap: 0.5rem;
        padding-top: 0rem !important;
    }
    
    /* 6. íˆ´íŒ ë° ì¸ìš© ë°°ì§€ ìŠ¤íƒ€ì¼ */
    .tooltip {
        position: relative;
        display: inline-block;
        color: #0068c9;
        font-weight: 600;
        cursor: help;
        text-decoration: underline dotted;
    }
    /* 6. ì¸ìš© ë°°ì§€ ìŠ¤íƒ€ì¼ */
    .citation-badge {
        display: inline-flex !important;
        align-items: center;
        justify-content: center;
        background-color: #f0f2f6 !important;
        color: #0068c9 !important;
        font-size: 0.75rem !important;
        font-weight: bold !important;
        padding: 0 6px !important;
        margin: 0 2px !important;
        border-radius: 4px !important;
        border: 1px solid #d1d5db !important;
        vertical-align: middle !important;
        height: 1.2rem !important;
        user-select: none;
    }
    </style>
    </style>
    """,
    unsafe_allow_html=True,
)



def render_sidebar(
    file_uploader_callback: Callable,
    model_selector_callback: Callable,
    embedding_selector_callback: Callable,
    is_generating: bool = False,
    current_file_name: str | None = None,
    current_embedding_model: str | None = None,
    available_models: list[str] | None = None,
):
    with st.sidebar:
        # 1. ë¸Œëœë“œ í—¤ë”
        st.title("ğŸ¤– GraphRAG")
        st.caption("Local Inference Model")
        st.divider()

        # 2. ë¬¸ì„œ ê´€ë¦¬
        st.subheader("ğŸ“„ Document")
        st.file_uploader(
            "Upload PDF",
            type="pdf",
            key="pdf_uploader",
            on_change=file_uploader_callback,
            disabled=is_generating,
            label_visibility="collapsed",
        )
        if current_file_name:
            st.success(f"Active: {current_file_name}")

        st.divider()

        # 3. ëª¨ë¸ ì„¤ì •
        st.subheader("âš™ï¸ Model Settings")
        from common.config import DEFAULT_OLLAMA_MODEL

        if available_models is None:
            st.info("Loading...")
        else:
            is_ollama_error = (
                available_models[0] == "Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•ŠìŠµë‹ˆë‹¤."
                if available_models
                else False
            )
            actual_models = (
                []
                if is_ollama_error
                else [m for m in available_models if "---" not in m]
            )

            last_model = SessionManager.get("last_selected_model")
            if not last_model or (actual_models and last_model not in actual_models):
                last_model = (
                    DEFAULT_OLLAMA_MODEL
                    if DEFAULT_OLLAMA_MODEL in actual_models
                    else (actual_models[0] if actual_models else available_models[0])
                )
                SessionManager.set("last_selected_model", last_model)

            st.selectbox(
                "ì¶”ë¡  ëª¨ë¸ (Inference)",
                available_models,
                index=available_models.index(last_model)
                if last_model in available_models
                else 0,
                key="model_selector",
                on_change=model_selector_callback,
                disabled=is_ollama_error or is_generating,
                label_visibility="collapsed",
            )

        with st.expander("ğŸ› ï¸ Advanced Settings"):
            last_emb = current_embedding_model or AVAILABLE_EMBEDDING_MODELS[0]
            st.selectbox(
                "ì„ë² ë”© ëª¨ë¸ (Embedding)",
                AVAILABLE_EMBEDDING_MODELS,
                index=AVAILABLE_EMBEDDING_MODELS.index(last_emb)
                if last_emb in AVAILABLE_EMBEDDING_MODELS
                else 0,
                key="embedding_model_selector",
                on_change=embedding_selector_callback,
                disabled=is_generating or (available_models is None),
            )


def render_left_column():
    _chat_fragment()


def render_message(
    role: str,
    content: str,
    thought: str | None = None,
    doc_ids: list[Any] | None = None,
    metrics: dict | None = None,
    processed_content: str | None = None,
):
    if role == "system":
        with st.chat_message("system", avatar="âš™ï¸"):
            st.caption("ì‹œìŠ¤í…œ ì‘ì—… ê¸°ë¡")
            st.markdown(content)
        return

    avatar_icon = "ğŸ¤–" if role == "assistant" else "ğŸ‘¤"
    with st.chat_message(role, avatar=avatar_icon):
        if thought and thought.strip():
            with st.expander("ğŸ§  ì‚¬ê³  ì™„ë£Œ", expanded=False):
                st.markdown(
                    f'<div class="thought-container">{thought}</div>',
                    unsafe_allow_html=True,
                )

        # [ìµœì í™”] ê°€ê³µëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ì‚¬ìš© (ì •ê·œì‹ ì—°ì‚° ìƒëµ)
        if processed_content:
            st.markdown(processed_content, unsafe_allow_html=True)
        else:
            # Fallback: ê°€ê³µë˜ì§€ ì•Šì€ ê²½ìš° (ì£¼ë¡œ ìœ ì € ë©”ì‹œì§€ ë˜ëŠ” ì´ì „ ë²„ì „ ë°ì´í„°)
            display_text = content
            if role == "assistant":
                from common.utils import (
                    apply_tooltips_to_response,
                    normalize_latex_delimiters,
                )
                display_text = normalize_latex_delimiters(display_text)

                if doc_ids:
                    doc_pool = SessionManager.get("doc_pool", {}) or {}
                    documents = [doc_pool[d_id] for d_id in doc_ids if d_id in doc_pool]
                    if documents:
                        display_text = apply_tooltips_to_response(display_text, documents)

            st.markdown(display_text, unsafe_allow_html=True)

        # [ì¶”ê°€] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° í”¼ë“œë°± ì„¹ì…˜
        if role == "assistant":
            m_col1, m_col2 = st.columns([0.7, 0.3])

            with m_col2:
                # ê³ ìœ  í‚¤ ìƒì„±ì„ ìœ„í•´ ë‚´ìš©ì˜ í•´ì‹œ ì‚¬ìš©
                import hashlib

                msg_hash = hashlib.md5(content.encode(), usedforsecurity=False).hexdigest()[:8]
                st.feedback("thumbs", key=f"fb_hist_{msg_hash}")

            with m_col1:
                if metrics:
                    # í†µí•© ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ìº¡ì…˜ (TTFT, TPS ë“± ëª¨ë“  ì •ë³´ í¬í•¨)
                    st.caption(
                        f"â±ï¸ {metrics.get('total_time', 0):.1f}s (TTFT: {metrics.get('ttft', 0):.2f}s) | "
                        f"ğŸš€ {metrics.get('tps', 0):.1f} t/s | "
                        f"ğŸ“„ {metrics.get('doc_count', 0)} refs | "
                        f"ğŸ¤– {metrics.get('model_name', 'Unknown')}"
                    )


def update_window_height():
    """JavaScriptë¥¼ í†µí•´ ë¸Œë¼ìš°ì € ì°½ì˜ ì‹¤ì œ ë†’ì´ë¥¼ ì¸¡ì •í•˜ê³  ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤."""
    from streamlit_javascript import st_javascript

    # ìœˆë„ìš° ì „ì²´ ë†’ì´ íšë“ (ë‹¨ í•œ ë²ˆë§Œ í˜¸ì¶œë¨)
    win_h = st_javascript("window.innerHeight", key="height_tracker")

    if win_h and win_h > 100:
        st.session_state.last_valid_height = int(win_h)


def _chat_fragment():
    # 1. ì´ë¯¸ ê³„ì‚°ëœ ë†’ì´ ê°€ì ¸ì˜¤ê¸° (í´ë°± 700)
    win_h = st.session_state.get("last_valid_height", 800)
    container_h = max(400, win_h - 250) # ìƒí•˜ë‹¨ ì—¬ë°± ì œì™¸

    chat_container = st.container(height=container_h, border=True)
    # [ìˆ˜ì •] ì„¸ì…˜ ì´ˆê¸°í™” ì „ì—ë„ ì•ˆì „í•˜ë„ë¡ ê¸°ë³¸ê°’ [] ì œê³µ
    messages = SessionManager.get_messages() or []
    pdf_path = SessionManager.get("pdf_file_path")
    pdf_processed = SessionManager.get("pdf_processed", False)
    is_generating = bool(st.session_state.get("is_generating_answer", False))

    # ë¬¸ì„œ ë¶„ì„ ì¤‘ì¸ì§€ íŒë³„ (íŒŒì¼ì€ ì—…ë¡œë“œëëŠ”ë° ì•„ì§ ì²˜ë¦¬ê°€ ì•ˆ ëœ ìƒíƒœ)
    is_processing_pdf = bool(pdf_path and not pdf_processed)

    # 1. ì±„íŒ… ì´ë ¥ ë Œë”ë§
    with chat_container:
        system_buffer = []

        def flush_system_buffer():
            if not system_buffer:
                return

            with st.chat_message("system", avatar="âš™ï¸"):
                log_items = []
                is_ready = False
                has_error = False
                chars_to_remove = ["âœ…", "â³", "âŒ", "âš™ï¸", "ğŸ“„", "â„¹ï¸", "ğŸ§ ", "âœ¨", "ğŸ”„", "â³", "ğŸ¯"]
                for m in system_buffer:
                    if m == "READY_FOR_QUERY":
                        is_ready = True
                        continue
                    if "âŒ" in m or "ì˜¤ë¥˜" in m or "ì‹¤íŒ¨" in m:
                        has_error = True
                    clean_m = m
                    for char in chars_to_remove:
                        clean_m = clean_m.replace(char, "")
                    clean_m = clean_m.strip()
                    if clean_m:
                        log_items.append(f"â””â”€ {'`ERROR`' if has_error else '`SUCCESS`'} {clean_m}")
                # ê²°ê³¼ ì¶œë ¥ ë¡œì§ ìµœì í™”
                if is_ready and not has_error:
                    # ëª¨ë‘ ì„±ê³µí–ˆë‹¤ë©´ ìš”ì•½ ë©”ì‹œì§€ë§Œ í‘œì‹œ
                    st.markdown("**ì‹œìŠ¤í…œ êµ¬ì„± ë° ë°ì´í„° ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.**")
                    st.markdown("\n**ì´ì œ ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!**")
                else:
                    # ì§„í–‰ ì¤‘ì´ê±°ë‚˜ ì—ëŸ¬ê°€ ìˆë‹¤ë©´ ìƒì„¸ ë¡œê·¸ í‘œì‹œ
                    st.markdown("**ì‹œìŠ¤í…œ ì‘ì—… ê¸°ë¡**\n")
                    st.markdown("  \n".join(log_items))
            system_buffer.clear()

        for msg in messages:
            if msg["role"] == "system":
                system_buffer.append(msg["content"])
            else:
                # ì¼ë°˜ ë©”ì‹œì§€ê°€ ë‚˜ì˜¤ê¸° ì „ì— ë²„í¼ì— ìŒ“ì¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë“¤ì„ ë¨¼ì € ì¶œë ¥
                flush_system_buffer()
                render_message(
                    msg["role"],
                    msg["content"],
                    thought=msg.get("thought"),
                    doc_ids=msg.get("doc_ids"),
                    metrics=msg.get("metrics"),
                    processed_content=msg.get("processed_content"),
                )

        # ë°˜ë³µë¬¸ ì¢…ë£Œ í›„ ë‚¨ì•„ìˆëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì²˜ë¦¬
        flush_system_buffer()

        if not messages:
            # ì‹œìŠ¤í…œ ì˜¨ë³´ë”© ê°€ì´ë“œ (âš™ï¸) - ì„¤ì • íŒŒì¼ì—ì„œ í†µí•©ëœ ê°€ì´ë“œ ë©”ì‹œì§€ ì‚¬ìš©
            with st.chat_message("system", avatar="âš™ï¸"):
                st.caption("ğŸš€ RAG System Quick Start")
                st.markdown(MSG_CHAT_GUIDE)

    # 2. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    # ì…ë ¥ì°½ ìƒíƒœ ê²°ì •
    input_disabled = is_generating or is_processing_pdf
    input_placeholder = (
        "ë¬¸ì„œ ë¶„ì„ ì¤‘ì—ëŠ” ì§ˆë¬¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤..."
        if is_processing_pdf
        else (
            "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."
            if is_generating
            else MSG_CHAT_INPUT_PLACEHOLDER
        )
    )

    # [ì¶”ê°€] ì¶”ì²œ ì§ˆë¬¸ ë²„íŠ¼ í´ë¦­ ì²˜ë¦¬ ë° ì¼ë°˜ ì…ë ¥ í†µí•©
    user_query = st.chat_input(input_placeholder, disabled=input_disabled, key="chat_input_clean")

    # ë²„íŠ¼ í´ë¦­ ë“±ìœ¼ë¡œ ëŒ€ê¸° ì¤‘ì¸ ì§ˆë¬¸ì´ ìˆë‹¤ë©´ ìš°ì„  ì²˜ë¦¬
    if "pending_query" in st.session_state and st.session_state.pending_query:
        user_query = st.session_state.pending_query
        del st.session_state.pending_query # ì²˜ë¦¬ í›„ ì‚­ì œ

    if user_query:
        SessionManager.add_message("user", user_query)
        SessionManager.add_status_log("ì§ˆë¬¸ ë¶„ì„ ì¤‘")

        # UI ì¦‰ì‹œ ì—…ë°ì´íŠ¸
        with chat_container:
            render_message("user", user_query)

        # RAG ì—”ì§„ í˜¸ì¶œ
        rag_engine = SessionManager.get("rag_engine")
        if rag_engine:
            from common.utils import sync_run

            result = sync_run(
                _stream_chat_response(rag_engine, user_query, chat_container)
            )

            final_answer = result.get("response", "")
            final_thought = result.get("thought", "")
            final_docs = result.get("documents", [])
            final_metrics = result.get("performance") # ë°±ì—”ë“œì—ì„œ ê³„ì‚°ëœ í†µí•© ë©”íŠ¸ë¦­

            if final_answer and not final_answer.startswith("âŒ"):
                # [ìµœì í™”] ì„¸ì…˜ ì €ì¥ ì „ ë¯¸ë¦¬ ê°€ê³µ (UI ìºì‹±ìš©)
                from common.utils import (
                    apply_tooltips_to_response,
                    normalize_latex_delimiters,
                )
                processed = normalize_latex_delimiters(final_answer)
                if final_docs:
                    processed = apply_tooltips_to_response(processed, final_docs)

                SessionManager.add_message(
                    "assistant",
                    final_answer,
                    processed_content=processed,
                    thought=final_thought,
                    documents=final_docs,
                    metrics=final_metrics,
                )
                SessionManager.replace_last_status_log("ë‹µë³€ ì‘ì„± ì™„ë£Œ")
                SessionManager.add_status_log("ì§ˆë¬¸ ê°€ëŠ¥")
                st.rerun()
        else:
            with chat_container:
                st.error(f"âš ï¸ {MSG_CHAT_NO_QA_SYSTEM}")
