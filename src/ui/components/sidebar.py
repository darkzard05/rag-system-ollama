"""
ì‚¬ì´ë“œë°” ì„¤ì • ë° ê´€ë¦¬ ì»´í¬ë„ŒíŠ¸.
"""

import streamlit as st

from common.config import (
    AVAILABLE_EMBEDDING_MODELS,
    DEFAULT_EMBEDDING_MODEL,
    DEFAULT_OLLAMA_MODEL,
    OLLAMA_NUM_CTX,
)
from core.session import SessionManager


def render_sidebar(
    file_uploader_callback,
    model_selector_callback,
    embedding_selector_callback,
    is_generating=False,
    current_file_name=None,
    current_embedding_model=None,
    available_models=None,
):
    with st.sidebar:
        st.header("ğŸ¤– GraphRAG-Ollama")

        # 1. ë¬¸ì„œ ì²˜ë¦¬ ì„¹ì…˜
        with st.container(border=True):
            st.subheader("ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")
            st.file_uploader(
                "PDF íŒŒì¼ ì—…ë¡œë“œ",
                type="pdf",
                key="pdf_uploader",
                on_change=file_uploader_callback,
                disabled=is_generating,
                label_visibility="collapsed",
            )
            if current_file_name:
                st.caption(f"í˜„ì¬ íŒŒì¼: :green[{current_file_name}]")

        # 2. í†µí•© ëª¨ë¸ ì„¤ì • ì„¹ì…˜
        with st.container(border=True):
            st.subheader("âš™ï¸ ëª¨ë¸ ì„¤ì •")

            # --- ëª¨ë¸ ëª©ë¡ í•„í„°ë§ ë° ë¶„ë¥˜ ---
            raw_models = [m for m in (available_models or []) if "---" not in m]

            # ì„ë² ë”© ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜
            embed_keywords = ["embed", "bge", "nomic", "mxbai", "snowflake"]

            # [ì§€ëŠ¥í˜• ë¶„ë¥˜]
            # 1. ì„ë² ë”© ëª¨ë¸ ëª©ë¡: í‚¤ì›Œë“œ ë§¤ì¹­ + ê¸°ë³¸ ì„ë² ë”© ëª¨ë¸
            embedding_candidates = [
                m for m in raw_models if any(kw in m.lower() for kw in embed_keywords)
            ]
            actual_embeddings = sorted(
                set(AVAILABLE_EMBEDDING_MODELS + embedding_candidates)
            )
            if DEFAULT_EMBEDDING_MODEL not in actual_embeddings:
                actual_embeddings.append(DEFAULT_EMBEDDING_MODEL)
            actual_embeddings.sort()

            # 2. LLM ëª¨ë¸ ëª©ë¡: ì„ë² ë”©ì´ ì•„ë‹Œ ê²ƒ + ê¸°ë³¸ LLM ëª¨ë¸
            llm_candidates = [m for m in raw_models if m not in embedding_candidates]
            actual_llms = llm_candidates if llm_candidates else [DEFAULT_OLLAMA_MODEL]
            if DEFAULT_OLLAMA_MODEL not in actual_llms:
                actual_llms.append(DEFAULT_OLLAMA_MODEL)
            actual_llms.sort()

            # --- A. ë‹µë³€ ìƒì„± ëª¨ë¸ (LLM) ---
            st.write("**ğŸ’¬ ë‹µë³€ ìƒì„± ëª¨ë¸ (LLM)**")

            last_model = (
                SessionManager.get("last_selected_model") or DEFAULT_OLLAMA_MODEL
            )
            if last_model not in actual_llms:
                last_model = actual_llms[0]

            try:
                def_idx = actual_llms.index(last_model)
            except ValueError:
                def_idx = 0

            st.selectbox(
                "LLM ì„ íƒ",
                actual_llms,
                index=def_idx,
                key="model_selector",
                on_change=model_selector_callback,
                disabled=is_generating,
                label_visibility="collapsed",
            )

            st.markdown("<div style='margin-top: 15px;'></div>", unsafe_allow_html=True)

            # --- B. ì§€ì‹ ë¶„ì„ ëª¨ë¸ (Embedding) ---
            st.write("**ğŸ” ì§€ì‹ ë¶„ì„ ëª¨ë¸ (Embedding)**")

            current_emb = (
                SessionManager.get("last_selected_embedding_model")
                or DEFAULT_EMBEDDING_MODEL
            )
            if current_emb not in actual_embeddings:
                current_emb = actual_embeddings[0]

            try:
                emb_idx = actual_embeddings.index(current_emb)
            except ValueError:
                emb_idx = 0

            st.selectbox(
                "ì„ë² ë”© ì„ íƒ",
                actual_embeddings,
                index=emb_idx,
                key="embedding_model_selector",
                on_change=embedding_selector_callback,
                disabled=is_generating or (available_models is None),
                label_visibility="collapsed",
            )

        # 3. ì¶”ê°€ ë„êµ¬/ìƒíƒœ ì„¹ì…˜ (í•„ìš”ì‹œ)
        with st.expander("ğŸ› ï¸ ê³ ê¸‰ ì„¤ì • ë° ë„êµ¬", expanded=False):
            if st.button("ğŸ—‘ï¸ VRAM ìºì‹œ ë¹„ìš°ê¸°", use_container_width=True):
                from core.model_loader import ModelManager

                ModelManager.clear_vram()
                st.toast("VRAMì´ ì„±ê³µì ìœ¼ë¡œ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

            st.caption("ë¦¬ë­ì»¤: qwen3 (ì§€ëŠ¥í˜• ì±„ì )")
            st.caption(f"ì»¨í…ìŠ¤íŠ¸: {OLLAMA_NUM_CTX} tokens")

            st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html=True)
            if st.button(
                "ğŸ—‘ï¸ ìºì‹œ ë° ì„¸ì…˜ ì´ˆê¸°í™”", use_container_width=True, type="secondary"
            ):
                SessionManager.reset_all_state()
                st.rerun()
