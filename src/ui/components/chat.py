"""
ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ë° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ê´€ë ¨ ì»´í¬ë„ŒíŠ¸.
"""

import logging
import re
import time
from contextlib import aclosing
from typing import Any

import streamlit as st

from api.streaming_handler import get_adaptive_controller, get_streaming_handler
from common.config import (
    MSG_CHAT_GUIDE,
    MSG_CHAT_INPUT_PLACEHOLDER,
    MSG_CHAT_NO_QA_SYSTEM,
)
from common.utils import (
    apply_tooltips_to_response,
    format_error_message,
    normalize_latex_delimiters,
    sync_run,
)
from core.session import SessionManager

logger = logging.getLogger(__name__)


async def _stream_chat_response(
    rag_engine, user_query: str, chat_container
) -> dict[str, Any]:
    """
    ì ì‘í˜• ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ê³  ê³¼ì •ê³¼ ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë Œë”ë§í•©ë‹ˆë‹¤.
    """

    state: dict[str, Any] = {
        "full_response": "",
        "full_thought": "",
        "retrieved_docs": [],
        "performance": None,
        "start_time": time.time(),
        "thinking_start_time": None,
        "thinking_end_time": None,
    }

    current_llm = SessionManager.get("llm")
    if not current_llm:
        return {
            "response": "âŒ ì˜¤ë¥˜: ì¶”ë¡  ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "thought": "",
            "documents": [],
        }

    # [Lazy Import]
    from core.rag_core import RAGSystem

    rag_sys = RAGSystem(session_id=SessionManager.get_session_id())

    SessionManager.set("is_generating_answer", True)

    handler = get_streaming_handler()
    controller = get_adaptive_controller()

    last_render_time = 0.0
    render_interval = 0.05

    try:
        with chat_container, st.chat_message("assistant", avatar="ğŸ¤–"):
            # [ìˆ˜ì •] ë¬¸ì„œ ë¶„ì„ íƒ€ì„ë¼ì¸ê³¼ ë™ì¼í•œ ìŠ¤íƒ€ì¼ì˜ ì‹¤ì‹œê°„ ìƒíƒœ ë°•ìŠ¤
            status_placeholder = st.empty()
            live_logs = []

            def update_status(msg: str, is_complete=False):
                if msg not in live_logs:
                    live_logs.append(msg)

                status_icon = "âœ…" if is_complete else "ğŸš€"
                # ìƒíƒœ ë©”ì‹œì§€ì—ì„œ 'íŒŒì´í”„ë¼ì¸ ê°€ë™ ì¤‘:' ì ‘ë‘ì‚¬ë¥¼ ì œê±°í•˜ì—¬ ë” ê¹”ë”í•˜ê²Œ í‘œì‹œ
                current_label = msg.split(": ", 1)[-1] if ": " in msg else msg
                expander_title = f"{status_icon} {current_label}"

                lines = "".join(
                    [
                        f"<div style='font-size: 0.85rem; color: var(--text-color); margin-bottom: 8px; display: flex; align-items: flex-start; line-height: 1.5; opacity: 0.9;'>"
                        f"<span style='color: #1e88e5; margin-right: 10px; font-weight: bold;'>â–¹</span>"
                        f"<span>{item}</span></div>"
                        for item in live_logs
                    ]
                )

                timeline_html = (
                    f"<div style='margin-bottom: 15px;'>"
                    f"<details {'open' if not is_complete else ''} class='timeline-container' style='border: 1px solid rgba(128,128,128,0.2); border-radius: 8px; padding: 10px;'>"
                    f"<summary class='timeline-summary' style='font-weight: 600; color: var(--text-color); cursor: pointer; list-style: none; display: flex; align-items: center; padding: 5px 0;'>"
                    f"{expander_title}</summary>"
                    f"<div style='margin-top: 12px; padding: 15px; background-color: rgba(128,128,128,0.05); border-radius: 8px; border-left: 3px solid #1e88e5;'>"
                    f"<div style='font-size: 0.75rem; color: var(--text-color); opacity: 0.6; margin-bottom: 12px; font-weight: 700; text-transform: uppercase; letter-spacing: 0.5px;'>"
                    f"â±ï¸ Live Pipeline Execution"
                    f"</div>{lines}</div></details></div>"
                )
                status_placeholder.markdown(timeline_html, unsafe_allow_html=True)

            thought_area = st.container()
            answer_area = st.empty()

            # [í•µì‹¬] ë£¨í”„ ì§„ì… ì „ ì´ˆê¸° ìƒíƒœ ì¦‰ì‹œ í‘œì‹œ
            update_status("ğŸš€ ì§ˆë¬¸ ë¶„ì„ ë° íŒŒì´í”„ë¼ì¸ ê°€ë™ ì¤‘...")

            # RAGSystem ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ íšë“
            event_generator = await rag_sys.astream_events(user_query, llm=current_llm)

            async with aclosing(  # type: ignore[type-var]
                handler.stream_graph_events(
                    event_generator, adaptive_controller=controller
                )
            ) as stream:
                async for chunk in stream:
                    if chunk.status:
                        # [ê°œì„ ] ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ê°€ ì œê³µí•˜ëŠ” ìƒì„¸ ìƒíƒœë¥¼ ê·¸ëŒ€ë¡œ í™œìš©
                        update_status(chunk.status)
                        SessionManager.add_status_log(chunk.status)

                    if chunk.metadata and "documents" in chunk.metadata:
                        state["retrieved_docs"] = chunk.metadata["documents"]
                        doc_msg = f"ğŸ“š ê´€ë ¨ ì§€ì‹ {len(state['retrieved_docs'])}ê°œ í™•ë³´ ë° ê²€ì¦ ì™„ë£Œ"
                        SessionManager.add_status_log(doc_msg)
                        update_status(doc_msg)

                    if chunk.performance:
                        state["performance"] = chunk.performance

                    if chunk.thought:
                        if not state["full_thought"]:
                            state["thinking_start_time"] = time.time()
                            thought_msg = "ğŸ§  AIê°€ ìµœì ì˜ ë‹µë³€ ë…¼ë¦¬ë¥¼ ì„¤ê³„ ì¤‘..."
                            SessionManager.add_status_log(thought_msg)
                            update_status(thought_msg)
                            with thought_area:
                                st.caption("AI of Thought:")
                                thought_display = st.empty()
                        state["full_thought"] += chunk.thought
                        if time.time() - last_render_time > render_interval:
                            thought_display.markdown(f"*{state['full_thought']}*")
                            last_render_time = time.time()

                    if chunk.content:
                        if not state["full_response"]:
                            gen_msg = "âœï¸ ì§€ì‹ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ë‹µë³€ ì‘ì„± ì‹œì‘"
                            SessionManager.add_status_log(gen_msg)
                            update_status(gen_msg, is_complete=True)
                            state["thinking_end_time"] = time.time()
                            if state["full_thought"]:
                                with thought_area:
                                    thinking_dur = (
                                        state["thinking_end_time"]
                                        - state["thinking_start_time"]
                                    )
                                    with st.expander(
                                        f"ğŸ’­ ì‚¬ê³  ì™„ë£Œ ({thinking_dur:.1f}ì´ˆ)",
                                        expanded=False,
                                    ):
                                        st.markdown(
                                            f'<div class="thought-container">{state["full_thought"]}</div>',
                                            unsafe_allow_html=True,
                                        )
                                    if "thought_display" in locals():
                                        thought_display.empty()

                        state["full_response"] += chunk.content
                        if (
                            time.time() - last_render_time > render_interval
                            or chunk.is_final
                        ):
                            display_text = _clean_response_redundancy(
                                state["full_response"]
                            )
                            display_text = normalize_latex_delimiters(display_text)
                            cursor = "â–Œ" if not chunk.is_final else ""
                            answer_area.markdown(display_text + cursor)
                            last_render_time = time.time()

            # [ìµœì í™”] ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜ (ì¤‘ë³µ ë Œë”ë§ ì œê±°ë¡œ ì§€ì—° ìµœì†Œí™”)
            SessionManager.add_status_log("âœ¨ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            cleaned_final = _clean_response_redundancy(state["full_response"])
            processed_final = apply_tooltips_to_response(
                cleaned_final, state["retrieved_docs"]
            )

            # --- [ì¶”ê°€] ìë™ ì í”„ íŠ¸ë¦¬ê±° ì„¤ì • ---
            if state["retrieved_docs"]:
                try:
                    # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ í˜ì´ì§€ ì¶”ì¶œ
                    first_doc = state["retrieved_docs"][0]
                    # Document ê°ì²´ ë˜ëŠ” dict í˜•íƒœ ëª¨ë‘ ëŒ€ì‘
                    if hasattr(first_doc, "metadata"):
                        metadata = first_doc.metadata
                    else:
                        metadata = first_doc.get("metadata", {})

                    target_p = metadata.get("page")
                    if target_p is not None:
                        # 0-indexedì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ 1-indexedë¡œ ë³´ì • (ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë”°ë¼ ë‹¤ë¦„)
                        # ë³´í†µ LangChain/PyMuPDFëŠ” 0ë¶€í„° ì‹œì‘í•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ
                        st.session_state.pdf_target_page = int(target_p) + 1
                except Exception as e:
                    logger.warning(f"ìë™ ì í”„ í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

            return {
                "response": state["full_response"],
                "processed_content": processed_final,
                "thought": state["full_thought"],
                "documents": state["retrieved_docs"],
                "performance": state["performance"],
            }
    except Exception as e:
        logger.error(f"UI ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}", exc_info=True)
        friendly_msg = format_error_message(e)
        SessionManager.add_status_log(friendly_msg)
        return {"response": friendly_msg, "thought": "", "documents": []}
    finally:
        SessionManager.set("is_generating_answer", False)


def _clean_response_redundancy(text: str) -> str:
    if not text:
        return text
    clean_patterns = [
        r"^#{1,4}\s*(?:ë‹µë³€|ê²°ê³¼|ë¶„ì„ ê²°ê³¼|Response|Answer|Result)[:\s]*",
        r"^\**\s*(?:ë‹µë³€|ê²°ê³¼|ë¶„ì„ ê²°ê³¼|Response|Answer|Result)[:\s]*\**\s*",
    ]
    result = text.strip()
    for pattern in clean_patterns:
        result = re.sub(pattern, "", result, flags=re.IGNORECASE | re.MULTILINE).strip()
    return result


def render_message(
    role: str,
    content: str,
    thought: str | None = None,
    documents: list[Any] | None = None,
    metrics: dict | None = None,
    processed_content: str | None = None,
    msg_type: str = "general",
    wrap_in_container: bool = True,
    status_logs: list[str] | None = None,
    is_latest: bool = True,
    msg_index: int = 0,
    **kwargs,
):
    """ë©”ì‹œì§€ë¥¼ ë Œë”ë§í•˜ëŠ” í†µí•© ì—”ì§„. msg_typeì— ë”°ë¼ ë ˆì´ì•„ì›ƒ ìë™ ê²°ì •."""

    if role == "system" or msg_type == "log":
        with st.chat_message("system", avatar="âš™ï¸"):
            if msg_type == "log":
                st.markdown(
                    f"<div style='font-size: 0.85rem; color: var(--text-color); opacity: 0.7;'>â””â”€ {content}</div>",
                    unsafe_allow_html=True,
                )
            elif "ì™„ë£Œ" in content or "ì„±ê³µ" in content:
                st.success(content, icon="âœ…")
            elif "ì‹¤íŒ¨" in content or "ì˜¤ë¥˜" in content:
                st.error(content, icon="âŒ")
            else:
                st.info(content, icon="â„¹ï¸")
        return

    avatar_icon = "ğŸ¤–" if role == "assistant" else "ğŸ‘¤"

    # [í•µì‹¬ ìµœì í™”] ì´ë¯¸ ì»¨í…Œì´ë„ˆê°€ ìˆëŠ” ê²½ìš°(ìŠ¤íŠ¸ë¦¬ë° ìµœì¢… ë‹¨ê³„) ì¤‘ë³µ ìƒì„±ì„ ë°©ì§€
    msg_container: Any
    if wrap_in_container:
        msg_container = st.chat_message(role, avatar=avatar_icon)
    else:
        # ê°€ì§œ ì»¨í…Œì´ë„ˆ
        from contextlib import nullcontext

        msg_container = nullcontext()

    with msg_container:
        # 0. [ì¤‘ë³µ ì œê±°] ê¸°ì¡´ assistant ë‚´ status_logs(st.status) ìœ„ì ¯ ì œê±°
        # ëª¨ë“  ë¡œê·¸ëŠ” ìƒë‹¨ ì‹œìŠ¤í…œ íƒ€ì„ë¼ì¸ì—ì„œ í†µí•© ê´€ë¦¬í•¨

        if thought and thought.strip():
            with st.expander("ğŸ§  ì‚¬ê³  ì™„ë£Œ", expanded=False):
                st.markdown(
                    f'<div class="thought-container" style="font-size: 0.85rem;">{thought}</div>',
                    unsafe_allow_html=True,
                )

        if processed_content:
            st.markdown(processed_content, unsafe_allow_html=True)
        else:
            display_text = normalize_latex_delimiters(content)
            if role == "assistant" and documents:
                display_text = apply_tooltips_to_response(display_text, documents)
            st.markdown(display_text, unsafe_allow_html=True)

        if role == "assistant":
            st.divider()
            if metrics:
                # [ë¦¬íŒ©í† ë§] ì§€í‘œë¥¼ 4ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ìƒì„¸ ì •ë³´ í‘œì‹œ
                m_col1, m_col2, m_col3, m_col4 = st.columns([1, 1, 1, 1.5])

                with m_col1:
                    in_tokens = metrics.get("input_token_count", 0)
                    st.markdown(
                        f"ğŸ“¥ **{in_tokens}** <small>In</small>", unsafe_allow_html=True
                    )

                with m_col2:
                    out_tokens = metrics.get("token_count", 0)
                    st.markdown(
                        f"ğŸ“¤ **{out_tokens}** <small>Out</small>",
                        unsafe_allow_html=True,
                    )

                with m_col3:
                    doc_count = metrics.get("doc_count", 0)
                    if documents and len(documents) > 0:
                        # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ ë° ì¤‘ë³µ ì œê±°
                        pages = []
                        for d in documents:
                            # Document ê°ì²´ ë˜ëŠ” dict ëŒ€ì‘
                            if hasattr(d, "metadata"):
                                m = d.metadata
                            else:
                                m = d.get("metadata", {})

                            p = m.get("page")
                            if p is not None:
                                # [ìˆ˜ì •] ì´ë¯¸ 1-indexedì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                pages.append(int(p))

                        unique_pages = sorted(set(pages))

                        if unique_pages:
                            # íŒì˜¤ë²„ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¹”ë”í•˜ê²Œ í‘œì‹œ
                            with st.popover(
                                f"ğŸ“„ {doc_count} Docs", use_container_width=True
                            ):
                                st.caption("ê·¼ê±° í˜ì´ì§€ë¡œ ì´ë™:")
                                cols = st.columns(min(len(unique_pages), 3))
                                for idx, p in enumerate(unique_pages):
                                    # [ìˆ˜ì •] ê³ ì •ëœ í‚¤ ì‚¬ìš© (time.time() ì œê±°) ë° í™•ì‹¤í•œ ì´ë²¤íŠ¸ ìº¡ì²˜
                                    button_key = f"jump_btn_{msg_index}_{p}_{idx}"
                                    if cols[idx % 3].button(f"{p}p", key=button_key):
                                        logger.info(
                                            f"[DEBUG] í˜ì´ì§€ ì í”„ ì‹¤í–‰: {p}p (Key: {button_key})"
                                        )
                                        SessionManager.set("pdf_target_page", p)
                                        st.rerun()
                        else:
                            st.markdown(
                                f"ğŸ“„ **{doc_count}** <small>Docs</small>",
                                unsafe_allow_html=True,
                            )
                    else:
                        st.markdown(
                            f"ğŸ“„ **{doc_count}** <small>Docs</small>",
                            unsafe_allow_html=True,
                        )

                with m_col4:
                    total = metrics.get("total_time", 0)
                    ttft = metrics.get("ttft", 0)
                    st.markdown(
                        f"â±ï¸ **{total:.1f}s** <small>(TTFT: {ttft:.2f}s)</small>",
                        unsafe_allow_html=True,
                    )


@st.fragment
def render_chat_interface():
    """ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ìµœìƒìœ„ ë Œë”ë§ í•¨ìˆ˜ (Fragment ê²©ë¦¬)"""
    _chat_fragment()


def _chat_fragment():
    # [ìˆ˜ì •] ìë™ ìŠ¤í¬ë¡¤ê³¼ í•˜ë‹¨ ê³ ì •ì„ ìœ„í•´ ê³ ì • ë†’ì´ ì»¨í…Œì´ë„ˆ ì‚¬ìš©
    # heightë¥¼ ì§€ì •í•˜ë©´ ë‚´ìš©ì´ ëŠ˜ì–´ë‚  ë•Œ ìë™ìœ¼ë¡œ í•˜ë‹¨ì„ ì¶”ì í•©ë‹ˆë‹¤.
    chat_container = st.container(height=700, border=False)

    messages = SessionManager.get_messages() or []
    pdf_path = SessionManager.get("pdf_file_path")
    pdf_processed = SessionManager.get("pdf_processed", False)
    pdf_error = SessionManager.get("pdf_processing_error")

    is_generating = bool(SessionManager.get("is_generating_answer", False))
    is_processing_pdf = bool(pdf_path and not pdf_processed and not pdf_error)

    with chat_container:
        if not messages:
            st.chat_message("system", avatar="âš™ï¸").markdown(MSG_CHAT_GUIDE)

        system_buffer: list[str] = []

        def flush_system_buffer():
            if not system_buffer:
                return

            # [ìˆ˜ì •] ì£¼ìš” ê³¼ì • ì„ ë³„ ê¸°ì¤€ ìµœì í™”
            MAJOR_STEPS = {
                "ğŸ“‘": "ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ë° ë§ˆí¬ë‹¤ìš´ ë³€í™˜",
                "âœ‚ï¸": "ë¬¸ì„œ ë¶„í•  ë° ì§€ì‹ ì²­í‚¹",
                "ğŸ§ ": "ì§€ì‹ ë²¡í„°í™” ë° ì¸ë±ì‹±",
                "ğŸ”": "ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ë° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰",
                "ğŸ“š": "ê´€ë ¨ ì§€ì‹ í™•ë³´ ë° ë¬¸ì„œ ê²€ì¦",
                "âš–ï¸": "ë¬¸ì„œ ìˆœìœ„ ì¬ì¡°ì • ë° ì í•©ë„ ê²€ì¦",
                "ğŸ¯": "í•µì‹¬ ë‹µë³€ ê·¼ê±° ì„ ì • ë° ì»¨í…ìŠ¤íŠ¸ ì •ì œ",
                "ğŸ§©": "ë‹µë³€ìš© ì§€ì‹ ì»¨í…ìŠ¤íŠ¸ ë³‘í•©",
                "âœï¸": "ì§€ì‹ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ë‹µë³€ ì‘ì„± ì‹œì‘",
            }

            is_doc_analysis = False
            is_complete = False
            has_error = False
            log_items: list[str] = []

            for m in system_buffer:
                # 1. ì™„ë£Œ ë° ì˜¤ë¥˜ ìƒíƒœ í™•ì¸
                if m == "READY_FOR_QUERY" or "ì™„ë£Œ" in m or "ì„±ê³µ" in m:
                    is_complete = True
                if any(x in m for x in ["âŒ", "ì˜¤ë¥˜", "ì‹¤íŒ¨"]):
                    has_error = True

                # 2. ì£¼ìš” ë‹¨ê³„ ë§¤ì¹­ (ì•„ì´ì½˜ ìš°ì„  ë§¤ì¹­)
                matched = False
                for icon, label in MAJOR_STEPS.items():
                    if icon in m:
                        # ê°™ì€ ì•„ì´ì½˜ì´ ì´ë¯¸ ìˆìœ¼ë©´ ë‚´ìš©ì„ ë³´ê³  ê²°ì • (ì•„ì´ì½˜ì€ ê°™ì€ë° ë‚´ìš©ì´ ë‹¤ë¥´ë©´ ì¶”ê°€)
                        if not any(icon in li for li in log_items):
                            log_items.append(f"{icon} {label}")
                            if icon in ["ğŸ“‘", "âœ‚ï¸", "ğŸ§ "]:
                                is_doc_analysis = True
                            if icon in ["ğŸ”", "ğŸ“š", "âš–ï¸", "ğŸ¯", "ğŸ§©", "âœï¸"]:
                                pass
                        matched = True
                        break

                # 3. ì•„ì´ì½˜ ë§¤ì¹­ ì•ˆ ëœ ê²½ìš° í‚¤ì›Œë“œ ë§¤ì¹­
                if not matched:
                    if any(x in m for x in ["ë¶„ì„", "ë§ˆí¬ë‹¤ìš´", "êµ¬ì¡°"]) and not any(
                        "ğŸ“‘" in li for li in log_items
                    ):
                        log_items.append(f"ğŸ“‘ {MAJOR_STEPS['ğŸ“‘']}")
                        is_doc_analysis = True
                    elif ("ë²¡í„°í™”" in m or "ì¸ë±ì‹±" in m) and not any(
                        "ğŸ§ " in li for li in log_items
                    ):
                        log_items.append(f"ğŸ§  {MAJOR_STEPS['ğŸ§ ']}")
                        is_doc_analysis = True

            if log_items:
                # [ê°œì„ ] ë§ˆì§€ë§‰ ë‹¨ê³„ë¥¼ ì œëª©ìœ¼ë¡œ ì‚¬ìš© (ë™ì  ì œëª© ì‹œìŠ¤í…œ)
                current_step_label = log_items[-1]
                is_expanded = not is_complete and not has_error

                # ìƒíƒœì— ë”°ë¥¸ ì•„ì´ì½˜ ë° ì ‘ë‘ì‚¬ ê²°ì •
                if has_error:
                    status_prefix = "âŒ ì˜¤ë¥˜: "
                elif is_complete:
                    status_prefix = "âœ… ì™„ë£Œ: "
                else:
                    status_prefix = "âš™ï¸ ì²˜ë¦¬ ì¤‘: "

                # ìµœì¢… ì œëª© êµ¬ì„± (ì•„ì´ì½˜ ì œì™¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ì¡°í•©)
                clean_label = current_step_label.split(" ", 1)[-1]
                expander_title = f"{status_prefix}{clean_label}"

                with st.chat_message("system", avatar="âš™ï¸"):
                    # [ìˆ˜ì •] ë‹¤í¬ëª¨ë“œ ì‹œì¸ì„±ì„ ìœ„í•´ í•˜ë“œì½”ë”©ëœ ìƒ‰ìƒ ì œê±° ë° í…Œë§ˆ ë³€ìˆ˜ í™œìš©
                    # [ìˆ˜ì •] ë‹¤í¬ëª¨ë“œ ì‹œì¸ì„±ì„ ìœ„í•´ í•˜ë“œì½”ë”©ëœ ìƒ‰ìƒ ì œê±° ë° í…Œë§ˆ ë³€ìˆ˜ í™œìš©
                    lines = "".join(
                        [
                            f"<div style='font-size: 0.85rem; color: var(--text-color); margin-bottom: 8px; display: flex; align-items: flex-start; line-height: 1.5; opacity: 0.9;'>"
                            f"<span style='color: #1e88e5; margin-right: 10px; font-weight: bold;'>â–¹</span>"
                            f"<span>{item}</span></div>"
                            for item in log_items
                        ]
                    )

                    timeline_html = (
                        f"<details {'open' if is_expanded else ''} class='timeline-container' style='border: 1px solid rgba(128,128,128,0.2); border-radius: 8px; padding: 10px; margin-bottom: 10px;'>"
                        f"<summary class='timeline-summary' style='font-weight: 600; color: var(--text-color); cursor: pointer; list-style: none; display: flex; align-items: center; padding: 5px 0;'>"
                        f"<span style='color: #1e88e5; margin-right: 10px;'>{'âœ…' if is_complete else 'âš™ï¸'}</span> {expander_title.split(': ', 1)[-1] if ': ' in expander_title else expander_title}"
                        f"</summary>"
                        f"<div style='margin-top: 12px; padding: 15px; background-color: rgba(128,128,128,0.05); border-radius: 8px; border-left: 3px solid #1e88e5;'>"
                        f"<div style='font-size: 0.75rem; color: var(--text-color); opacity: 0.6; margin-bottom: 12px; font-weight: 700; text-transform: uppercase; letter-spacing: 0.5px;'>"
                        f"â±ï¸ {'Document Analysis' if is_doc_analysis else 'Answer Generation'} Sequence"
                        f"</div>{lines}</div></details>"
                    )
                    st.markdown(timeline_html, unsafe_allow_html=True)

            system_buffer.clear()

        # [ìµœì í™”] ëŒ€í™” ì´ë ¥ ë Œë”ë§ ê³ ì†í™”
        msg_count = len(messages)
        for i, msg in enumerate(messages):
            is_latest = i == msg_count - 1

            if msg.get("role") == "system" or msg.get("msg_type") == "log":
                system_buffer.append(str(msg.get("content", "")))
            else:
                flush_system_buffer()

                # íƒ€ì… ì•ˆì „ì„±ì„ ìœ„í•´ ëª…ì‹œì  ì¶”ì¶œ
                msg_metrics = msg.get("metrics")
                msg_logs = msg.get("status_logs")

                render_message(
                    role=str(msg.get("role", "user")),
                    content=str(msg.get("content", "")),
                    thought=msg.get("thought"),
                    documents=msg.get("documents"),
                    metrics=msg_metrics if isinstance(msg_metrics, dict) else None,
                    processed_content=msg.get("processed_content"),
                    msg_type=str(msg.get("msg_type", "general")),
                    status_logs=msg_logs if isinstance(msg_logs, list) else None,
                    is_latest=is_latest,
                    msg_index=i,  # ì¸ë±ìŠ¤ ì „ë‹¬
                )
        flush_system_buffer()

    input_disabled = is_generating or is_processing_pdf
    input_placeholder = (
        "ë¬¸ì„œ ë¶„ì„ ì¤‘..."
        if is_processing_pdf
        else ("ë‹µë³€ ìƒì„± ì¤‘..." if is_generating else MSG_CHAT_INPUT_PLACEHOLDER)
    )

    user_query = st.chat_input(
        input_placeholder, disabled=input_disabled, key="chat_input_clean"
    )

    if user_query:
        SessionManager.add_message("user", user_query)
        with chat_container:
            render_message("user", user_query)

        rag_engine = SessionManager.get("rag_engine")
        if rag_engine:
            # [ì•ˆì „ ì¥ì¹˜] ë‹µë³€ ìƒì„± ì‹œì‘ ì‹œ í”Œë˜ê·¸ ì„¤ì • ë° ì˜¤ë¥˜ ë°œìƒ ì‹œ ë³µêµ¬ ë³´ì¥
            SessionManager.set("is_generating_answer", True)
            try:
                result = sync_run(
                    _stream_chat_response(rag_engine, user_query, chat_container)
                )

                final_answer = result.get("response", "")
                final_thought = result.get("thought", "")
                final_docs = result.get("documents", [])
                final_metrics = result.get("performance")
                processed_final = result.get("processed_content", "")

                if final_answer and not final_answer.startswith("âŒ"):
                    # [ì¶”ê°€] ë‹µë³€ ìƒì„±ì— ì‚¬ìš©ëœ ë¬¸ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ PDF í•˜ì´ë¼ì´íŠ¸ ìƒì„±
                    from common.utils import extract_annotations_from_docs

                    annotations = extract_annotations_from_docs(final_docs)
                    SessionManager.set("pdf_annotations", annotations)

                    # ìƒì„¸ ë¡œê¹… (ë””ë²„ê¹… ìš©ë„)
                    pages = sorted({a["page"] + 1 for a in annotations})
                    logger.info(
                        f"[UI] PDF í•˜ì´ë¼ì´íŠ¸ ì ìš© ì™„ë£Œ: {len(annotations)}ê°œ ì˜ì—­ (Pages: {pages})"
                    )

                    SessionManager.add_message(
                        role="assistant",
                        content=final_answer,
                        processed_content=processed_final,
                        thought=final_thought,
                        metrics=final_metrics,
                        msg_type="answer",
                        documents=final_docs,
                        status_logs=SessionManager.get("status_logs"),
                        source_file=SessionManager.get("last_uploaded_file_name"),
                    )
            except Exception as e:
                logger.error(f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
                st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            finally:
                # [í•µì‹¬] ì–´ë–¤ ê²½ìš°ì—ë„ ìƒì„± ì¤‘ í”Œë˜ê·¸ í•´ì œ
                SessionManager.set("is_generating_answer", False)
                # [ìµœì í™”] í”„ë˜ê·¸ë¨¼íŠ¸ ë²”ìœ„ ë‚´ì—ì„œë§Œ ë¦¬ëŸ°í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
                st.rerun(scope="fragment")
        else:
            st.error(MSG_CHAT_NO_QA_SYSTEM)
