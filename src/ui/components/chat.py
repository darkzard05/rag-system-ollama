"""
ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ë° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ê´€ë ¨ ì»´í¬ë„ŒíŠ¸.
"""

import logging
import re
import time
from contextlib import aclosing
from typing import Any

import streamlit as st

from api.streaming_handler import get_adaptive_controller, get_streaming_handler
from common.config import (
    MSG_CHAT_GUIDE,
    MSG_CHAT_INPUT_PLACEHOLDER,
    MSG_CHAT_NO_QA_SYSTEM,
)
from common.utils import (
    apply_tooltips_to_response,
    format_error_message,
    normalize_latex_delimiters,
    sync_run,
)
from core.session import SessionManager

logger = logging.getLogger(__name__)


async def _stream_chat_response(
    rag_engine, user_query: str, chat_container
) -> dict[str, Any]:
    """
    ì ì‘í˜• ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ê³  ê³¼ì •ê³¼ ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë Œë”ë§í•©ë‹ˆë‹¤.
    """

    state: dict[str, Any] = {
        "full_response": "",
        "full_thought": "",
        "retrieved_docs": [],
        "performance": None,
        "start_time": time.time(),
        "thinking_start_time": None,
        "thinking_end_time": None,
    }

    current_llm = SessionManager.get("llm")
    if not current_llm:
        return {
            "response": "âŒ ì˜¤ë¥˜: ì¶”ë¡  ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "thought": "",
            "documents": [],
        }

    run_config = {"configurable": {"llm": current_llm}}
    SessionManager.set("is_generating_answer", True)

    handler = get_streaming_handler()
    controller = get_adaptive_controller()

    last_render_time = 0.0
    render_interval = 0.05

    try:
        with chat_container, st.chat_message("assistant", avatar="ğŸ¤–"):
            status_box = st.status("ğŸš€ íŒŒì´í”„ë¼ì¸ ê°€ë™ ì¤‘...", expanded=True)

            def update_status(msg: str, state="running"):
                status_box.write(f"â””â”€ {msg}")
                if state == "complete":
                    status_box.update(
                        label="âœ… ë¶„ì„ ì™„ë£Œ", state="complete", expanded=False
                    )

            thought_area = st.container()
            answer_area = st.empty()

            event_generator = rag_engine.astream_events(
                {"input": user_query}, config=run_config, version="v2"
            )

            async with aclosing(
                handler.stream_graph_events(
                    event_generator, adaptive_controller=controller
                )
            ) as stream:
                async for chunk in stream:
                    if chunk.status:
                        update_status(chunk.status)
                        SessionManager.add_status_log(chunk.status)

                    if chunk.metadata and "documents" in chunk.metadata:
                        state["retrieved_docs"] = chunk.metadata["documents"]
                        update_status(
                            f"ê´€ë ¨ ì§€ì‹ {len(state['retrieved_docs'])}ê°œ í™•ë³´"
                        )

                    if chunk.performance:
                        state["performance"] = chunk.performance

                    if chunk.thought:
                        if not state["full_thought"]:
                            state["thinking_start_time"] = time.time()
                            with thought_area:
                                st.caption("AIì˜ ì‚¬ê³  íë¦„:")
                                thought_display = st.empty()
                        state["full_thought"] += chunk.thought
                        if time.time() - last_render_time > render_interval:
                            thought_display.markdown(f"*{state['full_thought']}*")
                            last_render_time = time.time()

                    if chunk.content:
                        if not state["full_response"]:
                            update_status("ë‹µë³€ ìƒì„± ì¤‘...", state="complete")
                            state["thinking_end_time"] = time.time()
                            if state["full_thought"]:
                                with thought_area:
                                    thinking_dur = (
                                        state["thinking_end_time"]
                                        - state["thinking_start_time"]
                                    )
                                    with st.expander(
                                        f"ğŸ’­ ì‚¬ê³  ì™„ë£Œ ({thinking_dur:.1f}ì´ˆ)",
                                        expanded=False,
                                    ):
                                        st.markdown(
                                            f'<div class="thought-container">{state["full_thought"]}</div>',
                                            unsafe_allow_html=True,
                                        )
                                    if "thought_display" in locals():
                                        thought_display.empty()

                        state["full_response"] += chunk.content
                        if (
                            time.time() - last_render_time > render_interval
                            or chunk.is_final
                        ):
                            display_text = _clean_response_redundancy(
                                state["full_response"]
                            )
                            display_text = normalize_latex_delimiters(display_text)
                            cursor = "â–Œ" if not chunk.is_final else ""
                            answer_area.markdown(display_text + cursor)
                            last_render_time = time.time()

            # [ìµœì í™”] ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜ (ì¤‘ë³µ ë Œë”ë§ ì œê±°ë¡œ ì§€ì—° ìµœì†Œí™”)
            cleaned_final = _clean_response_redundancy(state["full_response"])
            processed_final = apply_tooltips_to_response(
                cleaned_final, state["retrieved_docs"]
            )

            return {
                "response": state["full_response"],
                "processed_content": processed_final,
                "thought": state["full_thought"],
                "documents": state["retrieved_docs"],
                "performance": state["performance"],
            }
    except Exception as e:
        logger.error(f"UI ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}", exc_info=True)
        friendly_msg = format_error_message(e)
        SessionManager.add_status_log(friendly_msg)
        return {"response": friendly_msg, "thought": "", "documents": []}
    finally:
        SessionManager.set("is_generating_answer", False)


def _clean_response_redundancy(text: str) -> str:
    if not text:
        return text
    clean_patterns = [
        r"^#{1,4}\s*(?:ë‹µë³€|ê²°ê³¼|ë¶„ì„ ê²°ê³¼|Response|Answer|Result)[:\s]*",
        r"^\**\s*(?:ë‹µë³€|ê²°ê³¼|ë¶„ì„ ê²°ê³¼|Response|Answer|Result)[:\s]*\**\s*",
    ]
    result = text.strip()
    for pattern in clean_patterns:
        result = re.sub(pattern, "", result, flags=re.IGNORECASE | re.MULTILINE).strip()
    return result


def render_message(
    role: str,
    content: str,
    thought: str | None = None,
    documents: list[Any] | None = None,
    metrics: dict | None = None,
    processed_content: str | None = None,
    msg_type: str = "general",
    wrap_in_container: bool = True,
    status_logs: list[str] | None = None,
    is_latest: bool = True,  # [ì¶”ê°€] ìµœì‹  ë©”ì‹œì§€ ì—¬ë¶€
    **kwargs,
):
    """ë©”ì‹œì§€ë¥¼ ë Œë”ë§í•˜ëŠ” í†µí•© ì—”ì§„. msg_typeì— ë”°ë¼ ë ˆì´ì•„ì›ƒ ìë™ ê²°ì •."""

    if role == "system" or msg_type == "log":
        with st.chat_message("system", avatar="âš™ï¸"):
            if "ì™„ë£Œ" in content or "ì„±ê³µ" in content:
                st.success(content, icon="âœ…")
            elif "ì‹¤íŒ¨" in content or "ì˜¤ë¥˜" in content:
                st.error(content, icon="âŒ")
            else:
                st.info(content, icon="â„¹ï¸")
        return

    avatar_icon = "ğŸ¤–" if role == "assistant" else "ğŸ‘¤"

    # [í•µì‹¬ ìµœì í™”] ì´ë¯¸ ì»¨í…Œì´ë„ˆê°€ ìˆëŠ” ê²½ìš°(ìŠ¤íŠ¸ë¦¬ë° ìµœì¢… ë‹¨ê³„) ì¤‘ë³µ ìƒì„±ì„ ë°©ì§€
    msg_container: Any
    if wrap_in_container:
        msg_container = st.chat_message(role, avatar=avatar_icon)
    else:
        # ê°€ì§œ ì»¨í…Œì´ë„ˆ
        from contextlib import nullcontext

        msg_container = nullcontext()

    with msg_container:
        # 0. íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë¡œê·¸ (Status Logs)
        # [ìµœì í™”] ìµœì‹  ë©”ì‹œì§€ê°€ ì•„ë‹ˆë©´ ë¬´ê±°ìš´ st.status ìœ„ì ¯ ìƒì„±ì„ ìƒëµí•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
        if role == "assistant" and status_logs and is_latest:
            with st.status("âœ… ë¶„ì„ ì™„ë£Œ", state="complete", expanded=False):
                for log in status_logs:
                    if log not in ["ì‹œìŠ¤í…œ ëŒ€ê¸° ì¤‘", "ìƒˆ ë¬¸ì„œ ë¶„ì„ ì‹œì‘"]:
                        st.write(f"â””â”€ {log}")

        if thought and thought.strip():
            with st.expander("ğŸ§  ì‚¬ê³  ì™„ë£Œ", expanded=False):
                st.markdown(
                    f'<div class="thought-container">{thought}</div>',
                    unsafe_allow_html=True,
                )

        if processed_content:
            st.markdown(processed_content, unsafe_allow_html=True)
        else:
            display_text = normalize_latex_delimiters(content)
            if role == "assistant" and documents:
                display_text = apply_tooltips_to_response(display_text, documents)
            st.markdown(display_text, unsafe_allow_html=True)

        if role == "assistant":
            st.divider()
            if metrics:
                # [ë¦¬íŒ©í† ë§] ì§€í‘œë¥¼ 3ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‹œê°ì ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ë°°ì¹˜
                m_col1, m_col2, m_col3 = st.columns([1, 1, 1.5])

                with m_col1:
                    tokens = metrics.get("token_count", 0)
                    st.markdown(
                        f"ğŸ“ **{tokens}** <small>tokens</small>", unsafe_allow_html=True
                    )

                with m_col2:
                    tps = metrics.get("tps", 0)
                    st.markdown(
                        f"ğŸš€ **{tps:.1f}** <small>t/s</small>", unsafe_allow_html=True
                    )

                with m_col3:
                    total = metrics.get("total_time", 0)
                    ttft = metrics.get("ttft", 0)
                    st.markdown(
                        f"â±ï¸ **{total:.1f}s** <small>(TTFT: {ttft:.2f}s)</small>",
                        unsafe_allow_html=True,
                    )


@st.fragment
def render_chat_interface():
    """ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ìµœìƒìœ„ ë Œë”ë§ í•¨ìˆ˜ (Fragment ê²©ë¦¬)"""
    _chat_fragment()


def _chat_fragment():
    # [ìµœì í™”] ê³ ì •ëœ Viewport Height ê¸°ë°˜ì˜ ë†’ì´ ì„¤ì • (JS ì˜ì¡´ì„± ê°ì†Œ)
    win_h = st.session_state.get("last_valid_height", 800)
    container_h = max(400, win_h - 250)
    chat_container = st.container(height=container_h, border=True)

    messages = SessionManager.get_messages() or []
    pdf_path = SessionManager.get("pdf_file_path")
    pdf_processed = SessionManager.get("pdf_processed", False)
    pdf_error = SessionManager.get("pdf_processing_error")

    is_generating = bool(SessionManager.get("is_generating_answer", False))
    is_processing_pdf = bool(pdf_path and not pdf_processed and not pdf_error)

    with chat_container:
        if not messages:
            st.chat_message("system", avatar="âš™ï¸").markdown(MSG_CHAT_GUIDE)

        system_buffer = []

        def flush_system_buffer():
            if not system_buffer:
                return
            with st.chat_message("system", avatar="âš™ï¸"):
                is_ready, has_error = False, False
                log_items = []
                for m in system_buffer:
                    if m == "READY_FOR_QUERY":
                        is_ready = True
                        continue
                    if any(x in m for x in ["âŒ", "ì˜¤ë¥˜", "ì‹¤íŒ¨"]):
                        has_error = True
                    log_items.append(f"â””â”€ {m}")
                if is_ready and not has_error:
                    st.markdown("**ì‹œìŠ¤í…œ êµ¬ì„± ë° ë°ì´í„° ë¶„ì„ ì™„ë£Œ**")
                    st.markdown("ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!")
                else:
                    st.markdown("  \n".join(log_items))
            system_buffer.clear()

        # [ìµœì í™”] ëŒ€í™” ì´ë ¥ ë Œë”ë§ ê³ ì†í™”
        msg_count = len(messages)
        for i, msg in enumerate(messages):
            is_latest = i == msg_count - 1

            if msg.get("role") == "system" or msg.get("msg_type") == "log":
                system_buffer.append(msg["content"])
            else:
                flush_system_buffer()
                render_message(
                    role=msg.get("role", "user"),
                    content=msg.get("content", ""),
                    thought=msg.get("thought"),
                    metrics=msg.get("metrics"),
                    processed_content=msg.get("processed_content"),
                    msg_type=msg.get("msg_type", "general"),
                    status_logs=msg.get("status_logs"),
                    is_latest=is_latest,  # [ì¶”ê°€] ìµœì‹  ë©”ì‹œì§€ ì—¬ë¶€ ì „ë‹¬
                )
        flush_system_buffer()

    input_disabled = is_generating or is_processing_pdf
    input_placeholder = (
        "ë¬¸ì„œ ë¶„ì„ ì¤‘..."
        if is_processing_pdf
        else ("ë‹µë³€ ìƒì„± ì¤‘..." if is_generating else MSG_CHAT_INPUT_PLACEHOLDER)
    )

    user_query = st.chat_input(
        input_placeholder, disabled=input_disabled, key="chat_input_clean"
    )

    if user_query:
        SessionManager.add_message("user", user_query)
        with chat_container:
            render_message("user", user_query)

        rag_engine = SessionManager.get("rag_engine")
        if rag_engine:
            result = sync_run(
                _stream_chat_response(rag_engine, user_query, chat_container)
            )

            final_answer = result.get("response", "")
            final_thought = result.get("thought", "")
            final_docs = result.get("documents", [])
            final_metrics = result.get("performance")
            processed_final = result.get("processed_content", "")

            if final_answer and not final_answer.startswith("âŒ"):
                SessionManager.add_message(
                    role="assistant",
                    content=final_answer,
                    processed_content=processed_final,
                    thought=final_thought,
                    metrics=final_metrics,
                    msg_type="answer",
                    status_logs=SessionManager.get("status_logs"),
                    source_file=SessionManager.get("last_uploaded_file_name"),
                    documents=final_docs,
                )
                # [ìµœì í™”] í”„ë˜ê·¸ë¨¼íŠ¸ ë²”ìœ„ ë‚´ì—ì„œë§Œ ë¦¬ëŸ°í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
                st.rerun(scope="fragment")
        else:
            st.error(MSG_CHAT_NO_QA_SYSTEM)
