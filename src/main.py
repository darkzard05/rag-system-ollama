"""
RAG Chatbot ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë©”ì¸ ì§„ì…ì  íŒŒì¼ì…ë‹ˆë‹¤.
"""
import logging
import tempfile
import streamlit as st

# ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆ ì„í¬íŠ¸
from session import SessionManager
from ui import render_sidebar, render_chat_column, render_pdf_viewer
from rag_core import (
    process_pdf_and_build_chain, 
    create_qa_chain, 
    load_llm, 
    load_embedding_model, 
    create_vector_store,
    is_embedding_model_cached
)
from config import AVAILABLE_EMBEDDING_MODELS

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="RAG Chatbot",
    layout="wide"
)

# --- í•¸ë“¤ëŸ¬ ë° í—¬í¼ í•¨ìˆ˜ ---
def update_qa_system():
    """QA ì‹œìŠ¤í…œì„ í˜„ì¬ ì„¸ì…˜ ìƒíƒœì— ë§ì¶° ì—…ë°ì´íŠ¸í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        llm = SessionManager.get_llm()
        vector_store = SessionManager.get_vector_store()
        doc_splits = SessionManager.get_processed_document_splits()

        if not all([llm, vector_store, doc_splits]):
            st.warning("QA ì‹œìŠ¤í…œì„ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return

        qa_chain = create_qa_chain(llm, vector_store, doc_splits)
        SessionManager.set_qa_chain(qa_chain)
        st.rerun()
    except Exception as e:
        st.error(f"QA ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error("QA ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜", exc_info=True)

def handle_model_change(selected_model: str):
    """ëª¨ë¸ ë³€ê²½ì„ ì²˜ë¦¬í•˜ëŠ” í•¸ë“¤ëŸ¬"""
    if "---" in selected_model or \
       not selected_model or \
       selected_model == SessionManager.get_last_selected_model():
        return

    SessionManager.update_model(selected_model)
    
    if SessionManager.get_pdf_processed():
        with st.spinner(f"'{selected_model}' ëª¨ë¸ë¡œ QA ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì¤‘..."):
            llm = load_llm(selected_model)
            SessionManager.set_llm(llm)
            update_qa_system()

def handle_embedding_model_change(selected_embedding_model: str):
    """ì„ë² ë”© ëª¨ë¸ ë³€ê²½ì„ ì²˜ë¦¬í•˜ëŠ” í•¸ë“¤ëŸ¬. ì´ˆê¸° ì„¤ì • ì‹œì—ëŠ” ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŒ."""
    last_embedding_model = SessionManager.get_last_selected_embedding_model()

    if not selected_embedding_model or selected_embedding_model == last_embedding_model:
        return

    SessionManager.set_last_selected_embedding_model(selected_embedding_model)
    
    if last_embedding_model is not None:
        SessionManager.add_message("assistant", f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ì„ '{selected_embedding_model}'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.")

    if SessionManager.get_pdf_processed():
        if not is_embedding_model_cached(selected_embedding_model):
            st.info(f"'{selected_embedding_model}' ëª¨ë¸ì„ ì²˜ìŒ ë¡œë“œí•©ë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•˜ë©° ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        with st.spinner(f"'{selected_embedding_model}' ì„ë² ë”© ëª¨ë¸ë¡œ QA ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì¤‘..."):
            embedder = load_embedding_model(selected_embedding_model)
            SessionManager.set_embedder(embedder)
            
            doc_splits = SessionManager.get_processed_document_splits()
            vector_store = create_vector_store(doc_splits, embedder)
            SessionManager.set_vector_store(vector_store)
            
            update_qa_system()


def handle_file_upload(uploaded_file):
    """íŒŒì¼ ì—…ë¡œë“œë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¸ë“¤ëŸ¬"""
    if uploaded_file.name == SessionManager.get_last_uploaded_file_name():
        return

    file_bytes = uploaded_file.getvalue()
    SessionManager.reset_for_new_file(uploaded_file.name, file_bytes)
    
    try:
        # RAG Core ì²˜ë¦¬ë¥¼ ìœ„í•´ ì„ì‹œ íŒŒì¼ì€ ì—¬ì „íˆ í•„ìš”
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(file_bytes)
            temp_path = tmp_file.name

        SessionManager.add_message("assistant", f"ğŸ“‚ '{uploaded_file.name}' íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ.")
        
        # PDF ì²˜ë¦¬
        selected_model = SessionManager.get_last_selected_model()
        selected_embedding_model = SessionManager.get_last_selected_embedding_model() or AVAILABLE_EMBEDDING_MODELS[0]
        
        if not selected_model:
            st.warning("LLM ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return

        # ëª¨ë¸ ë¡œë“œ ì „ ìºì‹œ í™•ì¸ ë° ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ
        if not is_embedding_model_cached(selected_embedding_model):
            st.info(f"'{selected_embedding_model}' ëª¨ë¸ì„ ì²˜ìŒ ë¡œë“œí•©ë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•˜ë©° ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        with st.spinner(f"'{uploaded_file.name}' ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
            success_message = process_pdf_and_build_chain(
                uploaded_file,
                temp_path, # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì „ë‹¬
                selected_model,
                selected_embedding_model
            )
            SessionManager.add_message("assistant", success_message)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        import os
        os.remove(temp_path)
        
        st.rerun()

    except Exception as e:
        error_msg = f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg, exc_info=True)
        SessionManager.set_error_state(error_msg)
        st.rerun()

# --- ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ---
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ í•¨ìˆ˜"""
    SessionManager.init_session()
    
    render_sidebar(
        uploaded_file_handler=handle_file_upload,
        model_change_handler=handle_model_change,
        embedding_model_change_handler=handle_embedding_model_change
    )

    col_left, col_right = st.columns([1, 1])

    with col_left:
        render_chat_column()

    with col_right:
        render_pdf_viewer()

if __name__ == "__main__":
    main()