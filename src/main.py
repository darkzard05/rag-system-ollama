from re import split
import torch
torch.classes.__path__ = []
import streamlit as st
from langchain.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
import logging
from concurrent.futures import ThreadPoolExecutor
from utils import (
    get_ollama_models,
    load_pdf_docs,
    get_embedder,
    split_documents,
    create_vector_store,
    init_llm,
)

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="RAG Chatbot", layout="wide")
st.title("ğŸ“„ RAG Chatbot with Ollama LLM")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "last_selected_model" not in st.session_state:
    st.session_state.last_selected_model = None

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")
    models = get_ollama_models()
    selected_model = st.selectbox("ì‚¬ìš©í•  Ollama ëª¨ë¸ ì„ íƒ", models) if models else st.text("Ollama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type="pdf")

# ìƒë‹¨ì— ì €ì¥ëœ ëŒ€í™” ë©”ì‹œì§€ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ëª¨ë¸ ë³€ê²½ ì‹œ session_stateì— ë©”ì‹œì§€ ì¶”ê°€ (ì¶œë ¥ì€ ìœ„ forë¬¸ì—ì„œ í•œ ë²ˆì— ë¨)
if selected_model and selected_model != st.session_state.get("last_selected_model"):
    st.session_state.last_selected_model = selected_model  # ì„ íƒí•œ ëª¨ë¸ ì €ì¥
    new_message = {
        "role": "assistant",
        "content": f"ğŸ› ï¸ ëª¨ë¸ {selected_model}ì´(ê°€) ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤."
    }
    st.session_state.messages.append(new_message)
    st.rerun()  # ì—…ë°ì´íŠ¸ í›„ ì¬ì‹¤í–‰í•˜ì—¬ ì „ì²´ ëŒ€í™” ê¸°ë¡ ì¶œë ¥

# PDF íŒŒì¼ ì—…ë¡œë“œ ì‹œ ê¸°ì¡´ ìƒíƒœ ì´ˆê¸°í™” ë° ì²˜ë¦¬
if uploaded_file:
    file_bytes = uploaded_file.getvalue()  # íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°

    # ìƒˆë¡œìš´ PDF ì—…ë¡œë“œ ê°ì§€ í›„, í•œ ë²ˆë§Œ ìƒíƒœ ì´ˆê¸°í™”
    if st.session_state.get("last_uploaded_file") != file_bytes:
        st.session_state.last_uploaded_file = file_bytes  # íŒŒì¼ ë‚´ìš©ì„ ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ

        # ìºì‹œ ë¬´íš¨í™”
        load_pdf_docs.clear()  # í•¨ìˆ˜ í˜¸ì¶œ í›„ clear
        get_embedder.clear()  # í•¨ìˆ˜ í˜¸ì¶œ í›„ clear
        split_documents.clear()  # í•¨ìˆ˜ í˜¸ì¶œ í›„ clear
        create_vector_store.clear()  # í•¨ìˆ˜ í˜¸ì¶œ í›„ clear

        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        for key in ["pdf_processed", "pdf_completed", "qa_chain", "vector_store", "llm", "pdf_processing", "pdf_message_logged"]:
            st.session_state.pop(key, None)  # ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ì‚­ì œ

        # UI ì¦‰ì‹œ ê°±ì‹ í•˜ì—¬ ìƒˆë¡œìš´ PDF ê°ì§€
        st.rerun()

# "ë¬¸ì„œ ì²˜ë¦¬ ì¤‘" ë©”ì‹œì§€ê°€ í•œ ë²ˆë§Œ ì¶œë ¥ë˜ë„ë¡ ì¡°ì ˆ
if uploaded_file and not st.session_state.get("pdf_completed", False):
    if not st.session_state.get("pdf_message_logged", False):  
        st.session_state.messages.append({
            'role': 'assistant',
            'content': f'ğŸ“‚ PDF íŒŒì¼ {uploaded_file.name}ì´(ê°€) ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤...'
        })
        st.session_state.pdf_message_logged = True  # ë©”ì‹œì§€ ì¤‘ë³µ ì¶”ê°€ ë°©ì§€

    # ë¬¸ì„œ ì²˜ë¦¬ ê³¼ì •ì—ì„œ ê¹œë°•ì„ ë°©ì§€ë¥¼ ìœ„í•´ st.spinner() ì‚¬ìš©
    with st.spinner("ğŸ“„ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”."):
        # ë¬¸ì„œ ë¡œë”©
        docs = load_pdf_docs(uploaded_file.getvalue())
        if not docs:
            st.session_state.messages.append({
                'role': 'assistant', 
                'content': 'âŒ ë¬¸ì„œ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
            })
            st.stop()

        # ë¬¸ì„œ ì„ë² ë”© ìƒì„±
        embedder = get_embedder(model_name="BAAI/bge-m3",
                                model_kwargs={'device': 'cuda'})
        
        if embedder is None:
            st.session_state.messages.append({
                'role': 'assistant', 
                'content': 'âŒ ì„ë² ë” ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
            })
            st.stop()

        # ë¬¸ì„œ ë¶„í•  ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        with ThreadPoolExecutor() as executor:
            try:
                future_docs = executor.submit(split_documents, docs, embedder)
                documents = future_docs.result()
                if not documents:
                    raise ValueError("âŒ ë¬¸ì„œ ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

                future_vector_store = executor.submit(create_vector_store, documents, embedder)
                vector_store = future_vector_store.result()
                if not vector_store:
                    raise ValueError("âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.session_state.messages.append({
                    'role': 'assistant', 
                    'content': f'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}'
                })
                st.stop()
        
        st.session_state.vector_store = vector_store

        # LLM ì´ˆê¸°í™”
        if isinstance(selected_model, str):
            llm = init_llm(selected_model)
        else:
            llm = None
        if llm is None:
            st.session_state.messages.append({
                'role': 'assistant', 
                'content': 'âŒ LLM ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
            })
            st.stop()
        st.session_state.llm = llm  

        # QA ì²´ì¸ ìƒì„±
        QA_PROMPT = PromptTemplate.from_template(
        """
        ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ë° ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ë‚´ì˜ ì •ë³´ë§Œ í™œìš©í•˜ì—¬, ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
        ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ í™•ì¸ë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì‹œê³ , í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.

        [ì»¨í…ìŠ¤íŠ¸]
        {context}
        
        [ì§ˆë¬¸]
        {input}

        [ë‹µë³€]
        """
        )
        combine_chain = create_stuff_documents_chain(llm, QA_PROMPT)
        qa_chain = create_retrieval_chain(vector_store.as_retriever(search_type="similarity",
                                                                    search_kwargs={"k": 20}), combine_chain)

        # QA ì²´ì¸ ì €ì¥
        st.session_state.qa_chain = qa_chain
        st.session_state.pdf_completed = True  

        # ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ ë©”ì‹œì§€ ì¶”ê°€ (í•œ ë²ˆë§Œ ì‹¤í–‰ë¨)
        st.session_state.messages.append({
            'role': 'assistant', 
            'content': f'âœ… PDF íŒŒì¼ {uploaded_file.name}ì˜ ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'
        })

        # ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œëœ ì´í›„, ìµœì¢…ì ìœ¼ë¡œ í•œ ë²ˆë§Œ `st.rerun()`
        st.rerun()


# ì‚¬ìš©ì ì…ë ¥ ì‹œ ì²˜ë¦¬
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¨¼ì € ì¶œë ¥
    with st.chat_message("user"):
        st.write(user_input)
    
    # ì„¸ì…˜ ìƒíƒœì— ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    st.session_state.messages.append({
        'role': 'user', 
        'content': user_input
    })

    qa_chain = st.session_state.get("qa_chain")
    if not qa_chain:
        with st.chat_message("assistant"):
            st.write("âŒ ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")
        st.session_state.messages.append({
            'role': 'assistant', 
            'content': "âŒ ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”."
        })
        st.stop()

    # ë‹µë³€ ìƒì„± ì¤‘ ë¡œë”© í‘œì‹œ
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                response = st.session_state.qa_chain.invoke({"input": user_input})
                answer = response["answer"]

            except Exception as e:
                answer = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        
        st.write(answer)

    # ì„¸ì…˜ ìƒíƒœì— ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥
    st.session_state.messages.append({
        'role': 'assistant', 
        'content': answer
    })