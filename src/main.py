import torch
torch.classes.__path__ = [] # í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì„ì‹œ ì¡°ì¹˜
import tempfile
import os
import streamlit as st
from streamlit_pdf_viewer import pdf_viewer
import logging
import json
import re
import html
from utils import (
    SessionManager,
    get_ollama_models,
    load_llm,
    process_pdf,
    update_qa_chain as util_update_qa_chain, # utils.pyì˜ update_qa_chain ì‚¬ìš©
    RETRIEVER_CONFIG,  # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ìƒìˆ˜ import
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAG Chatbot",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
SessionManager.init_session()

def handle_model_change(selected_model: str):
    """ëª¨ë¸ ë³€ê²½ ì²˜ë¦¬"""
    if not selected_model or selected_model == st.session_state.get("last_selected_model"):
        return

    old_model = SessionManager.update_model(selected_model)
    logging.info(f"LLM ë³€ê²½ ê°ì§€: {old_model} -> {selected_model}")

    if not st.session_state.get("pdf_processed"):
        logging.info(f"ëª¨ë¸ ì„ íƒ ë³€ê²½ë¨ (PDF ë¯¸ì²˜ë¦¬ ìƒíƒœ): {selected_model}")
        return

    try:
        # 1. ìƒˆ LLM ë¡œë“œ
        with st.spinner(f"'{selected_model}' ëª¨ë¸ ë¡œë”© ì¤‘..."):
            st.session_state.llm = load_llm(selected_model)

        # 2. QA ì²´ì¸ ì—…ë°ì´íŠ¸
        if st.session_state.get("vector_store") and st.session_state.get("llm"):
            with st.spinner("QA ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì¤‘..."):
                st.session_state.qa_chain = util_update_qa_chain( # utils.pyì˜ í•¨ìˆ˜ ì‚¬ìš©
                    st.session_state.llm,
                    st.session_state.vector_store
                )
                logging.info(f"'{selected_model}' ëª¨ë¸ë¡œ QA ì²´ì¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
                # ëª¨ë¸ ë³€ê²½ ì™„ë£Œ ë©”ì‹œì§€ ì¶”ê°€
                success_message = f"âœ… '{selected_model}' ëª¨ë¸ë¡œ ë³€ê²½ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                SessionManager.add_message("assistant", success_message)
                st.session_state.last_model_change_message = success_message
        else:
            raise ValueError("ë²¡í„° ì €ì¥ì†Œ ë˜ëŠ” LLMì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDF ì¬ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        error_msg = f"ëª¨ë¸ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(f"{error_msg} ({selected_model})", exc_info=True)
        SessionManager.reset_session_state(["llm", "qa_chain"]) # pdf_processedëŠ” ìœ ì§€
        SessionManager.add_message("assistant", f"âŒ {error_msg}")
        st.session_state.last_model_change_message = f"âŒ {error_msg}"
        
    st.rerun()  # ì§ì ‘ rerun í˜¸ì¶œ

def handle_pdf_upload(uploaded_file):
    """PDF íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬"""
    if not uploaded_file:
        return

    if uploaded_file.name == st.session_state.get("last_uploaded_file_name"):
        return

    try:
        # 1. ì´ì „ PDF íŒŒì¼ ì •ë¦¬
        if st.session_state.temp_pdf_path and os.path.exists(st.session_state.temp_pdf_path):
            try:
                os.remove(st.session_state.temp_pdf_path)
                logging.info("ì´ì „ ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ ì„±ê³µ")
            except Exception as e:
                logging.warning(f"ì´ì „ ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

        # 2. ìƒˆ PDF íŒŒì¼ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded_file.getvalue())
            st.session_state.temp_pdf_path = tmp.name
            logging.info(f"ì„ì‹œ PDF íŒŒì¼ ìƒì„± ì„±ê³µ: {st.session_state.temp_pdf_path}")
        
        # 3. ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹
        SessionManager.reset_for_new_file(uploaded_file)
        
        # 4. ì´ˆê¸° ë©”ì‹œì§€ ì¶”ê°€
        SessionManager.add_message(
            "assistant", (
                f"ğŸ“‚ ìƒˆ PDF íŒŒì¼ '{uploaded_file.name}'ì´(ê°€) ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                "ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."
                )
        )
        
        # 5. í•œ ë²ˆë§Œ ë¦¬ëŸ°
        st.rerun()
        
    except Exception as e:
        error_msg = f"ì„ì‹œ PDF íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}"
        logging.error(error_msg)
        st.error(error_msg)
        st.session_state.temp_pdf_path = None

def handle_pdf_processing(uploaded_file):
    """PDF ì²˜ë¦¬ ìƒíƒœ ê´€ë¦¬ ë° ì‹¤í–‰"""
    if not (uploaded_file and st.session_state.temp_pdf_path):
        return

    if (st.session_state.get("pdf_processed") or 
        st.session_state.get("pdf_processing_error") or 
        st.session_state.get("pdf_is_processing")):
        return

    current_selected_model = st.session_state.get("last_selected_model")
    if not current_selected_model:
        st.warning("ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return

    st.session_state.pdf_is_processing = True
    SessionManager.add_message("assistant", f"â³ '{uploaded_file.name}' ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
    
    try:
        process_pdf(uploaded_file, current_selected_model, st.session_state.temp_pdf_path)
    except Exception as e:
        error_msg = f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg)
        SessionManager.set_error_state(error_msg)
    finally:
        st.session_state.pdf_is_processing = False

def _parse_llm_output(full_llm_output: str) -> tuple[str, str, dict | None]:
    """LLMì˜ ì „ì²´ ì¶œë ¥ì„ ìƒê° ê³¼ì •ê³¼ JSON ë¶€ë¶„ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  JSONì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    thought_content = ""
    json_to_parse = full_llm_output.strip()

    if json_to_parse.startswith("<think>"):
        think_end_tag = "</think>"
        think_end_idx = json_to_parse.find(think_end_tag)
        if think_end_idx != -1:
            thought_content = json_to_parse[len("<think>"):think_end_idx].strip()
            json_to_parse = json_to_parse[think_end_idx + len(think_end_tag):].strip()
    
    try:
        response_data = json.loads(json_to_parse)
        return thought_content, json_to_parse, response_data
    except json.JSONDecodeError:
        logging.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨. ì›ë³¸ LLM ì‘ë‹µ: {json_to_parse}")
        return thought_content, json_to_parse, None

def _validate_and_reformat_sources(
    answer_text: str, 
    llm_sources: list[dict], 
    session_source_docs: dict
) -> tuple[str, list[dict]]:
    """
    LLM ì‘ë‹µì˜ answerì™€ sourcesë¥¼ ê²€ì¦í•˜ê³ , í•„ìš”í•œ ê²½ìš° ì¶œì²˜ ë²ˆí˜¸ë¥¼ ì¬ì •ë ¬í•˜ë©°,
    íˆ´íŒì— ì‚¬ìš©ë  ìµœì¢… sources ì •ë³´ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    source_pattern = r'\[(\d+)\]'
    
    original_ids_in_answer = sorted(list(set(map(int, re.findall(source_pattern, answer_text)))))

    if not original_ids_in_answer:
        return answer_text, []

    id_map = {original_id: new_id for new_id, original_id in enumerate(original_ids_in_answer, 1)}
    
    def replace_id_in_answer(match):
        original_id = int(match.group(1))
        return f"[{id_map.get(original_id, original_id)}]"
    
    updated_answer_text = re.sub(source_pattern, replace_id_in_answer, answer_text)

    final_sources_for_tooltip = []
    llm_sources_map = {s.get("id"): s for s in llm_sources if isinstance(s.get("id"), int)}

    for original_id in original_ids_in_answer:
        new_id = id_map[original_id]
        
        source_info_from_llm = llm_sources_map.get(original_id)
        actual_doc_chunk = session_source_docs.get(str(original_id))

        text_for_tooltip = "ì¶œì²˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        page_for_tooltip = "N/A"

        if actual_doc_chunk and 'content' in actual_doc_chunk:
            text_for_tooltip = actual_doc_chunk['content']
            page_for_tooltip = actual_doc_chunk.get('page', 'N/A') # ì´ë¯¸ ë¬¸ìì—´ ë˜ëŠ” "N/A"
            logging.info(f"íˆ´íŒ ID {new_id} (ì›ë³¸ LLM ID: {original_id}): ì‹¤ì œ ë¬¸ì„œ ì¡°ê° ì‚¬ìš© (í˜ì´ì§€: {page_for_tooltip}).")
        elif source_info_from_llm and isinstance(source_info_from_llm.get("text"), str):
            text_for_tooltip = source_info_from_llm["text"]
            page_for_tooltip = source_info_from_llm.get("page", "N/A") # LLMì´ ë¬¸ìì—´ë¡œ ì œê³µí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ "N/A"
            logging.warning(f"íˆ´íŒ ID {new_id} (ì›ë³¸ LLM ID: {original_id}): ì‹¤ì œ ë¬¸ì„œ ì¡°ê° ì—†ìŒ. LLM ì œê³µ text ì‚¬ìš© (í˜ì´ì§€: {page_for_tooltip}).")
        else:
            logging.error(f"íˆ´íŒ ID {new_id} (ì›ë³¸ LLM ID: {original_id}): ì‹¤ì œ ë¬¸ì„œ ì¡°ê° ë° LLM ì œê³µ text ëª¨ë‘ ë¶€ì í•©/ëˆ„ë½.")

        final_sources_for_tooltip.append({
            "id": new_id,
            "text": text_for_tooltip,
            "page": page_for_tooltip,
            "original_llm_id": original_id
        })
        
    final_sources_for_tooltip.sort(key=lambda s: s["id"])
            
    return updated_answer_text, final_sources_for_tooltip

def _create_tooltip_html(source_id: str, source_text: str, source_page: str) -> str:
    """ê°œë³„ ì¶œì²˜ì— ëŒ€í•œ íˆ´íŒ HTMLì„ ìƒì„±í•©ë‹ˆë‹¤."""
    escaped_tooltip_text = html.escape(source_text)
    page_info = f" (í˜ì´ì§€ {source_page})" if source_page and source_page != "N/A" else ""
    
    tooltip_content_html = (
        f'<div style="margin-bottom:8px;padding-bottom:8px;border-bottom:1px solid #dee2e6;color:#666;font-size:0.8em">'
        f'ì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš©{page_info}:</div>'
        f'<div style="white-space: pre-wrap; word-wrap: break-word;">{escaped_tooltip_text}</div>'
    )
    
    return (
        f'<span style="position:relative;display:inline">'
        f'<sup style="color:#666;font-size:0.8em;cursor:help;margin-left:2px">[{source_id}]</sup>'
        f'<div style="visibility:hidden;position:absolute;z-index:1000;width:400px;background-color:#f8f9fa;'
        f'color:#333;text-align:left;border-radius:4px;padding:10px;box-shadow:0 2px 4px rgba(0,0,0,0.1);'
        f'font-size:0.9em;line-height:1.4;left:50%;transform:translateX(-50%);top:100%;margin-top:5px;'
        f'opacity:0;transition:opacity 0.2s; max-height: 300px; overflow-y: auto;">'
        f'{tooltip_content_html}</div></span>'
    )

def process_chat_response(qa_chain, user_input, chat_container):
    """ì±„íŒ… ì‘ë‹µ ì²˜ë¦¬"""
    with chat_container:
        with st.chat_message("assistant"):
            thought_expander = st.expander("ğŸ¤” ìƒê° ê³¼ì •", expanded=False)
            message_placeholder = st.empty()
            message_placeholder.write("ë‹µë³€ ìƒì„± ì¤‘...") # ì´ˆê¸° ë©”ì‹œì§€

            try:
                logging.info("ë‹µë³€ ìƒì„± ì‹œì‘...")
                
                full_llm_output_chunks = []
                accumulated_raw_output = ""
                displayed_thought_once = False # ìƒê° ê³¼ì •ì´ í•œ ë²ˆì´ë¼ë„ í‘œì‹œë˜ì—ˆëŠ”ì§€ ì¶”ì 
                
                # 1. Streamì„ ì‚¬ìš©í•˜ì—¬ LLM ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ìŒ
                for chunk in qa_chain.stream({"input": user_input}):
                    full_llm_output_chunks.append(chunk)
                    accumulated_raw_output = "".join(full_llm_output_chunks)

                    # "ìƒê° ê³¼ì •" ë™ì  ì—…ë°ì´íŠ¸
                    think_start_tag = "<think>"
                    think_end_tag = "</think>"
                    start_idx = accumulated_raw_output.find(think_start_tag)

                    if start_idx != -1:
                        # ì²« ë²ˆì§¸ <think> ë¸”ë¡ì˜ ëì„ ì°¾ìŒ
                        current_think_block_end_idx = accumulated_raw_output.find(think_end_tag, start_idx)
                        if current_think_block_end_idx != -1:
                            # ì™„ì „í•œ <think>...</think> ë¸”ë¡ì„ ì°¾ìŒ
                            current_thought_content = accumulated_raw_output[
                                start_idx + len(think_start_tag) : current_think_block_end_idx
                            ].strip()
                            thought_expander.markdown(current_thought_content)
                            displayed_thought_once = True
                            # ìƒê° ê³¼ì •ì´ í‘œì‹œëœ í›„ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
                            if message_placeholder: # placeholderê°€ ì•„ì§ ìœ íš¨í•œ ê²½ìš°
                                message_placeholder.write("ìƒê° ì™„ë£Œ, ë‹µë³€ êµ¬ì„± ì¤‘...")
                        else:
                            # <think> ì‹œì‘ì€ í–ˆì§€ë§Œ ì•„ì§ ëë‚˜ì§€ ì•ŠìŒ
                            partial_thought = accumulated_raw_output[start_idx + len(think_start_tag):].strip()
                            thought_expander.markdown(f"{partial_thought}...") # ì§„í–‰ ì¤‘ì„ì„ í‘œì‹œ
                            if message_placeholder:
                                message_placeholder.write("ìƒê° ì¤‘...")
                    elif not displayed_thought_once and message_placeholder: # ì•„ì§ <think> íƒœê·¸ê°€ ì—†ê±°ë‚˜, ì´ë¯¸ <think> ë¸”ë¡ì´ ëë‚¬ê³  JSON ë¶€ë¶„ì¼ ë•Œ
                        message_placeholder.write("ë‹µë³€ ìˆ˜ì‹  ì¤‘...")
                
                full_llm_output = accumulated_raw_output # ìµœì¢… ëˆ„ì ëœ ê²°ê³¼
                if not full_llm_output:
                    raise ValueError("LLMìœ¼ë¡œë¶€í„° ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤.")

                # 2. LLM ì¶œë ¥ íŒŒì‹± (ìƒê° ê³¼ì •, raw JSON, íŒŒì‹±ëœ ë°ì´í„°)
                thought_content, raw_json_part, response_data = _parse_llm_output(full_llm_output)

                # 3. ìµœì¢… ìƒê° ê³¼ì • í‘œì‹œ (ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—…ë°ì´íŠ¸ ë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë‚˜, ìµœì¢… íŒŒì‹± ê²°ê³¼ë¡œ í•œ ë²ˆ ë” ì—…ë°ì´íŠ¸)
                if thought_content:
                    thought_expander.markdown(thought_content)
                else:
                    thought_expander.empty()

                # 4. JSON íŒŒì‹± ê²°ê³¼ í™•ì¸ ë° ë‹µë³€ ì²˜ë¦¬
                if response_data is None: # JSON íŒŒì‹± ì‹¤íŒ¨
                    error_content = f"JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. LLMìœ¼ë¡œë¶€í„° ë°›ì€ JSON ë¶€ë¶„: \n```json\n{html.escape(raw_json_part)}\n```"
                    message_placeholder.error(error_content)
                    SessionManager.add_message("assistant", f"JSON íŒŒì‹± ì‹¤íŒ¨: {raw_json_part}")
                    return # ì—¬ê¸°ì„œ í•¨ìˆ˜ ì¢…ë£Œ

                answer = response_data.get("answer", "")
                llm_provided_sources = response_data.get("sources", [])

                if not answer:
                    message_placeholder.markdown("LLMìœ¼ë¡œë¶€í„° ë‹µë³€ ë‚´ìš©ì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    SessionManager.add_message("assistant", "LLMìœ¼ë¡œë¶€í„° ë‹µë³€ ë‚´ìš©ì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    return

                session_source_documents = st.session_state.get("source_documents", {})
                
                processed_answer, tooltip_sources = _validate_and_reformat_sources(
                    answer, 
                    llm_provided_sources, 
                    session_source_documents
                )
                
                tooltip_sources_map = {str(s["id"]): s for s in tooltip_sources}

                source_pattern = r'\[(\d+)\]'

                def replace_with_tooltip(match):
                    source_id_str = match.group(1)
                    source_data = tooltip_sources_map.get(source_id_str)
                    if source_data:
                        return _create_tooltip_html(
                            source_id_str, 
                            source_data["text"], 
                            source_data["page"]
                        )
                    return match.group(0)

                formatted_answer_with_tooltips = re.sub(source_pattern, replace_with_tooltip, processed_answer)
                
                tooltip_style = '<style>span:hover div{visibility:visible !important;opacity:1 !important}</style>'
                final_html_output = tooltip_style + formatted_answer_with_tooltips
                
                message_placeholder.markdown(final_html_output, unsafe_allow_html=True)
                SessionManager.add_message("assistant", final_html_output)

            except Exception as e:
                logging.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                error_message = f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                message_placeholder.error(error_message)
                SessionManager.add_message("assistant", error_message)

def display_chat_messages(chat_container):
    """ì±„íŒ… ì»¨í…Œì´ë„ˆì— ëª¨ë“  ë©”ì‹œì§€ë¥¼ í‘œì‹œ"""
    with chat_container:
        if "messages" in st.session_state:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"], unsafe_allow_html=True)

def main():
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ Settings")
        selected_model = None  # í•­ìƒ ì´ˆê¸°í™”
        try:
            models = get_ollama_models()
            last_model = st.session_state.get("last_selected_model")
            current_model_index = models.index(last_model) if last_model and last_model in models else 0
            if models:
                selected_model = st.selectbox(
                    "Select an Ollama model",
                    models,
                    index=current_model_index,
                    key="model_selector"
                )
            else:
                st.text("Failed to load Ollama models.")
        except Exception as e:
            st.error(f"Failed to load Ollama models: {e}")
            st.warning("Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€, Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

        # ëª¨ë¸ ë³€ê²½ ì‹œ ì²˜ë¦¬
        # ì„¸ì…˜ ìƒíƒœì— last_selected_modelì´ ì—†ê±°ë‚˜ ì„ íƒëœ ëª¨ë¸ê³¼ ë‹¤ë¥¼ ê²½ìš°
        if (
            selected_model
            and selected_model != st.session_state.get("last_selected_model")
        ):
            handle_model_change(selected_model)

        uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")

        # PDF ë·°ì–´ ì„¤ì •
        st.divider()
        resolution_boost = st.slider("Resolution boost", 1, 10, 1)
        width = st.slider("PDF width", 100, 1000, 1000)
        height = st.slider("PDF height", 100, 10000, 1000) # ìµœì†Œ ë†’ì´ë¥¼ 100ìœ¼ë¡œ ë³€ê²½

    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    col_left, col_right = st.columns([1, 1])

    # ì˜¤ë¥¸ìª½ ì»¬ëŸ¼: PDF ë¯¸ë¦¬ë³´ê¸°
    with col_right:
        st.subheader("ğŸ“„ PDF Preview")
        handle_pdf_upload(uploaded_file)
        
        if st.session_state.temp_pdf_path and os.path.exists(st.session_state.temp_pdf_path):
            try:
                pdf_viewer(
                    input=st.session_state.temp_pdf_path,
                    width=width,
                    height=height,
                    key=f'pdf_viewer_{os.path.basename(st.session_state.temp_pdf_path)}',
                    resolution_boost=resolution_boost
                )
            except Exception as e:
                st.error(f"PDF ë¯¸ë¦¬ë³´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        elif uploaded_file:
            st.warning("PDF ë¯¸ë¦¬ë³´ê¸°ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì™¼ìª½ ì»¬ëŸ¼: ì±„íŒ… ë° ì„¤ì •
    with col_left:
        st.subheader("ğŸ’¬ Chat")
        
        # ì±„íŒ… ì»¨í…Œì´ë„ˆ
        chat_container = st.container(height=500, border=True)
        # JavaScript ê¸°ë°˜ position:fixed íˆ´íŒ ì‚¬ìš© ì‹œ ìœ„ CSS ì˜¤ë²„ë¼ì´ë“œëŠ” ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        
        # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
        display_chat_messages(chat_container)

        # PDF ì²˜ë¦¬ ê´€ë ¨ ë¡œì§
        if not st.session_state.get("pdf_processed"):
            handle_pdf_processing(uploaded_file)

        # ì±„íŒ… ì…ë ¥ UI
        user_input = st.chat_input(
            "PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.",
            key='user_input',
            disabled=not SessionManager.is_ready_for_chat()
        )

        # ìƒˆ ë©”ì‹œì§€ ì²˜ë¦¬
        if user_input:
            SessionManager.add_message("user", user_input)
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(user_input)
                
            # QA ì²´ì¸ ê²€ì¦ ë° ì‘ë‹µ ìƒì„±
            qa_chain = st.session_state.get("qa_chain")
            if not qa_chain:
                error_message = "âŒ QA ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³€ê²½ì´ ì§„í–‰ ì¤‘ì´ê±°ë‚˜ PDF ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                with st.chat_message("assistant"):
                    st.markdown(error_message)
                SessionManager.add_message("assistant", error_message)
            else:
                try:
                    process_chat_response(qa_chain, user_input, chat_container)
                except Exception as e:
                    error_message = f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    with st.chat_message("assistant"):
                        st.markdown(error_message)
                    SessionManager.add_message("assistant", error_message)
                    logging.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)

if __name__ == "__main__":
    main()