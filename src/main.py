import torch
torch.classes.__path__ = []
import tempfile
import os
import streamlit as st
from streamlit_pdf_viewer import pdf_viewer
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import logging
from utils import (
    init_session_state,
    reset_session_state,
    prepare_chat_history,
    generate_example_questions,
    get_ollama_models,
    load_pdf_docs,
    get_embedder,
    split_documents,
    create_vector_store,
    init_llm,
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="RAG Chatbot", layout="wide")
st.title("ğŸ“„ RAG Chatbot with Ollama LLM")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
init_session_state()

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("Settings")
    try:
        models = get_ollama_models()
        current_model_index = models.index(st.session_state.last_selected_model) if st.session_state.last_selected_model in models else 0
        selected_model = st.selectbox(
            "Select an Ollama model",
            models,
            index=current_model_index
        ) if models else st.text("Failed to load Ollama models.")
    except Exception as e:
        st.error(f"Failed to load Ollama models: {e}")
        st.warning("Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€, Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        selected_model = None

    uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")

    # PDF ë·°ì–´ ì„¤ì •
    st.divider()
    st.header("ğŸ“„ PDF Viewer Settings")
    resolution_boost = st.slider(label="Resolution boost", min_value=1, max_value=10, value=1)
    width = st.slider(label="PDF width", min_value=100, max_value=1000, value=1000)
    height = st.slider(label="PDF height", min_value=-1, max_value=10000, value=1000)

# ë ˆì´ì•„ì›ƒ ì„¤ì •
col_left, col_right = st.columns([1, 1])

# ì˜¤ë¥¸ìª½ ì»¬ëŸ¼: PDF ë¯¸ë¦¬ë³´ê¸°
with col_right:
    st.header("ğŸ“„ PDF Preview")
    if uploaded_file and uploaded_file.name != st.session_state.get("last_uploaded_file_name"):
        if st.session_state.temp_pdf_path and os.path.exists(st.session_state.temp_pdf_path):
            try:
                os.remove(st.session_state.temp_pdf_path)
            except Exception as e:
                logging.warning(f"ì´ì „ ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(uploaded_file.getvalue())
                st.session_state.temp_pdf_path = tmp.name
        except Exception as e:
            st.error(f"ì„ì‹œ PDF íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            st.session_state.temp_pdf_path = None

    if st.session_state.temp_pdf_path and os.path.exists(st.session_state.temp_pdf_path):
        try:
            pdf_viewer(
                input=st.session_state.temp_pdf_path,
                width=width,
                height=height,
                key=f'pdf_viewer_{st.session_state.last_uploaded_file_name}',
                resolution_boost=resolution_boost
            )
        except Exception as e:
            st.error(f"PDF ë¯¸ë¦¬ë³´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    elif uploaded_file:
        st.warning("PDF ë¯¸ë¦¬ë³´ê¸°ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ì™¼ìª½ ì»¬ëŸ¼: ì±„íŒ… ë° ì„¤ì •
with col_left:
    st.header("ğŸ’¬ Chat")

    if selected_model and selected_model != st.session_state.last_selected_model:
        st.session_state.last_selected_model = selected_model
        if st.session_state.get("llm"):
            try:
                st.session_state.llm = init_llm(selected_model)
                if st.session_state.get("vector_store"):
                    QA_PROMPT = ChatPromptTemplate.from_messages([
                        ("system", "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ë§¥(context) ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë¬¸ë§¥ì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´, ëª¨ë¥¸ë‹¤ê³  ì†”ì§í•˜ê²Œ ë‹µí•˜ì„¸ìš”. ì¶”ì¸¡í•˜ê±°ë‚˜ ì™¸ë¶€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n<context>\n{context}\n</context>"),
                        MessagesPlaceholder(variable_name="chat_history"),
                        ("human", "{input}")
                    ])
                    combine_chain = create_stuff_documents_chain(st.session_state.llm, QA_PROMPT)
                    st.session_state.qa_chain = create_retrieval_chain(
                        st.session_state.vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 5}),
                        combine_chain
                    )
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": f"ğŸ› ï¸ ëª¨ë¸ì´ {selected_model}(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆê³  QA ì²´ì¸ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."
                    })
                else:
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": f"ğŸ› ï¸ ëª¨ë¸ì´ {selected_model}(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ í•´ë‹¹ ëª¨ë¸ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤."
                    })
                st.rerun()
            except Exception as e:
                st.error(f"LLM ë˜ëŠ” QA ì²´ì¸ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": f"âš ï¸ ëª¨ë¸ {selected_model} ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
                })
                st.rerun()

    new_file_uploaded = uploaded_file and uploaded_file.name != st.session_state.get("last_uploaded_file_name")

    if new_file_uploaded:
        reset_session_state(uploaded_file)
        st.session_state.messages.append({
            "role": "assistant",
            "content": f"ğŸ“‚ ìƒˆ PDF íŒŒì¼ '{uploaded_file.name}'ì´(ê°€) ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤..."
        })
        st.rerun()

    if uploaded_file and not st.session_state.pdf_processed and not st.session_state.pdf_processing_error:
        with st.spinner("ğŸ“„ PDF ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”."):
            try:
                logging.info("PDF ì²˜ë¦¬ ì‹œì‘...")
                file_bytes = uploaded_file.getvalue()

                logging.info("ë¬¸ì„œ ë¡œë”© ì¤‘...")
                docs = load_pdf_docs(file_bytes)
                if not docs: raise ValueError("PDF ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨")

                logging.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
                device = "cuda" if torch.cuda.is_available() else "cpu"
                embedder = get_embedder(model_name="BAAI/bge-m3",
                                        model_kwargs={'device': device},
                                        encode_kwargs={'normalize_embeddings': True, 'device': device})
                if not embedder: raise ValueError("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")

                logging.info("ë¬¸ì„œ ë¶„í•  ì¤‘...")
                documents = split_documents(docs, embedder)
                if not documents: raise ValueError("ë¬¸ì„œ ë¶„í•  ì‹¤íŒ¨")

                logging.info("ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
                vector_store = create_vector_store(documents, embedder)
                if not vector_store: raise ValueError("ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨")
                st.session_state.vector_store = vector_store

                logging.info("LLM ì´ˆê¸°í™” ì¤‘...")
                if isinstance(selected_model, str):
                    llm = init_llm(selected_model)
                    if not llm: raise ValueError("LLM ì´ˆê¸°í™” ì‹¤íŒ¨")
                    st.session_state.llm = llm
                else:
                    raise ValueError("LLM ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ëª¨ë¸ ë¯¸ì„ íƒ")

                logging.info("QA ì²´ì¸ ìƒì„± ì¤‘...")
                QA_PROMPT = ChatPromptTemplate.from_messages([
                    ("system", "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ë§¥(context) ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë¬¸ë§¥ì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´, ëª¨ë¥¸ë‹¤ê³  ì†”ì§í•˜ê²Œ ë‹µí•˜ì„¸ìš”. ì¶”ì¸¡í•˜ê±°ë‚˜ ì™¸ë¶€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n<context>\n{context}\n</context>"),
                    MessagesPlaceholder(variable_name="chat_history"),
                    ("human", "{input}")
                ])
                combine_chain = create_stuff_documents_chain(st.session_state.llm, QA_PROMPT)
                qa_chain = create_retrieval_chain(
                    st.session_state.vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 10}),
                    combine_chain
                )
                st.session_state.qa_chain = qa_chain
                st.session_state.pdf_processed = True
                logging.info("PDF ì²˜ë¦¬ ì™„ë£Œ.")

                st.session_state.messages.append({
                    "role": "assistant",
                    "content": f"âœ… PDF íŒŒì¼ '{uploaded_file.name}'ì˜ ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                })

                generate_example_questions()
                st.rerun()

            except Exception as e:
                logging.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                st.session_state.pdf_processing_error = str(e)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": f"âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                })
                st.rerun()

    # ì±„íŒ… ì»¨í…Œì´ë„ˆ ë° ë©”ì‹œì§€ í‘œì‹œ
    chat_container = st.container(height=500, border=True)
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.write(message["content"])

    is_ready_for_input = st.session_state.pdf_processed and not st.session_state.pdf_processing_error

    user_input = st.chat_input(
        "PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.",
        key='user_input',
        disabled=not is_ready_for_input
    )

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with chat_container:
            with st.chat_message("user"):
                st.write(user_input)

        qa_chain = st.session_state.get("qa_chain")
        if not qa_chain:
            error_message = "âŒ QA ì²´ì¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDF ë¬¸ì„œë¥¼ ë¨¼ì € ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤."
            st.session_state.messages.append({"role": "assistant", "content": error_message})
            with chat_container:
                with st.chat_message("assistant"):
                    st.warning(error_message)

        if qa_chain:
            with chat_container:
                with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    message_placeholder.write("â–Œ")
                    try:
                        chat_history = prepare_chat_history()
                        full_response = ""
                        stream = qa_chain.stream({
                            "input": user_input,
                            "chat_history": chat_history
                        })
                        for chunk in stream:
                            answer_part = chunk.get("answer", "")
                            if answer_part:
                                full_response += answer_part
                                message_placeholder.write(full_response + "â–Œ")
                        message_placeholder.write(full_response)
                    except Exception as e:
                        logging.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                        full_response = f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                        message_placeholder.error(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})