import torch
torch.classes.__path__ = [] # í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì„ì‹œ ì¡°ì¹˜
import tempfile
import os
import streamlit as st
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from streamlit_pdf_viewer import pdf_viewer
import logging
from utils import (
    SessionManager,
    get_ollama_models,
    load_llm,
    QA_PROMPT,
    process_pdf,
    RETRIEVER_CONFIG,  # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ìƒìˆ˜ import
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAG Chatbot",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
SessionManager.init_session()

def update_qa_chain(llm, vector_store):
    """QA ì²´ì¸ ì—…ë°ì´íŠ¸"""
    try:
        combine_chain = create_stuff_documents_chain(llm, QA_PROMPT)
        retriever = vector_store.as_retriever(
            search_type=RETRIEVER_CONFIG['search_type'],
            search_kwargs=RETRIEVER_CONFIG['search_kwargs']
        )
        return create_retrieval_chain(retriever, combine_chain)
    except Exception as e:
        raise ValueError(f"QA ì²´ì¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def handle_model_change(selected_model: str):
    """ëª¨ë¸ ë³€ê²½ ì²˜ë¦¬"""
    if not selected_model or selected_model == st.session_state.get("last_selected_model"):
        return

    old_model = SessionManager.update_model(selected_model)
    logging.info(f"LLM ë³€ê²½ ê°ì§€: {old_model} -> {selected_model}")

    if not st.session_state.get("pdf_processed"):
        logging.info(f"ëª¨ë¸ ì„ íƒ ë³€ê²½ë¨ (PDF ë¯¸ì²˜ë¦¬ ìƒíƒœ): {selected_model}")
        return

    try:
        # 1. ìƒˆ LLM ë¡œë“œ
        with st.spinner(f"'{selected_model}' ëª¨ë¸ ë¡œë”© ì¤‘..."):
            st.session_state.llm = load_llm(selected_model)

        # 2. QA ì²´ì¸ ì—…ë°ì´íŠ¸
        if st.session_state.get("vector_store") and st.session_state.get("llm"):
            with st.spinner("QA ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì¤‘..."):
                st.session_state.qa_chain = update_qa_chain(
                    st.session_state.llm,
                    st.session_state.vector_store
                )
                logging.info(f"'{selected_model}' ëª¨ë¸ë¡œ QA ì²´ì¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
        else:
            raise ValueError("ë²¡í„° ì €ì¥ì†Œ ë˜ëŠ” LLMì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDF ì¬ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        error_msg = f"ëª¨ë¸ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(f"{error_msg} ({selected_model})", exc_info=True)
        SessionManager.reset_session_state(["llm", "qa_chain", "pdf_processed"])
        SessionManager.add_message("assistant", f"âŒ {error_msg}")
        
    st.rerun()  # ì§ì ‘ rerun í˜¸ì¶œ

def handle_pdf_upload(uploaded_file):
    """PDF íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬"""
    if not uploaded_file:
        return

    if uploaded_file.name == st.session_state.get("last_uploaded_file_name"):
        return

    try:
        # 1. ì´ì „ PDF íŒŒì¼ ì •ë¦¬
        if st.session_state.temp_pdf_path and os.path.exists(st.session_state.temp_pdf_path):
            try:
                os.remove(st.session_state.temp_pdf_path)
                logging.info("ì´ì „ ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ ì„±ê³µ")
            except Exception as e:
                logging.warning(f"ì´ì „ ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

        # 2. ìƒˆ PDF íŒŒì¼ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded_file.getvalue())
            st.session_state.temp_pdf_path = tmp.name
            logging.info(f"ì„ì‹œ PDF íŒŒì¼ ìƒì„± ì„±ê³µ: {st.session_state.temp_pdf_path}")
        
        # 3. ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹
        SessionManager.reset_for_new_file(uploaded_file)
        
        # 4. ì´ˆê¸° ë©”ì‹œì§€ ì¶”ê°€
        SessionManager.add_message(
            "assistant", (
                f"ğŸ“‚ ìƒˆ PDF íŒŒì¼ '{uploaded_file.name}'ì´(ê°€) ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                "ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."
                )
        )
        
        # 5. í•œ ë²ˆë§Œ ë¦¬ëŸ°
        st.rerun()
        
    except Exception as e:
        error_msg = f"ì„ì‹œ PDF íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}"
        logging.error(error_msg)
        st.error(error_msg)
        st.session_state.temp_pdf_path = None

def handle_pdf_processing(uploaded_file):
    """PDF ì²˜ë¦¬ ìƒíƒœ ê´€ë¦¬ ë° ì‹¤í–‰"""
    if not (uploaded_file and st.session_state.temp_pdf_path):
        return

    if (st.session_state.get("pdf_processed") or 
        st.session_state.get("pdf_processing_error") or 
        st.session_state.get("pdf_is_processing")):
        return

    current_selected_model = st.session_state.get("last_selected_model")
    if not current_selected_model:
        st.warning("ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return

    st.session_state.pdf_is_processing = True
    SessionManager.add_message("assistant", f"â³ '{uploaded_file.name}' ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
    
    try:
        process_pdf(uploaded_file, current_selected_model, st.session_state.temp_pdf_path)
    except Exception as e:
        error_msg = f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg)
        SessionManager.set_error_state(error_msg)
    finally:
        st.session_state.pdf_is_processing = False

def process_thought_stream(chunk: str, thought_response: str) -> tuple[str, str, bool]:
    """ìƒê° ê³¼ì • ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
    if "</think>" in chunk:
        parts = chunk.split("</think>", 1)
        thought_part = parts[0]
        answer_part = parts[1]
        
        thought_response += thought_part
        cleaned_thought = thought_response.replace("<think>", "").strip()
        
        return cleaned_thought, answer_part, False
    return "", "", True

def process_chat_response(qa_chain, user_input, chat_container):
    """ì±„íŒ… ì‘ë‹µ ì²˜ë¦¬"""
    with chat_container:
        with st.chat_message("assistant"):
            thought_expander = st.expander("ğŸ¤” ìƒê° ê³¼ì •", expanded=False)
            message_placeholder = st.empty()
            message_placeholder.write("â–Œ")

            full_response = ""
            thought_response = ""
            processing_thought = True

            try:
                logging.info("ë‹µë³€ ìƒì„± ì‹œì‘...")
                stream = qa_chain.stream({"input": user_input})
                
                for chunk in stream:
                    answer_part = chunk.get("answer", "")
                    if not answer_part:
                        continue

                    if processing_thought:
                        cleaned_thought, remaining_answer, processing_thought = process_thought_stream(
                            answer_part, thought_response
                        )
                        
                        if cleaned_thought:
                            thought_expander.markdown(cleaned_thought)
                            thought_response = cleaned_thought
                        
                        if not processing_thought:
                            full_response = remaining_answer
                            if full_response:
                                message_placeholder.write(full_response + "â–Œ")
                        else:
                            thought_response += answer_part
                    else:
                        full_response += answer_part
                        message_placeholder.write(full_response + "â–Œ")

                # ìµœì¢… ì‘ë‹µ ì²˜ë¦¬
                if processing_thought:
                    full_response = thought_response.replace("<think>", "").strip()
                message_placeholder.write(full_response)

            except Exception as e:
                logging.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                error_message = f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                message_placeholder.error(error_message)
                full_response = error_message

            return full_response

def main():
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ Settings")
        try:
            models = get_ollama_models()
            last_model = st.session_state.get("last_selected_model")
            current_model_index = models.index(last_model) if last_model and last_model in models else 0
            selected_model = st.selectbox(
                "Select an Ollama model",
                models,
                index=current_model_index,
                key="model_selector"
            ) if models else st.text("Failed to load Ollama models.")
            
            if selected_model:
                handle_model_change(selected_model)
                
        except Exception as e:
            st.error(f"Failed to load Ollama models: {e}")
            st.warning("Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€, Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            selected_model = None

        uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")

        # PDF ë·°ì–´ ì„¤ì •
        st.divider()
        resolution_boost = st.slider("Resolution boost", 1, 10, 1)
        width = st.slider("PDF width", 100, 1000, 1000)
        height = st.slider("PDF height", -1, 10000, 1000)

    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    col_left, col_right = st.columns([1, 1])

    # ì˜¤ë¥¸ìª½ ì»¬ëŸ¼: PDF ë¯¸ë¦¬ë³´ê¸°
    with col_right:
        st.subheader("ğŸ“„ PDF Preview")
        handle_pdf_upload(uploaded_file)
        
        if st.session_state.temp_pdf_path and os.path.exists(st.session_state.temp_pdf_path):
            try:
                pdf_viewer(
                    input=st.session_state.temp_pdf_path,
                    width=width,
                    height=height,
                    key=f'pdf_viewer_{os.path.basename(st.session_state.temp_pdf_path)}',
                    resolution_boost=resolution_boost
                )
            except Exception as e:
                st.error(f"PDF ë¯¸ë¦¬ë³´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        elif uploaded_file:
            st.warning("PDF ë¯¸ë¦¬ë³´ê¸°ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì™¼ìª½ ì»¬ëŸ¼: ì±„íŒ… ë° ì„¤ì •
    with col_left:
        st.subheader("ğŸ’¬ Chat")
        
        # ì±„íŒ… ì»¨í…Œì´ë„ˆ
        chat_container = st.container(height=500, border=True)
        
        # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
        with chat_container:
            if "messages" in st.session_state:
                for message in st.session_state.messages:
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"])
        
        # PDF ì²˜ë¦¬ ê´€ë ¨ ë¡œì§
        if not st.session_state.get("pdf_processed"):
            handle_pdf_processing(uploaded_file)

        # ì±„íŒ… ì…ë ¥ UI
        user_input = st.chat_input(
            "PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.",
            key='user_input',
            disabled=not SessionManager.is_ready_for_chat()
        )

        # ìƒˆ ë©”ì‹œì§€ ì²˜ë¦¬
        if user_input:
            SessionManager.add_message("user", user_input)
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(user_input)
                
            # QA ì²´ì¸ ê²€ì¦ ë° ì‘ë‹µ ìƒì„±
            qa_chain = st.session_state.get("qa_chain")
            if not qa_chain:
                error_message = "âŒ QA ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³€ê²½ì´ ì§„í–‰ ì¤‘ì´ê±°ë‚˜ PDF ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                with st.chat_message("assistant"):
                    st.markdown(error_message)
                SessionManager.add_message("assistant", error_message)
            else:
                try:
                    response = process_chat_response(qa_chain, user_input, chat_container)
                    SessionManager.add_message("assistant", response)
                except Exception as e:
                    error_message = f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    with st.chat_message("assistant"):
                        st.markdown(error_message)
                    SessionManager.add_message("assistant", error_message)
                    logging.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)

if __name__ == "__main__":
    main()