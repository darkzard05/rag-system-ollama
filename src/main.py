import torch
torch.classes.__path__ = [] # í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì„ì‹œ ì¡°ì¹˜
import tempfile
import os
import streamlit as st
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from streamlit_pdf_viewer import pdf_viewer
import logging
from utils import (
    init_session_state,
    reset_session_state,
    get_ollama_models,
    load_llm,
    QA_PROMPT,
    process_pdf,
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAG Chatbot",
    layout="wide"
    )

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
    )

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
init_session_state()

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ Settings")
    try:
        models = get_ollama_models()
        # ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ ë§ˆì§€ë§‰ ëª¨ë¸ í™•ì¸ ë° ì¸ë±ìŠ¤ ì„¤ì •
        last_model = st.session_state.get("last_selected_model")
        current_model_index = models.index(last_model) if last_model and last_model in models else 0
        selected_model = st.selectbox(
            "Select an Ollama model",
            models,
            index=current_model_index,
            key="model_selector" # ìœ„ì ¯ ìƒíƒœ ìœ ì§€ë¥¼ ìœ„í•œ í‚¤ ì¶”ê°€
        ) if models else st.text("Failed to load Ollama models.")
    except Exception as e:
        st.error(f"Failed to load Ollama models: {e}")
        st.warning("Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€, Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        selected_model = None

    # --- ëª¨ë¸ ë³€ê²½ ê°ì§€ ë° ì²˜ë¦¬ ë¡œì§ ---
    if selected_model and selected_model != st.session_state.get("last_selected_model"):
        old_model = st.session_state.get("last_selected_model", "N/A")
        st.session_state.last_selected_model = selected_model
        st.session_state.llm = None # ì´ì „ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ ì œê±°
        st.session_state.qa_chain = None # ì´ì „ QA ì²´ì¸ ìƒíƒœ ì œê±°

        logging.info(f"LLM ë³€ê²½ ê°ì§€: {old_model} -> {selected_model}")

        if st.session_state.get("pdf_processed"):
            # PDFê°€ ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš°, ìƒˆ ëª¨ë¸ë¡œ LLM ë° QA ì²´ì¸ ì¬ìƒì„±
            try:
                # 1. ìƒˆ LLM ë¡œë“œ ë° ì €ì¥
                with st.spinner(f"'{selected_model}' ëª¨ë¸ ë¡œë”© ì¤‘..."):
                    # utilsì—ì„œ load_llm í•¨ìˆ˜ ì‚¬ìš©
                    new_llm = load_llm(selected_model)
                    st.session_state.llm = new_llm

                # 2. QA ì²´ì¸ ì¬ìƒì„± (ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©)
                if st.session_state.get("vector_store") and st.session_state.get("llm"):
                    with st.spinner("QA ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì¤‘..."):
                        # ë²¡í„° ì €ì¥ì†Œì™€ LLMì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ QA ì²´ì¸ ì—…ë°ì´íŠ¸
                        combine_chain = create_stuff_documents_chain(st.session_state.llm, QA_PROMPT)
                        retriever = st.session_state.vector_store.as_retriever(
                            search_type="mmr",
                            search_kwargs={'k': 5, 'fetch_k': 20, 'lambda_mult': 0.7},
                        )
                        new_qa_chain = create_retrieval_chain(retriever, combine_chain)
                        st.session_state.qa_chain = new_qa_chain
                        logging.info(f"'{selected_model}' ëª¨ë¸ë¡œ QA ì²´ì¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
                        # ì±„íŒ…ì— ë³€ê²½ ì™„ë£Œ ë©”ì‹œì§€ ì¶”ê°€
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": f"âœ… ëª¨ë¸ì´ '{selected_model}'(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤."
                        })
                        st.rerun() # ëª¨ë¸ ë³€ê²½ ë° QA ì²´ì¸ ì—…ë°ì´íŠ¸ í›„ ì¦‰ì‹œ ë°˜ì˜
                else:
                    st.error("QA ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: ë²¡í„° ì €ì¥ì†Œ ë˜ëŠ” LLMì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    logging.error("QA ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: ë²¡í„° ì €ì¥ì†Œ ë˜ëŠ” LLM ìƒíƒœ ì—†ìŒ")
                    st.session_state.pdf_processed = False # ì²˜ë¦¬ ì‹¤íŒ¨ ìƒíƒœë¡œ ë³€ê²½
                    st.session_state.qa_chain = None
                    # ì˜¤ë¥˜ ë©”ì‹œì§€ëŠ” ë‹¤ìŒ ì‚¬ìš©ì ì…ë ¥ ì‹œ ë˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ rerun ì‹œ í‘œì‹œë¨

            except Exception as e:
                st.error(f"ëª¨ë¸ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                logging.error(f"ëª¨ë¸ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ({selected_model}): {e}", exc_info=True)
                st.session_state.llm = None
                st.session_state.qa_chain = None
                st.session_state.pdf_processed = False # ì˜¤ë¥˜ ì‹œ ì²˜ë¦¬ ì‹¤íŒ¨ ìƒíƒœë¡œ ë³€ê²½
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": f"âŒ ëª¨ë¸ì„ '{selected_model}'(ìœ¼)ë¡œ ë³€ê²½í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                })
                st.rerun() # ì˜¤ë¥˜ ë°œìƒ í›„ ì¦‰ì‹œ ë°˜ì˜

        else:
            # PDFê°€ ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²½ìš°, ë¡œê·¸ë§Œ ë‚¨ê¹€ (ì‚¬ìš©ìì—ê²ŒëŠ” íŒŒì¼ ì—…ë¡œë“œ ì‹œ ë°˜ì˜ë¨)
            logging.info(f"ëª¨ë¸ ì„ íƒ ë³€ê²½ë¨ (PDF ë¯¸ì²˜ë¦¬ ìƒíƒœ): {selected_model}. PDF ì—…ë¡œë“œ ì‹œ ì ìš©ë©ë‹ˆë‹¤.")
            st.session_state.messages.append({
                "role": "assistant",
                "content": f"â„¹ï¸ ëª¨ë¸ì´ '{selected_model}'(ìœ¼)ë¡œ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤."
            })
            st.rerun() # ëª¨ë¸ ì„ íƒ ì•Œë¦¼ í›„ ì¦‰ì‹œ ë°˜ì˜

    uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")

    # PDF ë·°ì–´ ì„¤ì •
    st.divider()
    resolution_boost = st.slider(label="Resolution boost", min_value=1, max_value=10, value=1)
    width = st.slider(label="PDF width", min_value=100, max_value=1000, value=1000)
    height = st.slider(label="PDF height", min_value=-1, max_value=10000, value=1000)

# ë ˆì´ì•„ì›ƒ ì„¤ì •
col_left, col_right = st.columns([1, 1])

# ì˜¤ë¥¸ìª½ ì»¬ëŸ¼: PDF ë¯¸ë¦¬ë³´ê¸°
with col_right:
    st.subheader("ğŸ“„ PDF Preview")
    # PDF ë¯¸ë¦¬ë³´ê¸°
    if uploaded_file and uploaded_file.name != st.session_state.get("last_uploaded_file_name"):
        if st.session_state.temp_pdf_path and os.path.exists(st.session_state.temp_pdf_path):
            try:
                os.remove(st.session_state.temp_pdf_path)
                logging.info("ì´ì „ ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ ì„±ê³µ")
            except Exception as e:
                logging.warning(f"ì´ì „ ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(uploaded_file.getvalue())
                st.session_state.temp_pdf_path = tmp.name
                logging.info(f"ì„ì‹œ PDF íŒŒì¼ ìƒì„± ì„±ê³µ: {st.session_state.temp_pdf_path}")
        except Exception as e:
            st.error(f"ì„ì‹œ PDF íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            st.session_state.temp_pdf_path = None

    # PDF ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ
    if st.session_state.temp_pdf_path and os.path.exists(st.session_state.temp_pdf_path):
        try:
            pdf_viewer(
                input=st.session_state.temp_pdf_path,
                width=width,
                height=height,
                key=f'pdf_viewer_{os.path.basename(st.session_state.temp_pdf_path)}',
                resolution_boost=resolution_boost
            )
        except Exception as e:
            st.error(f"PDF ë¯¸ë¦¬ë³´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    elif uploaded_file:
        st.warning("PDF ë¯¸ë¦¬ë³´ê¸°ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ì™¼ìª½ ì»¬ëŸ¼: ì±„íŒ… ë° ì„¤ì •
with col_left:
    st.subheader("ğŸ’¬ Chat")
    chat_container = st.container(height=500, border=True)
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.write(message["content"])

    new_file_uploaded = uploaded_file and uploaded_file.name != st.session_state.get("last_uploaded_file_name")
    if new_file_uploaded:
        if st.session_state.temp_pdf_path:
            reset_session_state(uploaded_file)
            st.session_state.messages.append({
                "role": "assistant",
                "content": f"ğŸ“‚ ìƒˆ PDF íŒŒì¼ '{uploaded_file.name}'ì´(ê°€) ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
                })
            st.rerun() # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ ë©”ì‹œì§€ í›„ ì¦‰ì‹œ ë°˜ì˜
        else:
            st.warning("PDF íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

    # PDF ì²˜ë¦¬ ìƒíƒœ í™•ì¸ ë° ì‹œì‘
    # ë‹¨ê³„ 1: ì²˜ë¦¬ ì¤‘ ë©”ì‹œì§€ í‘œì‹œ ë° í”Œë˜ê·¸ ì„¤ì •
    if uploaded_file and st.session_state.temp_pdf_path and \
       not st.session_state.get("pdf_processed") and \
       not st.session_state.get("pdf_processing_error") and \
       not st.session_state.get("pdf_is_processing"): # ì•„ì§ ì²˜ë¦¬ ì‹œì‘ ì•ˆ í•¨

        current_selected_model = st.session_state.get("last_selected_model")
        if not current_selected_model:
            # ëª¨ë¸ ë¯¸ì„ íƒ ì‹œ ê²½ê³  (ë§¤ë²ˆ í‘œì‹œë  ìˆ˜ ìˆìŒ, ì‚¬ì´ë“œë°” ì„ íƒ ìœ ë„)
            st.warning("ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            # ëª¨ë¸ì´ ì„ íƒë˜ì—ˆìœ¼ë¯€ë¡œ ì²˜ë¦¬ ì‹œì‘ ë©”ì‹œì§€ í‘œì‹œ
            st.session_state.messages.append({
                "role": "assistant",
                "content": f"â³ '{uploaded_file.name}' ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."
            })
            st.session_state.pdf_is_processing = True
            st.rerun()

    # ë‹¨ê³„ 2: ì‹¤ì œ PDF ì²˜ë¦¬ (pdf_is_processing í”Œë˜ê·¸ê°€ Trueì¼ ë•Œ)
    if st.session_state.get("pdf_is_processing") and \
       not st.session_state.get("pdf_processed") and \
       not st.session_state.get("pdf_processing_error"):

        current_selected_model = st.session_state.get("last_selected_model")

        if uploaded_file and st.session_state.temp_pdf_path and current_selected_model:
            # process_pdf í•¨ìˆ˜ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ì„±ê³µ/ì‹¤íŒ¨ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ê³ ,
            # pdf_processed, pdf_processing_error, pdf_is_processing í”Œë˜ê·¸ë¥¼ ì—…ë°ì´íŠ¸í•˜ë©°,
            # st.rerun()ì„ í˜¸ì¶œí•¨.
            process_pdf(
                uploaded_file,
                current_selected_model,
                st.session_state.temp_pdf_path
            )
            # process_pdfê°€ rerunì„ í•˜ë¯€ë¡œ, ì´ ì•„ë˜ ì½”ë“œëŠ” í•´ë‹¹ ì‹¤í–‰ì—ì„œëŠ” ë„ë‹¬í•˜ì§€ ì•ŠìŒ.
            # pdf_is_processing í”Œë˜ê·¸ëŠ” process_pdf ë‚´ë¶€ì—ì„œ Falseë¡œ ì„¤ì •ë¨.
        else:
            logging.warning("PDF ì²˜ë¦¬ ì‹œì‘ ì¡°ê±´ ë¶ˆì¶©ì¡± (pdf_is_processing True ìƒíƒœ).")
            st.session_state.messages.append({
                "role": "assistant",
                "content": "âš ï¸ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ì—…ë¡œë“œ ìƒíƒœë‚˜ ëª¨ë¸ ì„ íƒì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
            })
            st.session_state.pdf_is_processing = False # í”Œë˜ê·¸ ë¦¬ì…‹
            st.session_state.pdf_processed = False
            st.session_state.pdf_processing_error = "ì²˜ë¦¬ ì‹œì‘ ì¡°ê±´ ë¶ˆì¶©ì¡±"
            st.rerun()

    # ì±„íŒ… ì…ë ¥ì°½
    # ì…ë ¥ ê°€ëŠ¥ ì¡°ê±´: PDF ì²˜ë¦¬ ì™„ë£Œ + ì˜¤ë¥˜ ì—†ìŒ + QA ì²´ì¸ ì¡´ì¬
    is_ready_for_input = st.session_state.get("pdf_processed") and not st.session_state.get("pdf_processing_error") and st.session_state.get("qa_chain") is not None
    user_input = st.chat_input(
        "PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.",
        key='user_input',
        disabled=not is_ready_for_input
    )

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_input:
        st.session_state.messages.append({
            "role": "user",
            "content": user_input,
            })
        with chat_container:
            with st.chat_message("user"):
                st.write(user_input)

        # ë‹µë³€ ìƒì„± ì „ QA ì²´ì¸ ìœ íš¨ì„± í™•ì¸ (ëª¨ë¸ ë³€ê²½ ì¤‘ Noneì¼ ìˆ˜ ìˆìŒ)
        qa_chain = st.session_state.get("qa_chain")
        if not qa_chain:
            error_message = "âŒ QA ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³€ê²½ì´ ì§„í–‰ ì¤‘ì´ê±°ë‚˜ PDF ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            st.session_state.messages.append({
                "role": "assistant",
                "content": error_message,
                })
            # ì±„íŒ…ì°½ì— ì˜¤ë¥˜ ë©”ì‹œì§€ ì¦‰ì‹œ í‘œì‹œ
            with chat_container:
                with st.chat_message("assistant"):
                    st.error(error_message) # ì˜¤ë¥˜ ê°•ì¡° í‘œì‹œ
        else:
            # QA ì²´ì¸ì´ ì¤€ë¹„ëœ ê²½ìš° ë‹µë³€ ìƒì„± ì§„í–‰
            with chat_container:
                with st.chat_message("assistant"):
                    # ìƒê° ê³¼ì • expanderë¥¼ ë¯¸ë¦¬ ìƒì„±í•˜ê³  ì ‘ì–´ë‘¡ë‹ˆë‹¤.
                    thought_expander = st.expander("ğŸ¤” ìƒê° ê³¼ì •", expanded=False)
                    message_placeholder = st.empty()
                    message_placeholder.write("â–Œ")

                    full_response = "" # ìµœì¢… ì‚¬ìš©ì ëŒ€ìƒ ë‹µë³€ìš©
                    thought_response = "" # ìƒê° ê³¼ì • ëˆ„ì ìš©
                    processing_thought = True # ìƒê° ê³¼ì • ì²˜ë¦¬ ì—¬ë¶€

                    try:
                        # ë‹µë³€ ìƒì„±
                        logging.info("ë‹µë³€ ìƒì„± ì‹œì‘...")
                        stream = qa_chain.stream({"input": user_input})
                        for chunk in stream:
                            answer_part = chunk.get("answer", "")
                            if not answer_part:
                                continue

                            if processing_thought:
                                if "</think>" in answer_part:
                                    # ì´ ì²­í¬ì—ì„œ ìƒê° ê³¼ì •ì˜ ëì„ ì°¾ìŒ
                                    parts = answer_part.split("</think>", 1)
                                    thought_part = parts[0]
                                    answer_part_after_think = parts[1]

                                    # ìƒê°ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ëˆ„ì 
                                    thought_response += thought_part

                                    # ìƒê°ì„ ìµìŠ¤íŒ¬ë”ì— í‘œì‹œ
                                    cleaned_thought = thought_response.replace("<think>", "").strip()
                                    if cleaned_thought:
                                        # ë¯¸ë¦¬ ìƒì„±ëœ expander ë‚´ë¶€ì— markdownìœ¼ë¡œ ë‚´ìš©ì„ ì±„ì›ë‹ˆë‹¤.
                                        thought_expander.markdown(cleaned_thought)

                                    processing_thought = False # ì‹¤ì œ ë‹µë³€ ì²˜ë¦¬ë¡œ ì „í™˜

                                    # ì²­í¬ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ë¶€í„° ì‹¤ì œ ë‹µë³€ ëˆ„ì  ì‹œì‘
                                    full_response += answer_part_after_think
                                    if full_response: # ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ í”Œë ˆì´ìŠ¤í™€ë” ì—…ë°ì´íŠ¸
                                        message_placeholder.write(full_response + "â–Œ")
                                else:
                                    # Still accumulating the thought part
                                    thought_response += answer_part
                            else:
                                # Accumulating the answer part after </think>
                                full_response += answer_part
                                message_placeholder.write(full_response + "â–Œ")

                        # ë£¨í”„ ì¢…ë£Œ í›„ ìµœì¢… ì—…ë°ì´íŠ¸
                        if processing_thought:
                            # </think>ë¥¼ ì°¾ì§€ ëª»í•¨, ëˆ„ì ëœ ì „ì²´ ìƒê°ì„ ë‹µë³€ìœ¼ë¡œ ì²˜ë¦¬
                            cleaned_thought = thought_response.replace("<think>", "").strip()
                            message_placeholder.write(cleaned_thought)
                            full_response = cleaned_thought # ìƒíƒœ ì €ì¥ì„ ìœ„í•´ í• ë‹¹
                        else:
                            # ìƒê°ê³¼ ë‹µë³€ ì²˜ë¦¬ í›„ ì •ìƒ ì¢…ë£Œ
                            message_placeholder.write(full_response) # ì»¤ì„œ ì—†ì´ ìµœì¢… ë‹µë³€ ì‘ì„±

                    except Exception as e:
                        logging.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                        error_message = f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                        message_placeholder.error(error_message)
                        full_response = error_message

            # ìµœì¢… *ë‹µë³€* ë¶€ë¶„ (ë˜ëŠ” ì˜¤ë¥˜)ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response,
                })