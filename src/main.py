import torch
# ì•„ë˜ ì½”ë“œëŠ” íŠ¹ì • PyTorch/Torchvision ë²„ì „ ê°„ í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¸í•´ torchvision.ops ë“±ì„ ì°¾ì§€ ëª»í•˜ëŠ” ì˜¤ë¥˜ë¥¼
# í•´ê²°í•˜ê¸° ìœ„í•œ ì„ì‹œ ì¡°ì¹˜ì…ë‹ˆë‹¤. (ì˜ˆ: torchvision ë¡œë”© ì‹œ `torch.classes.load_library` ê´€ë ¨ ì˜¤ë¥˜)
torch.classes.__path__ = []
import tempfile
import os
import streamlit as st
from streamlit_pdf_viewer import pdf_viewer
import logging
import time
from utils import (
    SessionManager,
    get_ollama_models,
    load_llm,
    process_pdf,
    update_qa_chain as util_update_qa_chain,
    RETRIEVER_CONFIG,
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

st.set_page_config(
    page_title="RAG Chatbot",
    layout="wide"
)
SessionManager.init_session()

def handle_model_change(selected_model: str):
    """ëª¨ë¸ ë³€ê²½ ì²˜ë¦¬"""
    if not selected_model or selected_model == st.session_state.get("last_selected_model"):
        return

    old_model = SessionManager.update_model(selected_model)
    logging.info(f"LLM ë³€ê²½ ê°ì§€: {old_model} -> {selected_model}")

    if not st.session_state.get("pdf_processed"):
        logging.info(f"ëª¨ë¸ ì„ íƒ ë³€ê²½ë¨ (PDF ë¯¸ì²˜ë¦¬ ìƒíƒœ): {selected_model}")
        return

    try:
        # 1. ìƒˆ LLM ë¡œë“œ
        with st.spinner(f"'{selected_model}' ëª¨ë¸ ë¡œë”© ì¤‘..."):
            st.session_state.llm = load_llm(selected_model)
        # 2. QA ì²´ì¸ ì—…ë°ì´íŠ¸
        if st.session_state.get("vector_store") and st.session_state.get("llm"):
            with st.spinner("QA ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì¤‘..."):
                st.session_state.qa_chain = util_update_qa_chain(
                    st.session_state.llm,
                    st.session_state.vector_store
                )
                logging.info(f"'{selected_model}' ëª¨ë¸ë¡œ QA ì²´ì¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
                success_message = f"âœ… '{selected_model}' ëª¨ë¸ë¡œ ë³€ê²½ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                SessionManager.add_message("assistant", success_message)
                st.session_state.last_model_change_message = success_message
        else:
            raise ValueError("ë²¡í„° ì €ì¥ì†Œ ë˜ëŠ” LLMì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDF ì¬ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        error_msg = f"ëª¨ë¸ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(f"{error_msg} ({selected_model})", exc_info=True)
        SessionManager.reset_session_state(["llm", "qa_chain"])
        SessionManager.add_message("assistant", f"âŒ {error_msg}")
        st.session_state.last_model_change_message = f"âŒ {error_msg}"

def handle_file_upload_and_process(uploaded_file):
    """PDF íŒŒì¼ ì—…ë¡œë“œì™€ ì²˜ë¦¬ë¥¼ í•œ ë²ˆì— ê´€ë¦¬"""
    if not uploaded_file:
        return

    # ê°™ì€ íŒŒì¼ì´ ë‹¤ì‹œ ì—…ë¡œë“œëœ ê²½ìš° ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
    if uploaded_file.name == st.session_state.get("last_uploaded_file_name"):
        return

    # 1. ì´ì „ ì„ì‹œ íŒŒì¼ ì •ë¦¬ (Best-effort)
    if st.session_state.get("temp_pdf_path") and os.path.exists(st.session_state.temp_pdf_path):
        try:
            os.remove(st.session_state.temp_pdf_path)
            logging.info("ì´ì „ ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ ì„±ê³µ")
        except Exception as e:
            logging.warning(f"ì´ì „ ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    # 2. ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹
    SessionManager.reset_for_new_file(uploaded_file)
    st.session_state.pdf_is_processing = True

    try:
        # 3. ìƒˆ PDF íŒŒì¼ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
        with st.spinner(f"'{uploaded_file.name}' íŒŒì¼ ì €ì¥ ì¤‘..."):
            temp_dir = tempfile.gettempdir()
            temp_pdf_path = os.path.join(temp_dir, f"rag_chatbot_{int(time.time())}_{uploaded_file.name}")
            with open(temp_pdf_path, 'wb') as f:
                f.write(uploaded_file.getvalue())
            
            st.session_state.temp_pdf_path = temp_pdf_path
            st.session_state.current_file_path = temp_pdf_path
            st.session_state["pdf_viewer_key"] = f"pdf_viewer_{uploaded_file.name}_{int(time.time())}"
            logging.info(f"ì„ì‹œ PDF íŒŒì¼ ìƒì„± ì„±ê³µ: {temp_pdf_path}")
        
        # 4. ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘ ë©”ì‹œì§€ í‘œì‹œ
        SessionManager.add_message(
            "assistant", 
            f"ğŸ“‚ '{uploaded_file.name}' íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ.\n\n"
            f"â³ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."
        )
        
        # 5. PDF ì²˜ë¦¬ ì‹¤í–‰ (rerun ì—†ì´ ë°”ë¡œ ì‹¤í–‰)
        current_selected_model = st.session_state.get("last_selected_model")
        if not current_selected_model:
            st.warning("ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            # ëª¨ë¸ ë¯¸ì„ íƒ ì‹œ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•˜ê³  ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
            SessionManager.set_error_state("ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•„ PDF ì²˜ë¦¬ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        process_pdf(uploaded_file, current_selected_model, st.session_state.temp_pdf_path)

    except Exception as e:
        error_msg = f"íŒŒì¼ ì—…ë¡œë“œ ë˜ëŠ” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg, exc_info=True)
        SessionManager.set_error_state(error_msg)
    finally:
        st.session_state.pdf_is_processing = False
        # ëª¨ë“  ê³¼ì •ì´ ëë‚œ í›„ UI ì „ì²´ë¥¼ ìµœì¢… ìƒíƒœë¡œ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ ë‹¨ í•œë²ˆ rerun
        st.rerun()

def process_chat_response(qa_chain, user_input, chat_container):
    """
    ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ LLM ì‘ë‹µì„ ì²˜ë¦¬í•˜ê³  ì±„íŒ… ì»¨í…Œì´ë„ˆì— í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
    """
    with chat_container:
        with st.chat_message("assistant"):
            # ìƒê° ê³¼ì • Expanderì™€ ê·¸ ì•ˆì˜ í”Œë ˆì´ìŠ¤í™€ë”
            thought_expander = st.expander("ğŸ¤” ìƒê° ê³¼ì •", expanded=False)
            thought_placeholder = thought_expander.empty()
            
            # 1. ë©”ì‹œì§€, ìƒê° ê³¼ì •, ì›ë¬¸ ë³´ê¸° ì˜ì—­ì— ëŒ€í•œ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ë¯¸ë¦¬ ìƒì„±
            message_placeholder = st.empty()
            
            # ì´ˆê¸° ë©”ì‹œì§€ ì„¤ì •
            message_placeholder.markdown("ë‹µë³€ ìƒì„± ì¤€ë¹„ ì¤‘...")
            thought_placeholder.markdown("ì•„ì§ ìƒê° ê³¼ì •ì´ ì—†ìŠµë‹ˆë‹¤.")

            try:
                start_time = time.time()
                thought_buffer = ""
                response_buffer = ""
                is_thinking = False
                update_counter = 0
                
                source_documents = []

                # 2. ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                for chunk in qa_chain.stream({"input": user_input}):
                    answer_chunk = chunk.get("answer", "")
                    if chunk.get("context"):
                        if not source_documents:
                            source_documents = chunk.get("context")
                    
                    if not answer_chunk:
                        continue
                        
                    if "<think>" in answer_chunk:
                        is_thinking = True
                        thought_buffer = answer_chunk.split("<think>")[1]
                        message_placeholder.markdown("ğŸ¤” ìƒê°ì„ ì •ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
                        continue
                        
                    if "</think>" in answer_chunk:
                        is_thinking = False
                        thought_end_idx = answer_chunk.find("</think>")
                        thought_buffer += answer_chunk[:thought_end_idx]
                        if thought_buffer.strip():
                            # ìƒê° ê³¼ì •ì´ ëë‚˜ë©´ ìµœì¢… ë‚´ìš©ì„ ì—…ë°ì´íŠ¸
                            thought_placeholder.markdown(thought_buffer)
                        response_buffer = answer_chunk[thought_end_idx + len("</think>"):]
                        message_placeholder.markdown("ë‹µë³€ì„ ì‘ì„±í•˜ëŠ” ì¤‘...")
                        continue
                    
                    if is_thinking:
                        thought_buffer += answer_chunk
                        if update_counter % 3 == 0:
                            thought_placeholder.markdown(thought_buffer + "â–Œ")
                    else:
                        response_buffer += answer_chunk
                        if update_counter % 3 == 0:
                            message_placeholder.markdown(response_buffer + "â–Œ")
                    
                    update_counter += 1
                    time.sleep(0.01)  
                
                # 3. ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„, ê° í”Œë ˆì´ìŠ¤í™€ë”ì— ìµœì¢… ë‚´ìš© ì±„ìš°ê¸°

                # ìµœì¢… ìƒê° ê³¼ì • ì—…ë°ì´íŠ¸
                if thought_buffer.strip():
                    thought_placeholder.markdown(thought_buffer)
                
                # ìµœì¢… ë‹µë³€ í‘œì‹œ
                final_answer = response_buffer.strip()
                if not final_answer:
                    final_answer = "ì£„ì†¡í•©ë‹ˆë‹¤, ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."
                message_placeholder.markdown(final_answer)
                SessionManager.add_message("assistant", final_answer)

                end_time = time.time()
                logging.info(f"LLM ë‹µë³€ ìƒì„± ì™„ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")

            except Exception as e:
                error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                logging.error(error_msg, exc_info=True)
                message_placeholder.error(error_msg)
                SessionManager.add_message("assistant", f"âŒ {error_msg}")

def main():
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ Settings")
        selected_model = None 
        try:
            models = get_ollama_models()
            last_model = st.session_state.get("last_selected_model")
            current_model_index = models.index(last_model) if last_model and last_model in models else 0
            if models:
                selected_model = st.selectbox(
                    "Select an Ollama model",
                    models,
                    index=current_model_index,
                    key="model_selector"
                )
            else:
                st.text("Failed to load Ollama models.")
        except Exception as e:
            st.error(f"Failed to load Ollama models: {e}")
            st.warning("Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€, Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

        if (
            selected_model
            and selected_model != st.session_state.get("last_selected_model")
        ):
            handle_model_change(selected_model)

        uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")
        
        # íŒŒì¼ ì—…ë¡œë“œê°€ ê°ì§€ë˜ë©´ ë°”ë¡œ ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
        handle_file_upload_and_process(uploaded_file)
        
        st.divider()
        resolution_boost = st.slider("Resolution boost", 1, 10, 1)
        width = st.slider("PDF width", 100, 1000, 1000)
        height = st.slider("PDF height", 100, 10000, 1000)

    col_left, col_right = st.columns([1, 1])

    # ë©”ì¸ ì»¨í…Œì´ë„ˆ ì„¤ì •
    with col_right:
        st.subheader("ğŸ“„ PDF Preview")
        
        # PDF ë·°ì–´ ë Œë”ë§
        if uploaded_file:
            if st.session_state.get("temp_pdf_path") and os.path.exists(st.session_state.temp_pdf_path):
                try:
                    viewer_key = st.session_state.get("pdf_viewer_key", "pdf_viewer_default")
                    pdf_viewer(
                        input=st.session_state.temp_pdf_path,
                        width=width,
                        height=height,
                        key=viewer_key,
                        resolution_boost=resolution_boost
                    )
                    logging.info(f"PDF ë·°ì–´ ë Œë”ë§ ì„±ê³µ - í‚¤: {viewer_key}")
                except Exception as e:
                    error_msg = f"PDF ë¯¸ë¦¬ë³´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                    logging.error(error_msg, exc_info=True)
                    st.error(error_msg)
                    
                    # PDF ë·°ì–´ ë³µêµ¬ ì‹œë„
                    try:
                        st.session_state["pdf_viewer_key"] = f"pdf_viewer_retry_{int(time.time())}"
                        st.rerun()
                    except Exception as retry_error:
                        logging.error(f"PDF ë·°ì–´ ë³µêµ¬ ì‹¤íŒ¨: {retry_error}")

    # ì±„íŒ… ì»¨í…Œì´ë„ˆ ì„¤ì •
    with col_left:
        st.subheader("ğŸ’¬ Chat")
        
        chat_container = st.container(height=650, border=True)
        
        # 1. ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë  ë•Œë§ˆë‹¤ ì„¸ì…˜ì— ì €ì¥ëœ ëª¨ë“  ë©”ì‹œì§€ë¥¼ í‘œì‹œ
        with chat_container:
            if "messages" in st.session_state:
                for message in st.session_state.messages:
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"], unsafe_allow_html=True)
            
        # 2. ì‚¬ìš©ì ì…ë ¥ì„ ë°›ìŒ
        if user_input := st.chat_input(
            "PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.",
            key='user_input',
            disabled=not SessionManager.is_ready_for_chat()
        ):
            # 3. ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€í•˜ê³  ì¦‰ì‹œ í™”ë©´ì— í‘œì‹œ
            SessionManager.add_message("user", user_input)
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(user_input)
            
            # 4. ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì²˜ë¦¬
            qa_chain = st.session_state.get("qa_chain")
            if not qa_chain:
                error_message = "âŒ QA ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³€ê²½ì´ ì§„í–‰ ì¤‘ì´ê±°ë‚˜ PDF ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                # ì—ëŸ¬ ë©”ì‹œì§€ë„ ì„¸ì…˜ì— ì¶”ê°€í•˜ê³  ì¦‰ì‹œ í‘œì‹œ
                SessionManager.add_message("assistant", error_message)
                with chat_container:
                    with st.chat_message("assistant"):
                        st.markdown(error_message)
            else:
                try:
                    # process_chat_responseê°€ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ chat_containerì— ì§ì ‘ í‘œì‹œ
                    process_chat_response(qa_chain, user_input, chat_container)
                except Exception as e:
                    error_message = f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    SessionManager.add_message("assistant", error_message)
                    with chat_container:
                        with st.chat_message("assistant"):
                            st.markdown(error_message)
                    logging.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)

if __name__ == "__main__":
    main()