import torch
torch.classes.__path__ = []
import tempfile
import streamlit as st
from streamlit_pdf_viewer import pdf_viewer
from langchain.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
import logging
from utils import (
    get_ollama_models,
    load_pdf_docs,
    get_embedder,
    split_documents,
    create_vector_store,
    init_llm,
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="RAG Chatbot", layout="wide")
st.title("ğŸ“„ RAG Chatbot with Ollama LLM")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("Settings")
    try:
        models = get_ollama_models()
        selected_model = st.selectbox("Select an Ollama model", models) if models else st.text("Failed to load Ollama models.")
    except Exception as e:
        st.error(f"Failed to load Ollama models: {e}")
        st.warning("Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€, Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        selected_model = None
    uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")
    resolution_boost = st.slider(label="Resolution boost", min_value=1, max_value=10, value=1)
    width = st.slider(label="PDF width", min_value=100, max_value=1000, value=1000)
    height = st.slider(label="PDF height", min_value=-1, max_value=10000, value=1000)

# ë©”ì‹œì§€ ì…ë ¥
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

# 2ì—´ ë ˆì´ì•„ì›ƒ: ì™¼ìª½ì—ëŠ” ì„¤ì • ë° ëŒ€í™”, ì˜¤ë¥¸ìª½ì—ëŠ” PDF ë¯¸ë¦¬ë³´ê¸°
col_left, col_right = st.columns([1, 1])

with col_right:
    st.header("ğŸ“„ PDF Preview")
    if uploaded_file:
        try:
            # ì„ì‹œ íŒŒì¼ì— ì—…ë¡œë“œëœ PDF íŒŒì¼ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name
            # ì„ì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ pdf_viewer í˜¸ì¶œ
            pdf_viewer(input=tmp_path, width=width, height=height, key='pdf_viewer', resolution_boost=resolution_boost)
        except Exception as e:
            st.error(f"PDF ë¯¸ë¦¬ë³´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

with col_left:
    st.header("ğŸ’¬ Chat")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "last_selected_model" not in st.session_state:
        st.session_state.last_selected_model = None

    # ìƒë‹¨ì— ì €ì¥ëœ ëŒ€í™” ë©”ì‹œì§€ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶œë ¥
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])
    
    # ëª¨ë¸ ë³€ê²½ ì‹œ ì—…ë°ì´íŠ¸
    if selected_model and selected_model != st.session_state.get("last_selected_model"):
        st.session_state.last_selected_model = selected_model
        st.session_state.messages.append({
            "role": "assistant",
            "content": f"ğŸ› ï¸ ëª¨ë¸ {selected_model}ì´(ê°€) ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤."
        })
        with st.chat_message("assistant"):
            st.write(f"ğŸ› ï¸ ëª¨ë¸ {selected_model}ì´(ê°€) ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # PDF íŒŒì¼ ì—…ë¡œë“œ ì‹œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if uploaded_file:
        file_bytes = uploaded_file.getvalue()
        
        # íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if st.session_state.get("last_uploaded_file") != file_bytes:
            st.session_state.last_uploaded_file = file_bytes
            
            # ê´€ë ¨ ìºì‹œ ë° ìƒíƒœ ì´ˆê¸°í™”
            load_pdf_docs.clear()
            get_embedder.clear()
            split_documents.clear()
            create_vector_store.clear()
            
            # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            for key in ["pdf_processed", "pdf_completed", "qa_chain", "vector_store", "llm", "pdf_processing", "pdf_message_logged"]:
                st.session_state.pop(key, None)
    
    # PDF ë¬¸ì„œ ì²˜ë¦¬
    if uploaded_file and not st.session_state.get("pdf_completed", False):
        # PDF íŒŒì¼ ì—…ë¡œë“œ í›„ ë©”ì‹œì§€ ì¶œë ¥
        if not st.session_state.get("pdf_message_logged", False):
            st.session_state.messages.append({
                "role": "assistant",
                "content": f"ğŸ“‚ PDF íŒŒì¼ {uploaded_file.name}ì´(ê°€) ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
            })
            st.session_state.pdf_message_logged = True
            with st.chat_message("assistant"):
                st.write(f"ğŸ“‚ PDF íŒŒì¼ {uploaded_file.name}ì´(ê°€) ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

        with st.spinner("ğŸ“„ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”."):
            try:
                docs = load_pdf_docs(uploaded_file.getvalue())        
                embedder = get_embedder(model_name="BAAI/bge-m3",
                                        model_kwargs={'device': 'cuda'},
                                        encode_kwargs={'device': 'cuda'})
                documents = split_documents(docs, embedder)
                vector_store = create_vector_store(documents, embedder)
                
                if isinstance(selected_model, str):
                    llm = init_llm(selected_model)
                else:
                    raise ValueError("âŒ LLM ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            except ValueError as e:
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": str(e)
                })
            
            # ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ í›„ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            st.session_state.vector_store = vector_store
            st.session_state.llm = llm  

            # QA ì²´ì¸ ìƒì„±
            QA_PROMPT = PromptTemplate.from_template(
            """
            ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ë° ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ì•„ë˜ ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ë‚´ì˜ ì •ë³´ë§Œ í™œìš©í•˜ì—¬, ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
            ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ í™•ì¸ë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì‹œê³ , í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.

            [ì»¨í…ìŠ¤íŠ¸]
            {context}
            
            [ì§ˆë¬¸]
            {input}

            [ë‹µë³€]
            """
            )
            
            # ë¬¸ì„œ ì²´ì¸ ìƒì„±
            combine_chain = create_stuff_documents_chain(llm, QA_PROMPT)
            qa_chain = create_retrieval_chain(vector_store.as_retriever(search_type="similarity",
                                                                        search_kwargs={"k": 20}), combine_chain)

            st.session_state.qa_chain = qa_chain
            st.session_state.pdf_completed = True  

            st.session_state.messages.append({
                "role": "assistant", 
                "content": f"âœ… PDF íŒŒì¼ {uploaded_file.name}ì˜ ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            })
            with st.chat_message("assistant"):
                st.write(f"âœ… PDF íŒŒì¼ {uploaded_file.name}ì˜ ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_input:
        with st.chat_message("user"):
            st.write(user_input)
        st.session_state.messages.append({
            "role": "user", 
            "content": user_input
        })

        # QA ì²´ì¸ ì´ˆê¸°í™” í™•ì¸
        qa_chain = st.session_state.qa_chain
        if not qa_chain:
            with st.chat_message("assistant"):
                st.write("âŒ ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")
            st.session_state.messages.append({
                "role": "assistant", 
                "content": "âŒ ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”."
            })
            st.stop()

        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘..."):
                try:
                    response = st.session_state.qa_chain.invoke({"input": user_input})
                    answer = response["answer"]
                except Exception as e:
                    answer = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            st.write(answer)

        st.session_state.messages.append({
            "role": "assistant", 
            "content": answer
        })