import torch
# ì•„ë˜ ì½”ë“œëŠ” íŠ¹ì • PyTorch/Torchvision ë²„ì „ ê°„ í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¸í•´ torchvision.ops ë“±ì„ ì°¾ì§€ ëª»í•˜ëŠ” ì˜¤ë¥˜ë¥¼
# í•´ê²°í•˜ê¸° ìœ„í•œ ì„ì‹œ ì¡°ì¹˜ì…ë‹ˆë‹¤. (ì˜ˆ: torchvision ë¡œë”© ì‹œ `torch.classes.load_library` ê´€ë ¨ ì˜¤ë¥˜)
torch.classes.__path__ = []
import tempfile
import os
import streamlit as st
from streamlit_pdf_viewer import pdf_viewer
import logging
import time
from utils import (
    SessionManager,
    get_ollama_models,
    load_llm,
    process_pdf,
    update_qa_chain as util_update_qa_chain,
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

st.set_page_config(
    page_title="RAG Chatbot",
    layout="wide"
)
SessionManager.init_session()

def handle_model_change(selected_model: str):
    """ëª¨ë¸ ë³€ê²½ ì²˜ë¦¬"""
    if not selected_model or selected_model == st.session_state.get("last_selected_model"):
        return

    old_model = SessionManager.update_model(selected_model)
    logging.info(f"LLM ë³€ê²½ ê°ì§€: {old_model} -> {selected_model}")

    if not st.session_state.get("pdf_processed"):
        logging.info(f"ëª¨ë¸ ì„ íƒ ë³€ê²½ë¨ (PDF ë¯¸ì²˜ë¦¬ ìƒíƒœ): {selected_model}")
        return

    try:
        # 1. ìƒˆ LLM ë¡œë“œ
        with st.spinner(f"'{selected_model}' ëª¨ë¸ ë¡œë”© ì¤‘..."):
            st.session_state.llm = load_llm(selected_model)
        # 2. QA ì²´ì¸ ì—…ë°ì´íŠ¸
        if st.session_state.get("vector_store") and st.session_state.get("llm"):
            with st.spinner("QA ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì¤‘..."):
                st.session_state.qa_chain = util_update_qa_chain(
                    st.session_state.llm,
                    st.session_state.vector_store
                )
                logging.info(f"'{selected_model}' ëª¨ë¸ë¡œ QA ì²´ì¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
                success_message = f"âœ… '{selected_model}' ëª¨ë¸ë¡œ ë³€ê²½ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                SessionManager.add_message("assistant", success_message)
                st.session_state.last_model_change_message = success_message
        else:
            raise ValueError("ë²¡í„° ì €ì¥ì†Œ ë˜ëŠ” LLMì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDF ì¬ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        error_msg = f"ëª¨ë¸ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(f"{error_msg} ({selected_model})", exc_info=True)
        SessionManager.reset_session_state(["llm", "qa_chain"])
        SessionManager.add_message("assistant", f"âŒ {error_msg}")
        st.session_state.last_model_change_message = f"âŒ {error_msg}"

def save_uploaded_file(uploaded_file) -> str | None:
    """UploadFile ê°ì²´ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        with st.spinner(f"'{uploaded_file.name}' íŒŒì¼ ì €ì¥ ì¤‘..."):
            temp_dir = tempfile.gettempdir()
            # íŒŒì¼ ì´ë¦„ì— íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì¶”ê°€í•˜ì—¬ ê³ ìœ ì„± ë³´ì¥
            timestamp = int(time.time())
            safe_filename = f"rag_chatbot_{timestamp}_{uploaded_file.name}"
            temp_pdf_path = os.path.join(temp_dir, safe_filename)
            
            with open(temp_pdf_path, 'wb') as f:
                f.write(uploaded_file.getvalue())
            
            logging.info(f"ì„ì‹œ PDF íŒŒì¼ ìƒì„± ì„±ê³µ: {temp_pdf_path}")
            return temp_pdf_path
    except Exception as e:
        logging.error(f"ì„ì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)
        st.error(f"ì„ì‹œ íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return None

def handle_file_upload_and_process(uploaded_file):
    """PDF íŒŒì¼ ì—…ë¡œë“œì™€ ì²˜ë¦¬ë¥¼ í•œ ë²ˆì— ê´€ë¦¬"""
    if not uploaded_file or uploaded_file.name == st.session_state.get("last_uploaded_file_name"):
        return

    # 1. ì´ì „ ì„ì‹œ íŒŒì¼ ì •ë¦¬ (Best-effort)
    if st.session_state.get("temp_pdf_path") and os.path.exists(st.session_state.temp_pdf_path):
        try:
            os.remove(st.session_state.temp_pdf_path)
            logging.info("ì´ì „ ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ ì„±ê³µ")
        except Exception as e:
            logging.warning(f"ì´ì „ ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    # 2. ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹ ë° ì²˜ë¦¬ ì‹œì‘ í”Œë˜ê·¸ ì„¤ì •
    SessionManager.reset_for_new_file(uploaded_file)
    st.session_state.pdf_is_processing = True

    try:
        # 3. ìƒˆ PDF íŒŒì¼ ì €ì¥
        temp_pdf_path = save_uploaded_file(uploaded_file)
        if not temp_pdf_path:
            # íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ëŠ” save_uploaded_fileì—ì„œ ì´ë¯¸ í‘œì‹œë¨
            SessionManager.set_error_state("ì„ì‹œ íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í•˜ì—¬ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return

        st.session_state.temp_pdf_path = temp_pdf_path
        st.session_state.current_file_path = temp_pdf_path
        st.session_state["pdf_viewer_key"] = f"pdf_viewer_{uploaded_file.name}_{int(time.time())}"
        
        # 4. ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘ ë©”ì‹œì§€ í‘œì‹œ
        SessionManager.add_message(
            "assistant", 
            f"ğŸ“‚ '{uploaded_file.name}' íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ.\n\n"
            f"â³ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."
        )
        
        # 5. PDF ì²˜ë¦¬ ì‹¤í–‰
        current_selected_model = st.session_state.get("last_selected_model")
        if not current_selected_model:
            st.warning("ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            SessionManager.set_error_state("ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•„ PDF ì²˜ë¦¬ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        process_pdf(uploaded_file, current_selected_model, st.session_state.temp_pdf_path)

    except Exception as e:
        error_msg = f"íŒŒì¼ ì—…ë¡œë“œ ë˜ëŠ” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg, exc_info=True)
        SessionManager.set_error_state(error_msg)
    finally:
        st.session_state.pdf_is_processing = False
        # ëª¨ë“  ê³¼ì •ì´ ëë‚œ í›„ UI ì „ì²´ë¥¼ ìµœì¢… ìƒíƒœë¡œ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ ë‹¨ í•œë²ˆ rerun
        st.rerun()

# --- Constants ---
THINK_START_TAG = "<think>"
THINK_END_TAG = "</think>"
MSG_PREPARING_ANSWER = "ë‹µë³€ ìƒì„± ì¤€ë¹„ ì¤‘..."
MSG_THINKING = "ğŸ¤” ìƒê°ì„ ì •ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."
MSG_WRITING_ANSWER = "ë‹µë³€ì„ ì‘ì„±í•˜ëŠ” ì¤‘..."
MSG_NO_THOUGHT_PROCESS = "ì•„ì§ ìƒê° ê³¼ì •ì´ ì—†ìŠµë‹ˆë‹¤."
MSG_NO_RELATED_INFO = "ì£„ì†¡í•©ë‹ˆë‹¤, ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."


def process_chat_response(qa_chain, user_input, chat_container):
    """
    ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ LLM ì‘ë‹µì„ ì²˜ë¦¬í•˜ê³  ì±„íŒ… ì»¨í…Œì´ë„ˆì— í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
    """
    with chat_container, st.chat_message("assistant"):
        thought_expander = st.expander("ğŸ¤” ìƒê° ê³¼ì •", expanded=False)
        thought_placeholder = thought_expander.empty()
        message_placeholder = st.empty()
        
        message_placeholder.markdown(MSG_PREPARING_ANSWER)
        thought_placeholder.markdown(MSG_NO_THOUGHT_PROCESS)

        try:
            start_time = time.time()
            thought_buffer = ""
            response_buffer = ""
            is_thinking = False
            
            for chunk in qa_chain.stream({"input": user_input}):
                answer_chunk = chunk.get("answer", "")
                if not answer_chunk:
                    continue
                
                # <think> íƒœê·¸ ì²˜ë¦¬
                if THINK_START_TAG in answer_chunk:
                    is_thinking = True
                    # <think> íƒœê·¸ ì´í›„ì˜ ë‚´ìš©ì„ ìƒê° ë²„í¼ì— ì¶”ê°€
                    # ì´ì „ ì²­í¬ì˜ </think> ì™€ ê°™ì€ ì²­í¬ì— <think> ê°€ ì˜¤ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ split ì‚¬ìš©
                    parts = answer_chunk.split(THINK_START_TAG, 1)
                    response_buffer += parts[0] # <think> ì´ì „ ë‚´ìš©ì€ ë‹µë³€ìœ¼ë¡œ ê°„ì£¼
                    thought_buffer = parts[1]
                    message_placeholder.markdown(MSG_THINKING)
                    continue
                
                # </think> íƒœê·¸ ì²˜ë¦¬
                if THINK_END_TAG in answer_chunk:
                    is_thinking = False
                    parts = answer_chunk.split(THINK_END_TAG, 1)
                    thought_buffer += parts[0] # </think> ì´ì „ ë‚´ìš©ì€ ìƒê°ìœ¼ë¡œ ê°„ì£¼
                    
                    if thought_buffer.strip():
                        thought_placeholder.markdown(thought_buffer)
                    
                    response_buffer += parts[1] # </think> ì´í›„ ë‚´ìš©ì€ ë‹µë³€ìœ¼ë¡œ ê°„ì£¼
                    message_placeholder.markdown(MSG_WRITING_ANSWER)
                    continue
                
                # ìŠ¤íŠ¸ë¦¬ë° ë‚´ìš© í‘œì‹œ
                if is_thinking:
                    thought_buffer += answer_chunk
                    thought_placeholder.markdown(thought_buffer + "â–Œ")
                else:
                    response_buffer += answer_chunk
                    message_placeholder.markdown(response_buffer + "â–Œ")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ìµœì¢… ë‚´ìš© í‘œì‹œ
            if thought_buffer.strip():
                thought_placeholder.markdown(thought_buffer)
            
            final_answer = response_buffer.strip()
            if not final_answer:
                final_answer = MSG_NO_RELATED_INFO
            
            message_placeholder.markdown(final_answer)
            SessionManager.add_message("assistant", final_answer)

            end_time = time.time()
            logging.info(f"LLM ë‹µë³€ ìƒì„± ì™„ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")

        except Exception as e:
            error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logging.error(error_msg, exc_info=True)
            message_placeholder.error(error_msg)
            SessionManager.add_message("assistant", f"âŒ {error_msg}")

def render_sidebar():
    """ì‚¬ì´ë“œë°” UIë¥¼ ë Œë”ë§í•˜ê³  ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    with st.sidebar:
        st.header("âš™ï¸ Settings")
        
        # ëª¨ë¸ ì„ íƒ
        selected_model = None
        try:
            models = get_ollama_models()
            last_model = st.session_state.get("last_selected_model")
            current_model_index = models.index(last_model) if last_model and last_model in models else 0
            if models:
                selected_model = st.selectbox(
                    "Select an Ollama model",
                    models,
                    index=current_model_index,
                    key="model_selector"
                )
            else:
                st.text("Failed to load Ollama models.")
        except Exception as e:
            st.error(f"Failed to load Ollama models: {e}")
            st.warning("Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€, Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

        if selected_model and selected_model != st.session_state.get("last_selected_model"):
            handle_model_change(selected_model)

        # íŒŒì¼ ì—…ë¡œë”
        uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")
        handle_file_upload_and_process(uploaded_file)
        
        st.divider()
        
        # PDF ë·°ì–´ ì„¤ì •
        st.session_state.resolution_boost = st.slider("Resolution boost", 1, 10, 1)
        st.session_state.pdf_width = st.slider("PDF width", 100, 1000, 1000)
        st.session_state.pdf_height = st.slider("PDF height", 100, 10000, 1000)

def render_pdf_viewer_column():
    """PDF ë·°ì–´ ì»¬ëŸ¼ì„ ë Œë”ë§í•©ë‹ˆë‹¤."""
    st.subheader("ğŸ“„ PDF Preview")
    
    if st.session_state.get("temp_pdf_path") and os.path.exists(st.session_state.temp_pdf_path):
        try:
            viewer_key = st.session_state.get("pdf_viewer_key", "pdf_viewer_default")
            pdf_viewer(
                input=st.session_state.temp_pdf_path,
                width=st.session_state.pdf_width,
                height=st.session_state.pdf_height,
                key=viewer_key,
                resolution_boost=st.session_state.resolution_boost
            )
        except Exception as e:
            error_msg = f"PDF ë¯¸ë¦¬ë³´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logging.error(error_msg, exc_info=True)
            st.error(error_msg)
            # PDF ë·°ì–´ ë³µêµ¬ ì‹œë„
            if st.button("PDF ë·°ì–´ ì¬ì‹œë„"):
                st.session_state["pdf_viewer_key"] = f"pdf_viewer_retry_{int(time.time())}"
                st.rerun()

def render_chat_column():
    """ì±„íŒ… ì»¬ëŸ¼ì„ ë Œë”ë§í•˜ê³  ì±„íŒ… ë¡œì§ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    st.subheader("ğŸ’¬ Chat")
    
    chat_container = st.container(height=650, border=True)
    
    with chat_container:
        # ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
        if "messages" in st.session_state:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"], unsafe_allow_html=True)
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_input := st.chat_input(
        "PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.",
        key='user_input',
        disabled=not SessionManager.is_ready_for_chat()
    ):
        SessionManager.add_message("user", user_input)
        with chat_container:
            with st.chat_message("user"):
                st.markdown(user_input)
        
        # QA ì²´ì¸ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        qa_chain = st.session_state.get("qa_chain")
        if not qa_chain:
            error_message = "âŒ QA ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³€ê²½ì´ ì§„í–‰ ì¤‘ì´ê±°ë‚˜ PDF ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            SessionManager.add_message("assistant", error_message)
            with chat_container:
                with st.chat_message("assistant"):
                    st.markdown(error_message)
        else:
            try:
                process_chat_response(qa_chain, user_input, chat_container)
            except Exception as e:
                error_message = f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                SessionManager.add_message("assistant", error_message)
                with chat_container:
                    with st.chat_message("assistant"):
                        st.markdown(error_message)
                logging.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ í•¨ìˆ˜"""
    render_sidebar()

    col_left, col_right = st.columns([1, 1])

    with col_left:
        render_chat_column()

    with col_right:
        render_pdf_viewer_column()

if __name__ == "__main__":
    main()