"""
LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•˜ê³  ì‹¤í–‰í•˜ëŠ” ë¡œì§ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import logging
from typing import Any, AsyncGenerator, Dict

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, END

from config import QA_SYSTEM_PROMPT
# from session import SessionManager
from utils import async_log_operation
from schemas import GraphState


logger = logging.getLogger(__name__)


# ğŸ’¡ build_graphê°€ ë” ì´ìƒ llmì„ ì¸ìë¡œ ë°›ì§€ ì•Šë„ë¡ ìˆ˜ì •
def build_graph(retriever: Any):
    """
    ë‹¨ìˆœí•œ RAG ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤.
    """
    def retrieve_documents(state: GraphState) -> Dict[str, Any]:
        """
        ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜.
        """
        logger.info("Node execution: 'retrieve_documents'")
        if not retriever:
            raise ValueError("A valid retriever was not provided to the graph.")
        documents = retriever.invoke(state["input"])
        return {"documents": documents}

    def format_context(state: GraphState) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í¬ë§·íŒ…í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜.
        """
        logger.info("Node execution: 'format_context'")
        documents = state["documents"]
        logger.info(f"Retrieved {len(documents)} documents.")
        
        if not documents:
            logger.warning("No documents were retrieved, context will be empty.")
        formatted_docs = []
        
        for i, doc in enumerate(documents):
            doc.metadata["doc_number"] = i + 1
            page_number = doc.metadata.get("page", "N/A")
            if page_number != "N/A":
                doc.metadata["page"] = str(int(page_number) + 1)
            formatted_docs.append(
                f"[{doc.metadata['doc_number']}] {doc.page_content} (p.{doc.metadata.get('page', 'N/A')})"
            )
        context = "\n\n".join(formatted_docs)
        logger.info(f"Formatted context length: {len(context)} characters")
        return {"context": context}

    @async_log_operation("Generate response")
    async def generate_response(
        state: GraphState, 
        config: RunnableConfig
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›).
        LLM ê°ì²´ëŠ” config['configurable']['llm']ì„ í†µí•´ ì£¼ì…ë°›ìŠµë‹ˆë‹¤.
        """
        # 1. configì—ì„œ LLM ê°ì²´ êº¼ë‚´ê¸°
        # ì‹¤í–‰ ì‹œì ì— ë„˜ê²¨ì¤€ config ë”•ì…”ë„ˆë¦¬ì˜ 'configurable' í‚¤ ë‚´ë¶€ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        llm = config.get("configurable", {}).get("llm")
        
        if not llm:
            raise ValueError(
                "LLM object not found in config. "
                "Make sure to pass 'config={'configurable': {'llm': llm_instance}}' when invoking the graph."
            )

        # 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ìƒëµë˜ì—ˆë˜ ë¶€ë¶„ ìƒì„¸ ì‘ì„±)
        # config.pyì— ì •ì˜ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ìì˜ ì§ˆë¬¸/ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•©ë‹ˆë‹¤.
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", QA_SYSTEM_PROMPT),
                # LangGraphì˜ stateì—ì„œ 'context'ì™€ 'input'ì´ ì±„ì›Œì§‘ë‹ˆë‹¤.
                ("human", "[ì°¸ê³  ë¬¸ì„œ]:\n{context}\n\n[ì§ˆë¬¸]: {input}"),
            ]
        )

        # 3. ì²´ì¸ êµ¬ì„± (Prompt -> LLM -> String Parser)
        chain = prompt | llm | StrOutputParser()

        # 4. ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        # ì—¬ê¸°ì„œë„ configë¥¼ ì²´ì¸ì— ì „ë‹¬í•˜ë©´, ì²´ì¸ ë‚´ë¶€ì˜ ì½œë°± ì„¤ì • ë“±ì´ ìœ ì§€ë©ë‹ˆë‹¤.
        async for chunk in chain.astream(
            {"input": state["input"], "context": state["context"]},
            config=config 
        ):
            yield {"response": chunk}
    # async def generate_response(
    #     state: GraphState,
    # ) -> AsyncGenerator[Dict[str, Any], None]:
    #     """
    #     LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›).
    #     """
    #     llm = SessionManager.get("llm")
    #     if not llm:
    #         raise ValueError("LLM not found in session.")

    #     prompt = ChatPromptTemplate.from_messages(
    #         [
    #             ("system", QA_SYSTEM_PROMPT),
    #             ("human", "[ì°¸ê³  ë¬¸ì„œ]:\n{context}\n\n[ì§ˆë¬¸]: {input}"),
    #         ]
    #     )
    #     chain = prompt | llm | StrOutputParser()
    #     async for chunk in chain.astream(
    #         {"input": state["input"], "context": state["context"]}
    #     ):
    #         yield {"response": chunk}

    # ì›Œí¬í”Œë¡œìš° êµ¬ì„±
    workflow = StateGraph(GraphState)

    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("format_context", format_context)
    workflow.add_node("generate_response", generate_response)

    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "format_context")
    workflow.add_edge("format_context", "generate_response")
    workflow.add_edge("generate_response", END)

    app = workflow.compile()
    logger.info("Simple RAG LangGraph workflow compiled successfully.")

    return app
