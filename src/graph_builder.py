"""
LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•˜ê³  ì‹¤í–‰í•˜ëŠ” ë¡œì§ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""
import logging
from typing import Any, AsyncGenerator, Dict

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END

from config import QA_SYSTEM_PROMPT
from session import SessionManager
from utils import async_log_operation
from schemas import GraphState


# ğŸ’¡ build_graphê°€ ë” ì´ìƒ llmì„ ì¸ìë¡œ ë°›ì§€ ì•Šë„ë¡ ìˆ˜ì •
def build_graph(retriever: Any):
    """
    ë‹¨ìˆœí•œ RAG ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤.
    """

    # ë¼ìš°í„° ë° ë¶„ê¸° ë…¸ë“œ(conversational, general) í•¨ìˆ˜ë“¤ ëª¨ë‘ ì‚­ì œ

    def retrieve_documents(state: GraphState) -> Dict[str, Any]:
        logging.info("ë…¸ë“œ ì‹¤í–‰: retrieve_documents")
        if not retriever:
            raise ValueError("ê·¸ë˜í”„ì— ìœ íš¨í•œ ë¦¬íŠ¸ë¦¬ë²„ê°€ ì£¼ì…ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        documents = retriever.invoke(state["input"])
        return {"documents": documents}

    def format_context(state: GraphState) -> Dict[str, Any]:
        logging.info("ë…¸ë“œ ì‹¤í–‰: format_context")
        documents = state["documents"]
        logging.info(f"ë¦¬íŠ¸ë¦¬ë²„ê°€ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.")
        if not documents:
            logging.warning("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì–´ ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        formatted_docs = []
        for i, doc in enumerate(documents):
            doc.metadata["doc_number"] = i + 1
            page_number = doc.metadata.get("page", "N/A")
            if page_number != "N/A":
                doc.metadata["page"] = str(int(page_number) + 1)
            formatted_docs.append(
                f"[{doc.metadata['doc_number']}] {doc.page_content} (p.{doc.metadata.get('page', 'N/A')})"
            )
        context = "\n\n".join(formatted_docs)
        logging.info(f"ìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ê¸¸ì´: {len(context)}ì")
        return {"context": context}

    @async_log_operation("í†µí•© ì‘ë‹µ ìƒì„±")
    async def generate_response(state: GraphState) -> AsyncGenerator[Dict[str, Any], None]:
        # ğŸ’¡ llmì„ SessionManagerì—ì„œ ë‹¤ì‹œ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •
        llm = SessionManager.get("llm")
        if not llm:
            raise ValueError("ì„¸ì…˜ì—ì„œ LLMì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", QA_SYSTEM_PROMPT),
                ("human", "[Context]:\n{context}\n\n[Question]: {input}"),
            ]
        )
        chain = prompt | llm | StrOutputParser()
        async for chunk in chain.astream({"input": state["input"], "context": state["context"]}):
            yield {"response": chunk}

    # --- ğŸ’¡ ê·¸ë˜í”„ ì—°ê²° ë¡œì§ì„ ë§¤ìš° ë‹¨ìˆœí•˜ê²Œ ë³€ê²½ ğŸ’¡ ---
    workflow = StateGraph(GraphState)

    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("format_context", format_context)
    workflow.add_node("generate_response", generate_response)

    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "format_context")
    workflow.add_edge("format_context", "generate_response")
    workflow.add_edge("generate_response", END)

    app = workflow.compile()
    logging.info("ë‹¨ìˆœ RAG LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì„±ê³µì ìœ¼ë¡œ ì»´íŒŒì¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return app