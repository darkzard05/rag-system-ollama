"""
LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•˜ê³  ì‹¤í–‰í•˜ëŠ” ë¡œì§ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import logging
from typing import Any, AsyncGenerator, Dict

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END

from config import QA_SYSTEM_PROMPT
from session import SessionManager
from utils import async_log_operation
from schemas import GraphState


logger = logging.getLogger(__name__)


# ğŸ’¡ build_graphê°€ ë” ì´ìƒ llmì„ ì¸ìë¡œ ë°›ì§€ ì•Šë„ë¡ ìˆ˜ì •
def build_graph(retriever: Any):
    """
    ë‹¨ìˆœí•œ RAG ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤.
    """

    # ë¼ìš°í„° ë° ë¶„ê¸° ë…¸ë“œ(conversational, general) í•¨ìˆ˜ë“¤ ëª¨ë‘ ì‚­ì œ

    def retrieve_documents(state: GraphState) -> Dict[str, Any]:
        logger.info("Node execution: 'retrieve_documents'")
        if not retriever:
            raise ValueError("A valid retriever was not provided to the graph.")
        documents = retriever.invoke(state["input"])
        return {"documents": documents}

    def format_context(state: GraphState) -> Dict[str, Any]:
        logger.info("Node execution: 'format_context'")
        documents = state["documents"]
        logger.info(f"Retrieved {len(documents)} documents.")
        if not documents:
            logger.warning("No documents were retrieved, context will be empty.")
        formatted_docs = []
        for i, doc in enumerate(documents):
            doc.metadata["doc_number"] = i + 1
            page_number = doc.metadata.get("page", "N/A")
            if page_number != "N/A":
                doc.metadata["page"] = str(int(page_number) + 1)
            formatted_docs.append(
                f"[{doc.metadata['doc_number']}] {doc.page_content} (p.{doc.metadata.get('page', 'N/A')})"
            )
        context = "\n\n".join(formatted_docs)
        logger.info(f"Formatted context length: {len(context)} characters")
        return {"context": context}

    @async_log_operation("Generate response")
    async def generate_response(
        state: GraphState,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        # ğŸ’¡ llmì„ SessionManagerì—ì„œ ë‹¤ì‹œ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •
        llm = SessionManager.get("llm")
        if not llm:
            raise ValueError("LLM not found in session.")

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", QA_SYSTEM_PROMPT),
                ("human", "[ì°¸ê³  ë¬¸ì„œ]:\n{context}\n\n[ì§ˆë¬¸]: {input}"),
            ]
        )
        chain = prompt | llm | StrOutputParser()
        async for chunk in chain.astream(
            {"input": state["input"], "context": state["context"]}
        ):
            yield {"response": chunk}

    # ì›Œí¬í”Œë¡œìš° êµ¬ì„±
    workflow = StateGraph(GraphState)

    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("format_context", format_context)
    workflow.add_node("generate_response", generate_response)

    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "format_context")
    workflow.add_edge("format_context", "generate_response")
    workflow.add_edge("generate_response", END)

    app = workflow.compile()
    logger.info("Simple RAG LangGraph workflow compiled successfully.")

    return app
