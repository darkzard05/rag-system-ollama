"""
ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ - Task 12
ì‹¤ì‹œê°„ í† í° ìŠ¤íŠ¸ë¦¬ë°, SSE ì§€ì›, UI ì—…ë°ì´íŠ¸ ìµœì í™”
"""

import logging
import time
from collections.abc import AsyncIterator, Callable, Coroutine
from dataclasses import dataclass
from datetime import datetime
from typing import Any

from services.monitoring.performance_monitor import (
    OperationType,
    get_performance_monitor,
)

logger = logging.getLogger(__name__)
monitor = get_performance_monitor()


@dataclass
class StreamChunk:
    """ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ì •ë³´"""

    content: str
    timestamp: float
    token_count: int
    chunk_index: int
    is_final: bool = False
    thought: str | None = None  # ì‚¬ê³  ê³¼ì • í•„ë“œ ì¶”ê°€
    metadata: dict[str, Any] | None = None  # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    node_name: str | None = None  # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë…¸ë“œ ì´ë¦„
    status: str | None = None  # í˜„ì¬ ìƒíƒœ ë©”ì‹œì§€
    performance: dict[str, Any] | None = None  # í†µí•© ì„±ëŠ¥ í†µê³„ ì¶”ê°€


@dataclass
class StreamingMetrics:
    """ìŠ¤íŠ¸ë¦¬ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­"""

    total_tokens: int = 0
    total_time: float = 0.0
    tokens_per_second: float = 0.0
    chunk_count: int = 0
    first_token_latency: float = 0.0
    avg_chunk_size: float = 0.0
    min_latency: float = float("inf")
    max_latency: float = 0.0


class TokenStreamBuffer:
    """
    í† í° ë²„í¼ - íš¨ìœ¨ì ì¸ ë²„í¼ë§ ë° ë°°ì¹˜ ì²˜ë¦¬

    íŠ¹ì§•:
    - ë™ì  ë²„í¼ í¬ê¸° ì¡°ì •
    - íƒ€ì„ì•„ì›ƒ ê¸°ë°˜ í”ŒëŸ¬ì‹œ
    - í† í° ì¹´ìš´íŒ…
    """

    def __init__(self, buffer_size: int = 10, timeout_ms: float = 100.0):
        self.buffer_size = buffer_size
        self.timeout_ms = timeout_ms
        self.buffer: list[str] = []
        self.last_flush_time: float = time.time()

    def add_token(self, token: str) -> str | None:
        """
        í† í° ì¶”ê°€ ë° ì¡°ê±´ë¶€ í”ŒëŸ¬ì‹œ
        """
        self.buffer.append(token)
        current_time = time.time()

        # 1. ë²„í¼ê°€ ì„¤ì •ëœ í¬ê¸°ì— ë„ë‹¬í–ˆê±°ë‚˜
        # 2. ë§ˆì§€ë§‰ í”ŒëŸ¬ì‹œ ì´í›„ ì§€ì •ëœ íƒ€ì„ì•„ì›ƒ(ms)ì´ ì§€ë‚¬ìœ¼ë©´ ì¦‰ì‹œ í”ŒëŸ¬ì‹œ
        if (len(self.buffer) >= self.buffer_size) or (
            (current_time - self.last_flush_time) * 1000 >= self.timeout_ms
        ):
            return self.flush()

        return None

    def flush(self) -> str | None:
        """ë²„í¼ í”ŒëŸ¬ì‹œ"""
        if not self.buffer:
            return None

        content = "".join(self.buffer)
        self.buffer.clear()
        self.last_flush_time = time.time()
        return content

    def reset(self) -> None:
        """ë²„í¼ ì´ˆê¸°í™”"""
        self.buffer.clear()
        self.last_flush_time = time.time()


class StreamingResponseHandler:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ê¸° - ì‹¤ì‹œê°„ í† í° ìŠ¤íŠ¸ë¦¬ë°

    íŠ¹ì§•:
    - í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°
    - ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    - ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
    - SSE í˜¸í™˜ì„±
    """

    def __init__(self, buffer_size: int = 1, timeout_ms: float = 30.0):
        self.buffer = TokenStreamBuffer(buffer_size, timeout_ms)
        self.metrics = StreamingMetrics()
        self.chunk_index = 0
        self.start_time: float | None = None
        self.first_token_time: float | None = None
        self.last_chunk_time: float | None = None

    async def stream_graph_events(
        self,
        event_stream: AsyncIterator[dict[str, Any]],
        adaptive_controller: "AdaptiveStreamingController | None" = None,
    ) -> AsyncIterator[StreamChunk]:
        """
        LangGraph ì´ë²¤íŠ¸ë¥¼ ì†Œë¹„í•˜ì—¬ ê°€ê³µëœ ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ë¥¼ ìƒì„± (ë¦¬ì†ŒìŠ¤ ì•ˆì „ ê´€ë¦¬ ìµœì í™”)

        [ì¤‘ìš”: FastAPI 0.128.0 ë¦¬ì†ŒìŠ¤ ìˆ˜ëª… ì£¼ê¸°]
        - ì´ ìƒì„±ê¸°ê°€ ì™„ë£Œë˜ë©´ FastAPIì˜ 'yield' ê¸°ë°˜ ì˜ì¡´ì„±(ì˜ˆ: DB ì„¸ì…˜)ì´ ì¦‰ì‹œ ë‹«í™ë‹ˆë‹¤.
        - ë”°ë¼ì„œ BackgroundTasksì—ì„œ ì´ ìŠ¤íŠ¸ë¦¼ì˜ ê²°ê³¼ë‚˜ ë¦¬ì†ŒìŠ¤ë¥¼ ê³µìœ í•˜ëŠ” ê²ƒì€ ìœ„í—˜í•©ë‹ˆë‹¤.
        - ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ì˜ ì •ë¦¬ ì‘ì—…ì€ ì´ ë©”ì„œë“œì˜ 'finally' ë¸”ë¡ ë‚´ì—ì„œ ì§ì ‘ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤.
        """
        from contextlib import aclosing

        self.start_time = time.time()
        self.last_chunk_time = self.start_time
        self.chunk_index = 0
        self.metrics = StreamingMetrics()
        self.first_token_time = None
        self.buffer.reset()

        # ë…¸ë“œ ì´ë¦„ ë§¤í•‘ (UI í”¼ë“œë°±ìš©) - [ìµœì í™”] ì„¤ì •ì— ë”°ë¼ í‘œì‹œ ì—¬ë¶€ ê²°ì •
        from common.config import QUERY_EXPANSION_CONFIG

        node_status_map = {}
        if QUERY_EXPANSION_CONFIG.get("enabled", True):
            node_status_map["generate_queries"] = "ê²€ìƒ‰ì–´ í™•ì¥ ì¤‘..."

        # ìƒì‹œ í™œì„± ë…¸ë“œ
        node_status_map.update(
            {
                "retrieve": "ğŸ” ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ë° í•˜ì´ë¸Œë¦¬ë“œ ì§€ì‹ ê²€ìƒ‰ ì¤‘",
                "rerank_documents": "âš–ï¸ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ë­í‚¹ ë° ë¬¸ì„œ ì í•©ë„ ê²€ì¦ ì¤‘",
                "grade_documents": "ğŸ¯ í•µì‹¬ ë‹µë³€ ê·¼ê±° ì„ ì • ë° ì»¨í…ìŠ¤íŠ¸ ì •ì œ",
                "format_context": "ğŸ§© ë‹µë³€ êµ¬ì„±ì„ ìœ„í•œ ì§€ì‹ ì»¨í…ìŠ¤íŠ¸ ë³‘í•© ì¤‘",
                "generate": "âœï¸ ì§€ì‹ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ë‹µë³€ ì‘ì„± ì‹œì‘",
            }
        )

        try:
            async with aclosing(event_stream) as stream:  # type: ignore[type-var]
                async for event in stream:
                    kind = event["event"]
                    name = event.get("name", "Unknown")
                    data = event.get("data", {})

                    # 1. ë…¸ë“œ ì‹œì‘ ì´ë²¤íŠ¸ ì²˜ë¦¬ (ìƒíƒœ ì—…ë°ì´íŠ¸ìš©)
                    if kind == "on_chain_start" and name in node_status_map:
                        yield StreamChunk(
                            content="",
                            timestamp=time.time(),
                            token_count=0,
                            chunk_index=self.chunk_index,
                            node_name=name,
                            status=node_status_map[name],
                        )
                        self.chunk_index += 1

                    # 2. ì»¤ìŠ¤í…€ ì‘ë‹µ ì²­í¬ ì´ë²¤íŠ¸ ì²˜ë¦¬ (astream_events v2 ê¸°ì¤€)
                    # [ì—…ë°ì´íŠ¸] get_stream_writerë¥¼ í†µí•´ ì „ë‹¬ëœ custom ë°ì´í„° í¬í•¨
                    elif (kind == "on_custom_event" and name == "response_chunk") or (
                        kind == "on_chain_stream" and "chunk" in data.get("chunk", {})
                        if isinstance(data.get("chunk"), dict)
                        else False
                    ):
                        # ë°ì´í„° ì •ê·œí™” (ì§ì ‘ ì „ì†¡ vs writer ì „ì†¡ ëŒ€ì‘)
                        chunk_data = (
                            data.get("chunk")
                            if isinstance(data.get("chunk"), dict)
                            else data
                        )
                        content = chunk_data.get("chunk", "")
                        thought = chunk_data.get("thought", "")
                        current_time = time.time()

                        # ì§€ì—° ì‹œê°„ ê¸°ë¡ ë° ì ì‘í˜• ì œì–´
                        if adaptive_controller and self.last_chunk_time:
                            latency_ms = (current_time - self.last_chunk_time) * 1000
                            adaptive_controller.record_latency(latency_ms)
                            self.buffer.buffer_size = (
                                adaptive_controller.get_buffer_size()
                            )

                        self.last_chunk_time = current_time

                        # ì‚¬ê³  ê³¼ì • ì²˜ë¦¬ (ë²„í¼ë§ ì—†ì´ ì¦‰ì‹œ ì „ì†¡)
                        if thought:
                            yield StreamChunk(
                                content="",
                                timestamp=current_time,
                                token_count=0,
                                chunk_index=self.chunk_index,
                                thought=thought,
                            )
                            self.chunk_index += 1

                        # ë‹µë³€ ë³¸ë¬¸ ì²˜ë¦¬ (ì ì‘í˜• ë²„í¼ë§)
                        if content:
                            if self.first_token_time is None:
                                self.first_token_time = current_time
                                # ì²« í† í°ì€ ë²„í¼ë§ ì—†ì´ ì¦‰ì‹œ í”ŒëŸ¬ì‹œí•˜ì—¬ TTFT ìµœì í™”
                                if self.buffer.buffer:
                                    flushed = self.buffer.flush()
                                    if flushed:
                                        content = flushed + content

                            buffered_content = self.buffer.add_token(content)
                            if buffered_content:
                                chunk = StreamChunk(
                                    content=buffered_content,
                                    timestamp=current_time,
                                    token_count=max(1, len(buffered_content) // 4),
                                    chunk_index=self.chunk_index,
                                )
                                self.metrics.total_tokens += chunk.token_count
                                self.metrics.chunk_count += 1
                                yield chunk
                                self.chunk_index += 1

                    # 2. ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
                    elif kind == "on_chain_end":
                        if name == "retrieve":
                            output = data.get("output", {})
                            docs = output.get("relevant_docs", [])
                            if docs:
                                yield StreamChunk(
                                    content="",
                                    timestamp=time.time(),
                                    token_count=0,
                                    chunk_index=self.chunk_index,
                                    metadata={"documents": docs},
                                )
                                self.chunk_index += 1

                        elif name == "format_context":
                            # [ìµœì í™”] ì´ì œ ë” ì´ìƒ ì£¼ì„(annotations)ì„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
                            pass

                        # [ì¶”ê°€] ë‹µë³€ ìƒì„± ì™„ë£Œ ì‹œ í†µí•© ì„±ëŠ¥ ì§€í‘œ ìº¡ì²˜
                        elif name == "generate":
                            output = data.get("output", {})
                            if "performance" in output:
                                yield StreamChunk(
                                    content="",
                                    timestamp=time.time(),
                                    token_count=0,
                                    chunk_index=self.chunk_index,
                                    performance=output["performance"],
                                )
                                self.chunk_index += 1
        except Exception as e:
            logger.error(f"[Streaming] ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ í˜„ì¬ê¹Œì§€ì˜ ë‚´ìš©ì´ë¼ë„ ë³´ë‚´ê¸° ìœ„í•´ ì•„ë˜ finally ì ˆë¡œ ì´ë™
        finally:
            # ë‚¨ì€ ë²„í¼ í”ŒëŸ¬ì‹œ
            remaining = self.buffer.flush()
            if remaining:
                final_chunk = StreamChunk(
                    content=remaining,
                    timestamp=time.time(),
                    token_count=len(remaining.split()),
                    chunk_index=self.chunk_index,
                    is_final=True,
                )
                self.metrics.total_tokens += final_chunk.token_count
                self.metrics.chunk_count += 1
                yield final_chunk

            # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
            self.metrics.total_time = time.time() - self.start_time
            if self.first_token_time:
                self.metrics.first_token_latency = (
                    self.first_token_time - self.start_time
                )
            if self.metrics.total_time > 0:
                self.metrics.tokens_per_second = (
                    self.metrics.total_tokens / self.metrics.total_time
                )

    async def stream_response(
        self,
        response_generator: AsyncIterator[str],
        on_chunk: Callable[[StreamChunk], Coroutine[Any, Any, None]],
        on_complete: Callable[[], Coroutine[Any, Any, None]] | None = None,
        on_error: Callable[[Exception], Coroutine[Any, Any, None]] | None = None,
        operation_name: str = "response_streaming",
        adaptive_controller: "AdaptiveStreamingController | None" = None,
    ) -> StreamingMetrics:
        """
        ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²˜ë¦¬

        Args:
            response_generator: í† í°ì„ ìƒì„±í•˜ëŠ” ë¹„ë™ê¸° ì´í„°ë ˆì´í„°
            on_chunk: ì²­í¬ê°€ ë„ì°©í•  ë•Œ í˜¸ì¶œí•  ì½œë°±
            on_complete: ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹œ í˜¸ì¶œí•  ì½œë°±
            on_error: ì—ëŸ¬ ë°œìƒ ì‹œ í˜¸ì¶œí•  ì½œë°±
            operation_name: ì‘ì—… ì´ë¦„
            adaptive_controller: ì ì‘í˜• ìŠ¤íŠ¸ë¦¬ë° ì œì–´ê¸°

        Returns:
            ìŠ¤íŠ¸ë¦¬ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­
        """
        self.start_time = time.time()
        self.metrics = StreamingMetrics()
        self.chunk_index = 0

        with monitor.track_operation(
            OperationType.LLM_INFERENCE,
            {"stage": "streaming", "buffer_size": self.buffer.buffer_size},
        ) as op:
            try:
                async for token in response_generator:
                    # ì ì‘í˜• ë²„í¼ í¬ê¸° ì ìš©
                    if adaptive_controller:
                        new_size = adaptive_controller.get_buffer_size()
                        if self.buffer.buffer_size != new_size:
                            self.buffer.buffer_size = new_size

                    # ì²« í† í° ì‹œê°„ ê¸°ë¡
                    if self.first_token_time is None:
                        self.first_token_time = time.time()
                        self.metrics.first_token_latency = (
                            self.first_token_time - self.start_time
                        )
                        logger.info(
                            f"[Streaming] ì²« í† í° ì§€ì—°: {self.metrics.first_token_latency * 1000:.2f}ms"
                        )

                    # ë²„í¼ì— í† í° ì¶”ê°€
                    buffered_content = self.buffer.add_token(token)

                    if buffered_content:
                        # ì²­í¬ ìƒì„± ë° ì „ì†¡
                        chunk = StreamChunk(
                            content=buffered_content,
                            timestamp=time.time(),
                            token_count=len(buffered_content.split()),
                            chunk_index=self.chunk_index,
                            is_final=False,
                        )

                        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                        self.metrics.total_tokens += chunk.token_count
                        self.metrics.chunk_count += 1

                        # ì§€ì—° ì‹œê°„ ì¶”ì 
                        latency = chunk.timestamp - self.start_time
                        self.metrics.min_latency = min(
                            self.metrics.min_latency, latency
                        )
                        self.metrics.max_latency = max(
                            self.metrics.max_latency, latency
                        )

                        await on_chunk(chunk)
                        self.chunk_index += 1

                # ë‚¨ì€ ë²„í¼ í”ŒëŸ¬ì‹œ
                remaining = self.buffer.flush()
                if remaining:
                    final_chunk = StreamChunk(
                        content=remaining,
                        timestamp=time.time(),
                        token_count=len(remaining.split()),
                        chunk_index=self.chunk_index,
                        is_final=True,
                    )
                    self.metrics.total_tokens += final_chunk.token_count
                    self.metrics.chunk_count += 1
                    await on_chunk(final_chunk)

                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìµœì¢… ê³„ì‚°
                self.metrics.total_time = time.time() - self.start_time
                self.metrics.tokens_per_second = (
                    self.metrics.total_tokens / self.metrics.total_time
                    if self.metrics.total_time > 0
                    else 0
                )
                self.metrics.avg_chunk_size = (
                    self.metrics.total_tokens / self.metrics.chunk_count
                    if self.metrics.chunk_count > 0
                    else 0
                )

                # ì™„ë£Œ ì½œë°±
                if on_complete:
                    await on_complete()

                logger.info(
                    f"[Streaming] ì™„ë£Œ: "
                    f"{self.metrics.total_tokens} í† í°, "
                    f"{self.metrics.total_time:.2f}ì´ˆ, "
                    f"{self.metrics.tokens_per_second:.1f} tok/s"
                )

                op.tokens = self.metrics.total_tokens

            except Exception as e:
                logger.error(f"[Streaming] ì—ëŸ¬: {e}")
                op.error = str(e)

                if on_error:
                    await on_error(e)
                else:
                    raise

        return self.metrics


class ServerSentEventsHandler:
    """
    Server-Sent Events (SSE) ì²˜ë¦¬ê¸°

    íŠ¹ì§•:
    - SSE í‘œì¤€ ì¤€ìˆ˜ (W3C Recommendation)
    - ë©€í‹°ë¼ì¸ ë°ì´í„° ì§€ì›
    - Keep-alive ë° ì¬ì—°ê²° ì„¤ì • ì§€ì›
    """

    @staticmethod
    def format_sse_event(
        event_type: str, data: dict[str, Any], event_id: int | None = None
    ) -> str:
        """
        SSE í˜•ì‹ìœ¼ë¡œ ì´ë²¤íŠ¸ í¬ë§¤íŒ… (orjson ê³ ì† ì§ë ¬í™” ì ìš©)
        """
        import orjson

        lines = []

        if event_id is not None:
            lines.append(f"id: {event_id}")

        if event_type:
            lines.append(f"event: {event_type}")

        # [ìµœì í™”] orjson ì‚¬ìš©ìœ¼ë¡œ ê³ ì† ì§ë ¬í™” (ensure_ascii=False íš¨ê³¼ í¬í•¨)
        # orjson.dumpsëŠ” bytesë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ decode í•„ìš”
        json_data = orjson.dumps(data).decode("utf-8")

        # SSE ê·œê²©: í•œ ì¤„ ë°ì´í„° ì „ì†¡ (orjsonì€ ê¸°ë³¸ì ìœ¼ë¡œ í•œ ì¤„ì„)
        lines.append(f"data: {json_data}")

        lines.append("")  # ë¹ˆ ì¤„ë¡œ ì´ë²¤íŠ¸ ì¢…ë£Œ êµ¬ë¶„
        return "\n".join(lines) + "\n"

    @staticmethod
    def format_sse_error(error_message: str, error_code: int = 500) -> str:
        """í‘œì¤€í™”ëœ SSE ì—ëŸ¬ ì´ë²¤íŠ¸ í¬ë§¤íŒ…"""
        data = {
            "error": error_message,
            "code": error_code,
            "timestamp": datetime.now().isoformat(),
        }
        return ServerSentEventsHandler.format_sse_event("error", data)

    @staticmethod
    def format_sse_keepalive(message: str = "keep-alive") -> str:
        """SSE keep-alive (ì£¼ì„ í˜•ì‹) í¬ë§¤íŒ… - ì—°ê²° ìœ ì§€ìš©"""
        return f": {message}\n\n"


class StreamingResponseBuilder:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë¹Œë” - ì²­í¬ë¥¼ ëˆ„ì í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±
    """

    def __init__(self, max_buffer_size: int = 100000):
        self.chunks: list[StreamChunk] = []
        self.max_buffer_size = max_buffer_size
        self.total_content = ""

    def add_chunk(self, chunk: StreamChunk) -> None:
        """ì²­í¬ ì¶”ê°€"""
        if len(self.total_content) + len(chunk.content) > self.max_buffer_size:
            logger.warning("[StreamingBuilder] ë²„í¼ í¬ê¸° ì´ˆê³¼, ìµœì‹  ì²­í¬ë¶€í„° ë³´ê´€")
            # ì˜¤ë˜ëœ ì²­í¬ ì œê±°
            while self.chunks and len(self.total_content) > self.max_buffer_size * 0.8:
                removed = self.chunks.pop(0)
                self.total_content = self.total_content[len(removed.content) :]

        self.chunks.append(chunk)
        self.total_content += chunk.content

    def get_content(self) -> str:
        """ëˆ„ì ëœ ì „ì²´ ë‚´ìš© ë°˜í™˜"""
        return self.total_content

    def get_chunks(self) -> list[StreamChunk]:
        """ëª¨ë“  ì²­í¬ ë°˜í™˜"""
        return self.chunks

    def reset(self) -> None:
        """ë¹Œë” ì´ˆê¸°í™”"""
        self.chunks.clear()
        self.total_content = ""


class AdaptiveStreamingController:
    """
    ì ì‘í˜• ìŠ¤íŠ¸ë¦¬ë° ì œì–´ê¸° - ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ë²„í¼ í¬ê¸° ìë™ ì¡°ì •

    íŠ¹ì§•:
    - ë„¤íŠ¸ì›Œí¬ ì§€ì—° ê°ì§€
    - ë™ì  ë²„í¼ í¬ê¸° ì¡°ì •
    - ì²˜ë¦¬ëŸ‰ ìµœì í™”
    """

    def __init__(
        self,
        initial_buffer_size: int = 1,
        min_buffer_size: int = 1,
        max_buffer_size: int = 10,
    ):
        self.current_buffer_size = initial_buffer_size
        self.min_buffer_size = min_buffer_size
        self.max_buffer_size = max_buffer_size
        self.latency_samples: list[float] = []
        self.max_samples = 50

    def record_latency(self, latency_ms: float) -> None:
        """ì§€ì—° ì‹œê°„ ê¸°ë¡"""
        self.latency_samples.append(latency_ms)

        # ìƒ˜í”Œ ìœ ì§€
        if len(self.latency_samples) > self.max_samples:
            self.latency_samples.pop(0)

        # ë²„í¼ í¬ê¸° ì¡°ì •
        self._adjust_buffer_size()

    def _adjust_buffer_size(self) -> None:
        """ì§€ì—° ì‹œê°„ ê¸°ë°˜ ë²„í¼ í¬ê¸° ì¡°ì •"""
        if len(self.latency_samples) < 10:
            return

        avg_latency = sum(self.latency_samples) / len(self.latency_samples)

        # ì§€ì—°ì´ ë†’ìœ¼ë©´ ë²„í¼ í¬ê¸° ì¦ê°€ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ íšŸìˆ˜ ê°ì†Œ)
        if avg_latency > 300:  # 300ms ì´ìƒìœ¼ë¡œ ê¸°ì¤€ ìƒí–¥ (ë„ˆë¬´ ë¹ˆë²ˆí•œ ì¡°ì ˆ ë°©ì§€)
            new_size = min(
                self.current_buffer_size + 2, self.max_buffer_size
            )  # ê¸‰ê²©í•œ ì¦ê°€ ë°©ì§€ (+5 -> +2)
            if new_size != self.current_buffer_size:
                logger.info(
                    f"[AdaptiveStreaming] ì§€ì—° ë†’ìŒ ({avg_latency:.1f}ms), "
                    f"ë²„í¼ ì¦ê°€: {self.current_buffer_size} â†’ {new_size}"
                )
                self.current_buffer_size = new_size

        # ì§€ì—°ì´ ë‚®ìœ¼ë©´ ë²„í¼ í¬ê¸° ê°ì†Œ (ë” ë¹ˆë²ˆí•œ ì—…ë°ì´íŠ¸)
        elif avg_latency < 100:  # 100ms ì´í•˜ë¡œ ê¸°ì¤€ ìƒí–¥
            new_size = max(self.current_buffer_size - 1, self.min_buffer_size)
            if new_size != self.current_buffer_size:
                logger.info(
                    f"[AdaptiveStreaming] ì§€ì—° ë‚®ìŒ ({avg_latency:.1f}ms), "
                    f"ë²„í¼ ê°ì†Œ: {self.current_buffer_size} â†’ {new_size}"
                )
                self.current_buffer_size = new_size

    def get_buffer_size(self) -> int:
        """í˜„ì¬ ë²„í¼ í¬ê¸° ë°˜í™˜"""
        return self.current_buffer_size

    def get_metrics(self) -> dict[str, float]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        if not self.latency_samples:
            return {}

        return {
            "avg_latency_ms": sum(self.latency_samples) / len(self.latency_samples),
            "min_latency_ms": min(self.latency_samples),
            "max_latency_ms": max(self.latency_samples),
            "current_buffer_size": self.current_buffer_size,
            "sample_count": len(self.latency_samples),
        }


def get_streaming_handler() -> StreamingResponseHandler:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ê¸° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    [ë³´ì•ˆ/ë™ì‹œì„± ìˆ˜ì •] ìš”ì²­ë³„ ê²©ë¦¬ë¥¼ ìœ„í•´ í•­ìƒ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    return StreamingResponseHandler()


def get_adaptive_controller() -> AdaptiveStreamingController:
    """
    ì ì‘í˜• ìŠ¤íŠ¸ë¦¬ë° ì œì–´ê¸° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    [ë³´ì•ˆ/ë™ì‹œì„± ìˆ˜ì •] ìš”ì²­ë³„ ê²©ë¦¬ë¥¼ ìœ„í•´ í•­ìƒ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    return AdaptiveStreamingController()
