"""
Streamlit UI ì»´í¬ë„ŒíŠ¸ ë Œë”ë§ í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ íŒŒì¼.
Clean & Minimal Version: ë¶€ê°€ ìš”ì†Œ ì œê±°, ì§ê´€ì ì¸ ë¡œë”© ë° ìŠ¤íŠ¸ë¦¬ë°.
"""

import time
import logging
from typing import Callable, Optional

import streamlit as st
from streamlit_pdf_viewer import pdf_viewer
import fitz  # PyMuPDF

from session import SessionManager
from model_loader import get_available_models
from utils import sync_run
from config import (
    AVAILABLE_EMBEDDING_MODELS,
    OLLAMA_MODEL_NAME,
    UI_CONTAINER_HEIGHT,
    MSG_SIDEBAR_TITLE,
    MSG_PDF_UPLOADER_LABEL,
    MSG_MODEL_SELECTOR_LABEL,
    MSG_EMBEDDING_SELECTOR_LABEL,
    MSG_SYSTEM_STATUS_TITLE,
    MSG_PDF_VIEWER_TITLE,
    MSG_PDF_VIEWER_NO_FILE,
    MSG_PDF_VIEWER_PREV_BUTTON,
    MSG_PDF_VIEWER_NEXT_BUTTON,
    MSG_PDF_VIEWER_ERROR,
    MSG_CHAT_TITLE,
    MSG_CHAT_INPUT_PLACEHOLDER,
    MSG_CHAT_NO_QA_SYSTEM,
    MSG_CHAT_WELCOME,
    MSG_ERROR_OLLAMA_NOT_RUNNING,
)

logger = logging.getLogger(__name__)


async def _stream_chat_response(qa_chain, user_input: str, chat_container) -> str:
    """
    ë¯¸ë‹ˆë©€ ë‹µë³€ ìƒì„± í•¨ìˆ˜.
    ë¡œë”© ë©”ì‹œì§€ í‘œì‹œ í›„ í† í°ì´ ë“¤ì–´ì˜¤ë©´ ë‹µë³€ìœ¼ë¡œ ì¦‰ì‹œ ëŒ€ì²´í•©ë‹ˆë‹¤.
    """
    full_response = ""
    start_time = time.time()
    last_update_time = 0
    update_interval = 0.05
    has_started = False  # ì²« í† í° ìˆ˜ì‹  ì—¬ë¶€

    current_llm = SessionManager.get("llm")
    if not current_llm:
        return "âŒ ì˜¤ë¥˜: LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    run_config = {"configurable": {"llm": current_llm}}
    SessionManager.set("is_generating_answer", True)

    try:
        with chat_container:
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                # ë‹¨ í•˜ë‚˜ì˜ ì»¨í…Œì´ë„ˆë¡œ ë¡œë”©ê³¼ ë‹µë³€ ëª¨ë‘ ì²˜ë¦¬
                answer_container = st.empty()
                answer_container.markdown("âŒ› ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

                async for event in qa_chain.astream_events(
                    {"input": user_input},
                    config=run_config,
                    version="v1"
                ):
                    kind = event["event"]
                    chunk_text = None

                    # 1. ì‹¤ì‹œê°„ í† í° ìˆ˜ì‹  (íŒŒì„œ ìŠ¤íŠ¸ë¦¼ ìš°ì„ )
                    if kind == "on_parser_stream":
                        chunk = event["data"].get("chunk")
                        if isinstance(chunk, str):
                            chunk_text = chunk
                    
                    # 2. ë°±ì—…: ëª¨ë¸ ìŠ¤íŠ¸ë¦¼ (íŒŒì„œê°€ ì—†ê±°ë‚˜ ë†“ì¹œ ê²½ìš°)
                    elif kind == "on_chat_model_stream":
                        chunk = event["data"].get("chunk")
                        if hasattr(chunk, "content") and chunk.content:
                            # ì´ë¯¸ íŒŒì„œì—ì„œ ì²˜ë¦¬ëœ ê²½ìš° ì¤‘ë³µ ë°©ì§€ê°€ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜,
                            # StrOutputParserë¥¼ ì“°ë©´ ë³´í†µ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ íƒ€ê±°ë‚˜ ë‚´ìš©ì´ ë™ì¼í•¨.
                            # ì—¬ê¸°ì„œëŠ” íŒŒì„œ ì´ë²¤íŠ¸ê°€ ì—†ì„ ë•Œë¥¼ ëŒ€ë¹„í•´ ì‚¬ìš©
                            if not full_response: # ì•„ì£¼ ì´ˆê¸°ì—ë§Œ ìœ ìš©í•  ìˆ˜ ìˆìŒ (ë˜ëŠ” íŒŒì„œ ë¯¸ì‚¬ìš© ì‹œ)
                                pass 
                            # ì£¼ì˜: LangChainì—ì„œ íŒŒì„œì™€ ëª¨ë¸ ìŠ¤íŠ¸ë¦¼ì´ ë™ì‹œì— ë°œìƒí•  ìˆ˜ ìˆìŒ.
                            # StrOutputParserê°€ ìˆë‹¤ë©´ on_parser_streamë§Œ ë¯¿ëŠ” ê²ƒì´ ì¤‘ë³µ ì¶œë ¥ ë°©ì§€ì— ì•ˆì „í•¨.
                            # í•˜ì§€ë§Œ íŒŒì„œê°€ ë™ì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš°ë¥¼ ìœ„í•´ ë‚¨ê²¨ë‘  (ë‹¨, ì¤‘ë³µ ì£¼ì˜)
                            pass

                    if chunk_text:
                        full_response += chunk_text
                        if not has_started:
                            has_started = True
                        
                        now = time.time()
                        if now - last_update_time > update_interval:
                            answer_container.markdown(full_response + "â–Œ")
                            last_update_time = now

                    # 3. ì•ˆì „ ì¥ì¹˜: ìµœì¢… ê²°ê³¼ ê°€ë¡œì±„ê¸° (generate_response ë…¸ë“œ ì™„ë£Œ ì‹œ)
                    if kind == "on_chain_end" and event.get("name") == "generate_response":
                        output = event.get("data", {}).get("output")
                        # GraphState ë”•ì…”ë„ˆë¦¬ì—ì„œ response ì¶”ì¶œ
                        if isinstance(output, dict) and "response" in output:
                            final_node_res = output["response"]
                            # ìŠ¤íŠ¸ë¦¬ë°ëœ ê²ƒë³´ë‹¤ ìµœì¢… ê²°ê³¼ê°€ ê¸¸ë‹¤ë©´ êµì²´ (ëˆ„ë½ ë°©ì§€)
                            if len(final_node_res) > len(full_response):
                                full_response = final_node_res

                # ìµœì¢… ë Œë”ë§ (ì»¤ì„œ ì œê±° ë° ìµœì¢… í…ìŠ¤íŠ¸ í™•ì •)
                elapsed_time = time.time() - start_time
                if full_response:
                    answer_container.markdown(full_response)
                    logger.info(f"[UI] ë‹µë³€ ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ")
                else:
                    # ìƒì„¸ ë¡œê¹… ì¶”ê°€
                    llm_model = getattr(current_llm, "model", "Unknown")
                    has_qa_chain = qa_chain is not None
                    
                    log_msg = (
                        f"[UI] ë‹µë³€ ìƒì„± ì‹¤íŒ¨ (ë¹ˆ ì‘ë‹µ). "
                        f"ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ, "
                        f"LLM ëª¨ë¸: {llm_model}, "
                        f"QAì²´ì¸ ì¡´ì¬: {has_qa_chain}, "
                        f"ì´ë²¤íŠ¸ ì‹œì‘ ì—¬ë¶€: {has_started}"
                    )
                    logger.warning(log_msg)
                    
                    error_detail = "âš ï¸ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì‘ë‹µ ê°’ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.)"
                    if not has_started:
                        error_detail = "âš ï¸ ë‹µë³€ ìƒì„±ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Ollama ì„œë²„ ìƒíƒœë‚˜ ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
                    
                    answer_container.error(error_detail)

        return full_response
    except Exception as e:
        logger.error(f"Streaming error: {e}", exc_info=True)
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    finally:
        SessionManager.set("is_generating_answer", False)


def render_sidebar(
    file_uploader_callback: Callable,
    model_selector_callback: Callable,
    embedding_selector_callback: Callable
):
    """
    ìµœì†Œí•œì˜ ì„¤ì •ë§Œ ë‚¨ê¸´ ì‚¬ì´ë“œë°”ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
    ê³µê°„ íš¨ìœ¨ì„ ìœ„í•´ êµ¬ë¶„ì„ ì„ ì œê±°í•˜ê³  Expanderë¥¼ í™œìš©í•©ë‹ˆë‹¤.

    Args:
        file_uploader_callback: íŒŒì¼ ì—…ë¡œë“œ ì‹œ ì‹¤í–‰ë  ì½œë°± í•¨ìˆ˜
        model_selector_callback: LLM ëª¨ë¸ ë³€ê²½ ì‹œ ì‹¤í–‰ë  ì½œë°± í•¨ìˆ˜
        embedding_selector_callback: ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ì‹œ ì‹¤í–‰ë  ì½œë°± í•¨ìˆ˜
    """
    with st.sidebar:
        st.header(MSG_SIDEBAR_TITLE)
        
        # 1. íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ (ê°€ì¥ ì¤‘ìš”í•˜ë¯€ë¡œ ìƒì‹œ ë…¸ì¶œ)
        st.file_uploader(
            MSG_PDF_UPLOADER_LABEL, 
            type="pdf", 
            key="pdf_uploader", 
            on_change=file_uploader_callback
        )

        # 2. ëª¨ë¸ ì„¤ì • ì„¹ì…˜ (ì ‘ì´ì‹ìœ¼ë¡œ ê³µê°„ ì ˆì•½)
        # expanded=Falseë¡œ ì„¤ì •í•˜ì—¬ ê¸°ë³¸ì ìœ¼ë¡œëŠ” ìˆ¨ê¹€ ì²˜ë¦¬ (ì‚¬ìš©ì ê²½í—˜ì— ë”°ë¼ ë³€ê²½ ê°€ëŠ¥)
        with st.expander("âš™ï¸ ëª¨ë¸ ì„¤ì •", expanded=False):
            # ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            available_models = get_available_models()
            is_ollama_error = (
                bool(available_models) and 
                available_models[0] == MSG_ERROR_OLLAMA_NOT_RUNNING
            )
            actual_models = [] if is_ollama_error else [m for m in available_models if "---" not in m]
            
            last_model = SessionManager.get("last_selected_model")
            if not last_model or (actual_models and last_model not in actual_models):
                last_model = actual_models[0] if actual_models else OLLAMA_MODEL_NAME
                SessionManager.set("last_selected_model", last_model)

            try:
                idx = available_models.index(last_model)
            except ValueError:
                idx = 0

            # ê¸´ ëª¨ë¸ ì´ë¦„ì´ ì˜ë¦¬ëŠ” ë¬¸ì œë¥¼ ìœ„í•´ help íˆ´íŒ ì¶”ê°€
            st.selectbox(
                MSG_MODEL_SELECTOR_LABEL, 
                available_models, 
                index=idx, 
                key="model_selector", 
                on_change=model_selector_callback, 
                disabled=is_ollama_error,
                help="ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”."
            )

            # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
            last_emb = SessionManager.get("last_selected_embedding_model") or AVAILABLE_EMBEDDING_MODELS[0]
            try:
                emb_idx = AVAILABLE_EMBEDDING_MODELS.index(last_emb)
            except ValueError:
                emb_idx = 0
                
            st.selectbox(
                MSG_EMBEDDING_SELECTOR_LABEL, 
                AVAILABLE_EMBEDDING_MODELS, 
                index=emb_idx, 
                key="embedding_model_selector", 
                on_change=embedding_selector_callback,
                help="ë¬¸ì„œ ê²€ìƒ‰ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤."
            )
        
        # 3. ì‹œìŠ¤í…œ ìƒíƒœ ì„¹ì…˜ (êµ¬ë¶„ì„  ì—†ì´ ì—¬ë°±ìœ¼ë¡œ ë¶„ë¦¬)
        st.markdown("#### " + MSG_SYSTEM_STATUS_TITLE)
        
        # ìƒíƒœ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ë¹ˆ ì»¨í…Œì´ë„ˆ ë°˜í™˜
        return st.container()


def render_pdf_viewer():
    _pdf_viewer_fragment()


@st.fragment
def _pdf_viewer_fragment():
    """PDF ë·°ì–´ (Fragment) - ê°œì„ ëœ ë„¤ë¹„ê²Œì´ì…˜"""
    st.subheader(MSG_PDF_VIEWER_TITLE)
    
    pdf_bytes = SessionManager.get("pdf_file_bytes")
    if not pdf_bytes:
        st.info(MSG_PDF_VIEWER_NO_FILE)
        return
    
    try:
        with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
            total_pages = len(doc)
            
            # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            if "current_page" not in st.session_state: 
                st.session_state.current_page = 1
            
            # í˜ì´ì§€ ë²”ìœ„ ë³´ì • (íŒŒì¼ ë³€ê²½ ë“±ìœ¼ë¡œ ì´ í˜ì´ì§€ê°€ ì¤„ì—ˆì„ ë•Œ)
            if st.session_state.current_page > total_pages:
                st.session_state.current_page = 1
            if st.session_state.current_page < 1:
                st.session_state.current_page = 1

            # --- PDF ë·°ì–´ ë Œë”ë§ ---
            pdf_viewer(
                input=pdf_bytes, 
                height=UI_CONTAINER_HEIGHT, 
                pages_to_render=[st.session_state.current_page]
            )

            # --- í•˜ë‹¨ ë„¤ë¹„ê²Œì´ì…˜ ---
            # í—¬í¼ í•¨ìˆ˜: í˜ì´ì§€ ë³€ê²½ ì½œë°±
            def go_prev():
                if st.session_state.current_page > 1:
                    st.session_state.current_page -= 1
            
            def go_next():
                if st.session_state.current_page < total_pages:
                    st.session_state.current_page += 1

            # ë ˆì´ì•„ì›ƒ: [ì´ì „] [í˜ì´ì§€ ì…ë ¥ / ì´í˜ì´ì§€] [ë‹¤ìŒ]
            c1, c2, c3 = st.columns([1, 1, 1])
            
            with c1:
                # ì´ì „ í˜ì´ì§€ ë²„íŠ¼ (on_click ì‚¬ìš©)
                st.button(
                    MSG_PDF_VIEWER_PREV_BUTTON, 
                    key="btn_pdf_prev", 
                    use_container_width=True,
                    disabled=(st.session_state.current_page <= 1),
                    on_click=go_prev
                )

            with c2:
                # ê°€ìš´ë° ë³‘í•©ëœ 'í˜„ì¬ / ì´ page' ë ˆì´ì•„ì›ƒ
                p1, p2 = st.columns([1, 1])
                with p1:
                    def update_page_input():
                        # number_inputì˜ ê°’ì´ ë³€ê²½ë˜ë©´ session_stateì— ë°˜ì˜
                        # keyê°€ 'num_input_page'ì´ë¯€ë¡œ st.session_state.num_input_pageì— ê°’ì´ ìˆìŒ
                        st.session_state.current_page = int(st.session_state.num_input_page)

                    st.number_input(
                        "í˜ì´ì§€ ì´ë™", 
                        min_value=1, 
                        max_value=total_pages, 
                        value=st.session_state.current_page, 
                        label_visibility="collapsed",
                        key="num_input_page",
                        on_change=update_page_input
                    )
                with p2:
                    st.markdown(
                        f"<div style='line-height: 2.3em; font-size: 1.0em;'>"
                        f"&nbsp;/ {total_pages} pages</div>", 
                        unsafe_allow_html=True
                    )

            with c3:
                # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ (on_click ì‚¬ìš©)
                st.button(
                    MSG_PDF_VIEWER_NEXT_BUTTON, 
                    key="btn_pdf_next", 
                    use_container_width=True,
                    disabled=(st.session_state.current_page >= total_pages),
                    on_click=go_next
                )
            
    except Exception as e:
        logger.error(f"PDF ë·°ì–´ ì˜¤ë¥˜: {e}", exc_info=True)
        st.error(f"PDF ì˜¤ë¥˜: {e}")


def render_left_column():
    _chat_fragment()


def render_message(role: str, content: str):
    """
    ë©”ì‹œì§€ë¥¼ ì—­í• ì— ë”°ë¼ ìŠ¤íƒ€ì¼ë§í•˜ì—¬ ë Œë”ë§í•©ë‹ˆë‹¤.
    Args:
        role: ë©”ì‹œì§€ ì‘ì„±ì ì—­í•  ('user' ë˜ëŠ” 'assistant')
        content: ë©”ì‹œì§€ ë³¸ë¬¸
    """
    # ì•„ë°”íƒ€ ì„¤ì •: ì‚¬ìš©ì(ğŸ‘¤), ì–´ì‹œìŠ¤í„´íŠ¸(ğŸ¤–)
    avatar_icon = "ğŸ¤–" if role == "assistant" else "ğŸ‘¤"
    
    with st.chat_message(role, avatar=avatar_icon):
        # ë‹µë³€ ë‚´ì˜ ì¶œì²˜(Sources) ë¶€ë¶„ì„ ì‹œê°ì ìœ¼ë¡œ ë¶„ë¦¬
        # ê°€ì •: LLM ë‹µë³€ì— 'ì¶œì²˜:' ë˜ëŠ” 'Sources:' ë¼ëŠ” êµ¬ë¶„ìê°€ ìˆë‹¤ê³  ê°€ì •
        # ì‹¤ì œ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ êµ¬ë¶„ìëŠ” ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
        
        # ê°„ë‹¨í•œ íŒŒì‹± ë¡œì§ (í•„ìš”ì‹œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ê³ ë„í™” ê°€ëŠ¥)
        separator = None
        if "ì¶œì²˜:" in content:
            separator = "ì¶œì²˜:"
        elif "Sources:" in content:
            separator = "Sources:"
            
        if role == "assistant" and separator:
            try:
                parts = content.split(separator, 1)
                main_content = parts[0].strip()
                sources = parts[1].strip()
                
                st.markdown(main_content)
                if sources:
                    with st.expander("ğŸ“š ì°¸ê³  ë¬¸í—Œ (Sources)", expanded=False):
                        st.markdown(f"**{separator}**\n{sources}")
            except Exception:
                # íŒŒì‹± ì—ëŸ¬ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ì¶œë ¥
                st.markdown(content)
        else:
            st.markdown(content)


@st.fragment
def _chat_fragment():
    """ì±„íŒ… êµ¬ì—­ (Fragment)"""
    st.subheader(MSG_CHAT_TITLE)
    chat_container = st.container(height=UI_CONTAINER_HEIGHT, border=True)

    messages = SessionManager.get_messages()
    for msg in messages:
        with chat_container:
            render_message(msg["role"], msg["content"])

    if not messages:
        with chat_container: 
            st.info(MSG_CHAT_WELCOME)

    is_gen = SessionManager.get("is_generating_answer")
    if user_input := st.chat_input(MSG_CHAT_INPUT_PLACEHOLDER, disabled=is_gen, key="chat_input_clean"):
        SessionManager.add_message("user", user_input)
        
        # ì¦‰ì‹œ ì‚¬ìš©ì ë©”ì‹œì§€ ë Œë”ë§
        with chat_container:
            render_message("user", user_input)

        qa_chain = SessionManager.get("qa_chain")
        if qa_chain:
            final_ans = sync_run(_stream_chat_response(qa_chain, user_input, chat_container))
            if final_ans and not final_ans.startswith("âŒ"):
                SessionManager.add_message("assistant", final_ans)
                st.rerun()
        else:
            st.error(MSG_CHAT_NO_QA_SYSTEM)
