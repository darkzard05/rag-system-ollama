"""
Streamlit UI ì»´í¬ë„ŒíŠ¸ ë Œë”ë§ í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ íŒŒì¼.
"""
from email import message
import os
import time
import logging
import streamlit as st
from streamlit_pdf_viewer import pdf_viewer

from session import SessionManager
from rag_core import get_available_models
from config import (
    AVAILABLE_EMBEDDING_MODELS,
    OLLAMA_MODEL_NAME,
    THINK_START_TAG,
    THINK_END_TAG,
    MSG_PREPARING_ANSWER,
    MSG_THINKING,
    MSG_NO_THOUGHT_PROCESS,
    MSG_NO_RELATED_INFO,
)

def _process_chat_response(qa_chain, user_input, chat_container):
    """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ LLM ì‘ë‹µì„ ì²˜ë¦¬í•˜ê³  ì±„íŒ… ì»¨í…Œì´ë„ˆì— í‘œì‹œ"""
    with chat_container, st.chat_message("assistant"):
        thought_expander = st.expander("ğŸ¤” ìƒê° ê³¼ì •", expanded=False)
        thought_placeholder = thought_expander.empty()
        message_placeholder = st.empty()
        
        message_placeholder.markdown(MSG_PREPARING_ANSWER)
        thought_placeholder.markdown(MSG_THINKING)

        try:
            thought_buffer = ""
            response_buffer = ""
            is_thinking = False
            
            start_time = time.time()
            
            for chunk in qa_chain.stream({"input": user_input}):
                answer_chunk = chunk.get("answer", "")
                if not answer_chunk:
                    continue
                
                if THINK_START_TAG in answer_chunk and not is_thinking:
                    is_thinking = True
                    parts = answer_chunk.split(THINK_START_TAG, 1)
                    response_buffer += parts[0]
                    thought_buffer = parts[1]
                elif THINK_END_TAG in answer_chunk:
                    is_thinking = False
                    parts = answer_chunk.split(THINK_END_TAG, 1)
                    thought_buffer += parts[0]
                    response_buffer += parts[1]
                elif is_thinking:
                    thought_buffer += answer_chunk
                else:
                    response_buffer += answer_chunk
                
                if thought_buffer.strip():
                    thought_placeholder.markdown(thought_buffer + "â–Œ")

                # ë‹µë³€ placeholderì˜ ë‚´ìš©ì„ ê²°ì •
                current_message = response_buffer
                if not current_message.strip() and is_thinking:
                    current_message = MSG_THINKING
                
                message_placeholder.markdown(current_message + "â–Œ")

            end_time = time.time()
            elapsed_time = end_time - start_time

            # ìµœì¢… ë‚´ìš© ì—…ë°ì´íŠ¸
            if thought_buffer.strip():
                thought_placeholder.markdown(thought_buffer)
            else:
                # ìŠ¤íŠ¸ë¦¼ì´ ëª¨ë‘ ëë‚œ í›„ì—ë„ ìƒê° ë‚´ìš©ì´ ì—†ìœ¼ë©´ ê·¸ë•Œ ë©”ì‹œì§€ í‘œì‹œ
                thought_placeholder.markdown(MSG_NO_THOUGHT_PROCESS)
            
            final_answer = response_buffer.strip()
            if not final_answer:
                final_answer = MSG_NO_RELATED_INFO
            
            message_placeholder.markdown(final_answer)
            SessionManager.add_message("assistant", final_answer)
            
            # ë‹µë³€ ìƒì„± ì‹œê°„ ë° ê¸€ì ìˆ˜ ë¡œê¹…
            logging.info(
                f"LLM ë‹µë³€ ìƒì„± ì™„ë£Œ. "
                f"ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ, "
                f"ë‹µë³€ ê¸¸ì´: {len(final_answer)}ì"
            )

        except Exception as e:
            error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logging.error(error_msg, exc_info=True)
            message_placeholder.error(error_msg)
            SessionManager.add_message("assistant", f"âŒ {error_msg}")

def render_sidebar(uploaded_file_handler, model_change_handler, embedding_model_change_handler):
    """ì‚¬ì´ë“œë°” UIë¥¼ ë Œë”ë§í•˜ê³  ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # --- íŒŒì¼ ì—…ë¡œë” ---
        uploaded_file = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type="pdf")
        if uploaded_file:
            uploaded_file_handler(uploaded_file)
        
        st.divider()

        # --- LLM ëª¨ë¸ ì„ íƒ (ë™ì  ëª©ë¡) ---
        available_models = get_available_models()
        
        last_model = SessionManager.get_last_selected_model()
        # ë§ˆì§€ë§‰ìœ¼ë¡œ ì„ íƒí•œ ëª¨ë¸ì´ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª©ë¡ì— ì—†ìœ¼ë©´, ëª©ë¡ì˜ ì²« ë²ˆì§¸ ëª¨ë¸ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        if last_model not in available_models:
            last_model = available_models[0] if available_models else OLLAMA_MODEL_NAME

        current_model_index = available_models.index(last_model) if last_model in available_models else 0
        
        selected_model = st.selectbox(
            "LLM ëª¨ë¸ ì„ íƒ",
            available_models,
            index=current_model_index,
            key="model_selector"
        )
        model_change_handler(selected_model)

        # --- ì„ë² ë”© ëª¨ë¸ ì„ íƒ ---
        last_embedding_model = SessionManager.get_last_selected_embedding_model() or AVAILABLE_EMBEDDING_MODELS[0]
        current_embedding_model_index = AVAILABLE_EMBEDDING_MODELS.index(last_embedding_model) if last_embedding_model in AVAILABLE_EMBEDDING_MODELS else 0
        
        selected_embedding_model = st.selectbox(
            "ì„ë² ë”© ëª¨ë¸ ì„ íƒ",
            AVAILABLE_EMBEDDING_MODELS,
            index=current_embedding_model_index,
            key="embedding_model_selector"
        )
        embedding_model_change_handler(selected_embedding_model)

        st.divider()

        # --- PDF ë·°ì–´ ì„¤ì • ---
        resolution_boost = st.slider("í•´ìƒë„", 1, 10, SessionManager.get_resolution_boost())
        SessionManager.set_resolution_boost(resolution_boost)
        pdf_width = st.slider("PDF ë„ˆë¹„", 100, 1000, SessionManager.get_pdf_width())
        SessionManager.set_pdf_width(pdf_width)
        pdf_height = st.slider("PDF ë†’ì´", 100, 10000, SessionManager.get_pdf_height())
        SessionManager.set_pdf_height(pdf_height)

def render_pdf_viewer():
    """PDF ë·°ì–´ ì»¬ëŸ¼ì„ ë Œë”ë§í•©ë‹ˆë‹¤."""
    st.subheader("ğŸ“„ PDF ë¯¸ë¦¬ë³´ê¸°")
    
    temp_pdf_path = SessionManager.get_temp_pdf_path()
    if temp_pdf_path and os.path.exists(temp_pdf_path):
        try:
            pdf_viewer(
                input=temp_pdf_path,
                width=SessionManager.get_pdf_width(),
                height=SessionManager.get_pdf_height(),
                key=f"pdf_viewer_{SessionManager.get_last_uploaded_file_name()}",
                resolution_boost=SessionManager.get_resolution_boost()
            )
        except Exception as e:
            st.error(f"PDF ë¯¸ë¦¬ë³´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logging.error("PDF ë¯¸ë¦¬ë³´ê¸° ì˜¤ë¥˜", exc_info=True)

def render_chat_column():
    """ì±„íŒ… ì»¬ëŸ¼ì„ ë Œë”ë§í•˜ê³  ì±„íŒ… ë¡œì§ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    st.subheader("ğŸ’¬ ì±„íŒ…")
    
    chat_container = st.container(height=650, border=True)
    
    # ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
    for message in SessionManager.get_messages():
        with chat_container, st.chat_message(message["role"]):
            st.markdown(message["content"], unsafe_allow_html=True)
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_input := st.chat_input(
        "PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.",
        disabled=not SessionManager.is_ready_for_chat()
    ):
        SessionManager.add_message("user", user_input)
        with chat_container, st.chat_message("user"):
            st.markdown(user_input)
        
        qa_chain = SessionManager.get_qa_chain()
        if qa_chain:
            _process_chat_response(qa_chain, user_input, chat_container)
        else:
            st.error("QA ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")

    # ì´ˆê¸° ì•ˆë‚´ ë©”ì‹œì§€
    if not SessionManager.get_messages():
        with chat_container:
            st.info(
                "**RAG-Chatì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!**\n\n"
                "ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•œ ëŒ€í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”."
            )
            
            st.markdown(
                """
                **ğŸ’¡ ì‚¬ìš© ê°€ì´ë“œ:**
                - **PDF ì—…ë¡œë“œ:** ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•˜ê³  ì‹¶ì€ PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.
                - **ëª¨ë¸ ì„ íƒ:** ë¡œì»¬ `Ollama` ëª¨ë¸ ë˜ëŠ” `Gemini` API ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - **ì§ˆë¬¸í•˜ê¸°:** ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´, ë‚´ìš©ì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - **PDF ë·°ì–´:** ìš°ì¸¡ì—ì„œ ì›ë³¸ ë¬¸ì„œë¥¼ í•¨ê»˜ ë³´ë©° ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í•´ìƒë„/í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥)
                """
            )

            with st.expander("âš ï¸ ì•Œì•„ë‘ì‹¤ ì "):
                st.warning(
                    "**ë‹µë³€ì˜ ì •í™•ì„±:** ë‹µë³€ì€ ì—…ë¡œë“œëœ PDF ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ë˜ë©°, ì‚¬ì‹¤ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                st.warning(
                    "**ê°œì¸ì •ë³´:** Gemini ëª¨ë¸ ì‚¬ìš© ì‹œ, ì§ˆë¬¸ ë‚´ìš©ì´ Google ì„œë²„ë¡œ ì „ì†¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                st.warning(
                    "**ì´ˆê¸° ë¡œë”©:** ì„ë² ë”© ëª¨ë¸ì„ ì²˜ìŒ ì‚¬ìš©í•˜ë©´ ë‹¤ìš´ë¡œë“œì— ëª‡ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )