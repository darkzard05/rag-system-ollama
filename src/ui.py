"""
Streamlit UI ì»´í¬ë„ŒíŠ¸ ë Œë”ë§ í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ íŒŒì¼.
Clean & Minimal Version: ë¶€ê°€ ìš”ì†Œ ì œê±°, ì§ê´€ì ì¸ ë¡œë”© ë° ìŠ¤íŠ¸ë¦¬ë°.
"""

import time
import logging
from typing import Callable, Optional

import streamlit as st
from streamlit_pdf_viewer import pdf_viewer
import fitz  # PyMuPDF

from session import SessionManager
from model_loader import get_available_models
from utils import sync_run
from config import (
    AVAILABLE_EMBEDDING_MODELS,
    OLLAMA_MODEL_NAME,
    UI_CONTAINER_HEIGHT,
    MSG_SIDEBAR_TITLE,
    MSG_PDF_UPLOADER_LABEL,
    MSG_MODEL_SELECTOR_LABEL,
    MSG_EMBEDDING_SELECTOR_LABEL,
    MSG_SYSTEM_STATUS_TITLE,
    MSG_PDF_VIEWER_TITLE,
    MSG_PDF_VIEWER_NO_FILE,
    MSG_PDF_VIEWER_PREV_BUTTON,
    MSG_PDF_VIEWER_NEXT_BUTTON,
    MSG_PDF_VIEWER_ERROR,
    MSG_CHAT_TITLE,
    MSG_CHAT_INPUT_PLACEHOLDER,
    MSG_CHAT_NO_QA_SYSTEM,
    MSG_CHAT_WELCOME,
    MSG_ERROR_OLLAMA_NOT_RUNNING,
    MSG_PREPARING_ANSWER,
)

logger = logging.getLogger(__name__)


async def _stream_chat_response(qa_chain, user_input: str, chat_container) -> str:
    """
    ìµœì í™”ëœ ë‹µë³€ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜.
    [ìµœì í™”] ì¸ìœ„ì ì¸ ë”œë ˆì´(update_interval)ë¥¼ ì œê±°í•˜ì—¬ ë°˜ì‘ ì†ë„ ê·¹ëŒ€í™”.
    """
    full_response = ""
    start_time = time.time()
    
    current_llm = SessionManager.get("llm")
    if not current_llm:
        return "âŒ ì˜¤ë¥˜: LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    run_config = {"configurable": {"llm": current_llm}}
    SessionManager.set("is_generating_answer", True)

    try:
        with chat_container:
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                answer_container = st.empty()
                # ë¡œë”© ë©”ì‹œì§€ ë³µêµ¬ (ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜)
                answer_container.markdown(f"âŒ› {MSG_PREPARING_ANSWER}")

                # ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„
                async for event in qa_chain.astream_events(
                    {"input": user_input},
                    config=run_config,
                    version="v1"
                ):
                    kind = event["event"]
                    name = event.get("name", "Unknown")
                    data = event.get("data", {})
                    
                    # [Debug] ëª¨ë“  ì´ë²¤íŠ¸ ë¡œê¹… (ë¬¸ì œ í•´ê²° í›„ ì£¼ì„ ì²˜ë¦¬ ì˜ˆì •)
                    logger.debug(f"[Stream Event] Kind: {kind} | Name: {name} | Keys: {list(data.keys())}")

                    chunk_text = None

                    # 1. íŒŒì„œ ìŠ¤íŠ¸ë¦¼ (ê¸°ì¡´ ë¡œì§)
                    if kind == "on_parser_stream":
                        chunk = data.get("chunk")
                        if isinstance(chunk, str):
                            chunk_text = chunk
                    
                    # 2. ëª¨ë¸ ìŠ¤íŠ¸ë¦¼ (ì¶”ê°€: ë” ë‚®ì€ ë ˆë²¨ì˜ í† í° ì´ë²¤íŠ¸)
                    elif kind == "on_chat_model_stream":
                        chunk = data.get("chunk")
                        # AIMessageChunk ê°ì²´ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
                        if hasattr(chunk, "content"):
                            chunk_text = chunk.content
                        elif isinstance(chunk, dict) and "content" in chunk:
                            chunk_text = chunk["content"]
                        elif isinstance(chunk, str):
                            chunk_text = chunk
                    
                    # 3. ì»¤ìŠ¤í…€ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ (ê°•ì œ ìŠ¤íŠ¸ë¦¬ë° fallback)
                    elif kind == "on_custom_event" and name == "response_chunk":
                        chunk = data.get("chunk")
                        if isinstance(chunk, str):
                            chunk_text = chunk
                    
                    if chunk_text:
                        full_response += chunk_text
                        # [ìµœì í™”] ë”œë ˆì´ ì—†ì´ ì¦‰ì‹œ ë Œë”ë§
                        answer_container.markdown(full_response + "â–Œ")

                    # 3. ìµœì¢… ê²°ê³¼ ë³´ì •
                    if kind == "on_chain_end" and name == "generate_response":
                        output = data.get("output")
                        if isinstance(output, dict) and "response" in output:
                            final_node_res = output["response"]
                            # ìŠ¤íŠ¸ë¦¬ë°ëœ ê²ƒë³´ë‹¤ ìµœì¢… ê²°ê³¼ê°€ ê¸¸ë‹¤ë©´ êµì²´ (ëˆ„ë½ ë°©ì§€)
                            if len(final_node_res) > len(full_response):
                                full_response = final_node_res
                                logger.info("[Stream] ìµœì¢… ê²°ê³¼ë¡œ ì‘ë‹µ ë³´ì •ë¨")

                # ìµœì¢… ë Œë”ë§ (ì»¤ì„œ ì œê±°)
                elapsed_time = time.time() - start_time
                if full_response:
                    answer_container.markdown(full_response)
                    logger.info(f"[UI] ë‹µë³€ ìƒì„± ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ")
                else:
                    logger.warning(f"[UI] ë‹µë³€ ìƒì„± ì‹¤íŒ¨. ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                    answer_container.error("âš ï¸ ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        return full_response

    except Exception as e:
        logger.error(f"Streaming error: {e}", exc_info=True)
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    finally:
        SessionManager.set("is_generating_answer", False)


def render_sidebar(
    file_uploader_callback: Callable,
    model_selector_callback: Callable,
    embedding_selector_callback: Callable
):
    """
    ìµœì†Œí•œì˜ ì„¤ì •ë§Œ ë‚¨ê¸´ ì‚¬ì´ë“œë°”ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
    ê³µê°„ íš¨ìœ¨ì„ ìœ„í•´ êµ¬ë¶„ì„ ì„ ì œê±°í•˜ê³  Expanderë¥¼ í™œìš©í•©ë‹ˆë‹¤.

    Args:
        file_uploader_callback: íŒŒì¼ ì—…ë¡œë“œ ì‹œ ì‹¤í–‰ë  ì½œë°± í•¨ìˆ˜
        model_selector_callback: LLM ëª¨ë¸ ë³€ê²½ ì‹œ ì‹¤í–‰ë  ì½œë°± í•¨ìˆ˜
        embedding_selector_callback: ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ì‹œ ì‹¤í–‰ë  ì½œë°± í•¨ìˆ˜
    """
    with st.sidebar:
        st.header(MSG_SIDEBAR_TITLE)

        # ë‹µë³€ ìƒì„± ì¤‘ì¸ì§€ í™•ì¸ (ì‚¬ì´ë“œë°” ì „ì²´ ì ê¸ˆìš©)
        is_generating = SessionManager.get("is_generating_answer")
        
        # 1. íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ (ê°€ì¥ ì¤‘ìš”í•˜ë¯€ë¡œ ìƒì‹œ ë…¸ì¶œ)
        st.file_uploader(
            MSG_PDF_UPLOADER_LABEL, 
            type="pdf", 
            key="pdf_uploader", 
            on_change=file_uploader_callback,
            disabled=is_generating  # ìƒì„± ì¤‘ ì—…ë¡œë“œ ë°©ì§€
        )

        # 2. ëª¨ë¸ ì„¤ì • ì„¹ì…˜ (ì ‘ì´ì‹ìœ¼ë¡œ ê³µê°„ ì ˆì•½)
        # expanded=Falseë¡œ ì„¤ì •í•˜ì—¬ ê¸°ë³¸ì ìœ¼ë¡œëŠ” ìˆ¨ê¹€ ì²˜ë¦¬ (ì‚¬ìš©ì ê²½í—˜ì— ë”°ë¼ ë³€ê²½ ê°€ëŠ¥)
        with st.expander("âš™ï¸ ëª¨ë¸ ì„¤ì •", expanded=False):
            # ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            available_models = get_available_models()
            is_ollama_error = (
                bool(available_models) and 
                available_models[0] == MSG_ERROR_OLLAMA_NOT_RUNNING
            )
            actual_models = [] if is_ollama_error else [m for m in available_models if "---" not in m]
            
            last_model = SessionManager.get("last_selected_model")
            if not last_model or (actual_models and last_model not in actual_models):
                last_model = actual_models[0] if actual_models else OLLAMA_MODEL_NAME
                SessionManager.set("last_selected_model", last_model)

            try:
                idx = available_models.index(last_model)
            except ValueError:
                idx = 0

            # ê¸´ ëª¨ë¸ ì´ë¦„ì´ ì˜ë¦¬ëŠ” ë¬¸ì œë¥¼ ìœ„í•´ help íˆ´íŒ ì¶”ê°€
            st.selectbox(
                MSG_MODEL_SELECTOR_LABEL, 
                available_models, 
                index=idx, 
                key="model_selector", 
                on_change=model_selector_callback, 
                disabled=(is_ollama_error or is_generating), # ì—ëŸ¬ê±°ë‚˜ ìƒì„± ì¤‘ì´ë©´ ë¹„í™œì„±
                help="ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”."
            )

            # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
            last_emb = SessionManager.get("last_selected_embedding_model") or AVAILABLE_EMBEDDING_MODELS[0]
            try:
                emb_idx = AVAILABLE_EMBEDDING_MODELS.index(last_emb)
            except ValueError:
                emb_idx = 0
                
            st.selectbox(
                MSG_EMBEDDING_SELECTOR_LABEL, 
                AVAILABLE_EMBEDDING_MODELS, 
                index=emb_idx, 
                key="embedding_model_selector", 
                on_change=embedding_selector_callback,
                disabled=is_generating, # ìƒì„± ì¤‘ ë³€ê²½ ë°©ì§€
                help="ë¬¸ì„œ ê²€ìƒ‰ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤."
            )
        
        # 3. ì‹œìŠ¤í…œ ìƒíƒœ ì„¹ì…˜ (êµ¬ë¶„ì„  ì—†ì´ ì—¬ë°±ìœ¼ë¡œ ë¶„ë¦¬)
        st.markdown("#### " + MSG_SYSTEM_STATUS_TITLE)
        
        # ìƒíƒœ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ë¹ˆ ì»¨í…Œì´ë„ˆ ë°˜í™˜
        return st.container()


def render_pdf_viewer():
    _pdf_viewer_fragment()


@st.fragment
def _pdf_viewer_fragment():
    """PDF ë·°ì–´ (Fragment) - ê°œì„ ëœ ë„¤ë¹„ê²Œì´ì…˜"""
    st.subheader(MSG_PDF_VIEWER_TITLE)
    
    pdf_bytes = SessionManager.get("pdf_file_bytes")
    if not pdf_bytes:
        st.info(MSG_PDF_VIEWER_NO_FILE)
        return
    
    try:
        with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
            total_pages = len(doc)
            
            # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            if "current_page" not in st.session_state: 
                st.session_state.current_page = 1
            
            # í˜ì´ì§€ ë²”ìœ„ ë³´ì •
            if st.session_state.current_page > total_pages:
                st.session_state.current_page = 1
            if st.session_state.current_page < 1:
                st.session_state.current_page = 1

            # ë‹µë³€ ìƒì„± ì¤‘ì¸ì§€ í™•ì¸
            is_generating = SessionManager.get("is_generating_answer")

            # --- PDF ë·°ì–´ ë Œë”ë§ ---
            pdf_viewer(
                input=pdf_bytes, 
                height=UI_CONTAINER_HEIGHT, 
                pages_to_render=[st.session_state.current_page]
            )

            # --- í•˜ë‹¨ ë„¤ë¹„ê²Œì´ì…˜ ---
            # í—¬í¼ í•¨ìˆ˜: í˜ì´ì§€ ë³€ê²½ ì½œë°±
            def go_prev():
                if st.session_state.current_page > 1:
                    st.session_state.current_page -= 1
            
            def go_next():
                if st.session_state.current_page < total_pages:
                    st.session_state.current_page += 1

            # ë ˆì´ì•„ì›ƒ: [ì´ì „] [í˜ì´ì§€ ì…ë ¥ / ì´í˜ì´ì§€] [ë‹¤ìŒ]
            c1, c2, c3 = st.columns([1, 1, 1])
            
            with c1:
                # ì´ì „ í˜ì´ì§€ ë²„íŠ¼ (on_click ì‚¬ìš©)
                st.button(
                    MSG_PDF_VIEWER_PREV_BUTTON, 
                    key="btn_pdf_prev", 
                    use_container_width=True,
                    disabled=(st.session_state.current_page <= 1 or is_generating),
                    on_click=go_prev
                )

            with c2:
                # ê°€ìš´ë° ë³‘í•©ëœ 'í˜„ì¬ / ì´ page' ë ˆì´ì•„ì›ƒ
                p1, p2 = st.columns([1, 1])
                with p1:
                    def update_page_input():
                        # number_inputì˜ ê°’ì´ ë³€ê²½ë˜ë©´ session_stateì— ë°˜ì˜
                        # keyê°€ 'num_input_page'ì´ë¯€ë¡œ st.session_state.num_input_pageì— ê°’ì´ ìˆìŒ
                        st.session_state.current_page = int(st.session_state.num_input_page)

                    st.number_input(
                        "í˜ì´ì§€ ì´ë™", 
                        min_value=1, 
                        max_value=total_pages, 
                        value=st.session_state.current_page, 
                        label_visibility="collapsed",
                        key="num_input_page",
                        disabled=is_generating,
                        on_change=update_page_input
                    )
                with p2:
                    st.markdown(
                        f"<div style='line-height: 2.3em; font-size: 1.0em;'>"
                        f"&nbsp;/ {total_pages} pages</div>", 
                        unsafe_allow_html=True
                    )

            with c3:
                # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ (on_click ì‚¬ìš©)
                st.button(
                    MSG_PDF_VIEWER_NEXT_BUTTON, 
                    key="btn_pdf_next", 
                    use_container_width=True,
                    disabled=(st.session_state.current_page >= total_pages or is_generating),
                    on_click=go_next
                )
            
    except Exception as e:
        logger.error(f"PDF ë·°ì–´ ì˜¤ë¥˜: {e}", exc_info=True)
        st.error(f"PDF ì˜¤ë¥˜: {e}")


def render_left_column():
    _chat_fragment()


def render_message(role: str, content: str):
    """
    ë©”ì‹œì§€ë¥¼ ì—­í• ì— ë”°ë¼ ìŠ¤íƒ€ì¼ë§í•˜ì—¬ ë Œë”ë§í•©ë‹ˆë‹¤.
    Args:
        role: ë©”ì‹œì§€ ì‘ì„±ì ì—­í•  ('user' ë˜ëŠ” 'assistant')
        content: ë©”ì‹œì§€ ë³¸ë¬¸
    """
    # ì•„ë°”íƒ€ ì„¤ì •: ì‚¬ìš©ì(ğŸ‘¤), ì–´ì‹œìŠ¤í„´íŠ¸(ğŸ¤–)
    avatar_icon = "ğŸ¤–" if role == "assistant" else "ğŸ‘¤"
    
    with st.chat_message(role, avatar=avatar_icon):
        # ë‹µë³€ ë‚´ì˜ ì¶œì²˜(Sources) ë¶€ë¶„ì„ ì‹œê°ì ìœ¼ë¡œ ë¶„ë¦¬
        # ê°€ì •: LLM ë‹µë³€ì— 'ì¶œì²˜:' ë˜ëŠ” 'Sources:' ë¼ëŠ” êµ¬ë¶„ìê°€ ìˆë‹¤ê³  ê°€ì •
        # ì‹¤ì œ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ êµ¬ë¶„ìëŠ” ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
        
        # ê°„ë‹¨í•œ íŒŒì‹± ë¡œì§ (í•„ìš”ì‹œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ê³ ë„í™” ê°€ëŠ¥)
        separator = None
        if "ì¶œì²˜:" in content:
            separator = "ì¶œì²˜:"
        elif "Sources:" in content:
            separator = "Sources:"
            
        if role == "assistant" and separator:
            try:
                parts = content.split(separator, 1)
                main_content = parts[0].strip()
                sources = parts[1].strip()
                
                st.markdown(main_content)
                if sources:
                    with st.expander("ğŸ“š ì°¸ê³  ë¬¸í—Œ (Sources)", expanded=False):
                        st.markdown(f"**{separator}**\n{sources}")
            except Exception:
                # íŒŒì‹± ì—ëŸ¬ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ì¶œë ¥
                st.markdown(content)
        else:
            st.markdown(content)


@st.fragment
def _chat_fragment():
    """ì±„íŒ… êµ¬ì—­ (Fragment)"""
    st.subheader(MSG_CHAT_TITLE)
    chat_container = st.container(height=UI_CONTAINER_HEIGHT, border=True)

    messages = SessionManager.get_messages()
    for msg in messages:
        with chat_container:
            render_message(msg["role"], msg["content"])

    if not messages:
        with chat_container: 
            st.info(MSG_CHAT_WELCOME)

    is_gen = SessionManager.get("is_generating_answer")
    if user_input := st.chat_input(MSG_CHAT_INPUT_PLACEHOLDER, disabled=is_gen, key="chat_input_clean"):
        SessionManager.add_message("user", user_input)
        
        # ì¦‰ì‹œ ì‚¬ìš©ì ë©”ì‹œì§€ ë Œë”ë§
        with chat_container:
            render_message("user", user_input)

        qa_chain = SessionManager.get("qa_chain")
        if qa_chain:
            final_ans = sync_run(_stream_chat_response(qa_chain, user_input, chat_container))
            if final_ans and not final_ans.startswith("âŒ"):
                SessionManager.add_message("assistant", final_ans)
                st.rerun()
        else:
            st.toast(MSG_CHAT_NO_QA_SYSTEM, icon="âš ï¸")