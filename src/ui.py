"""
Streamlit UI ì»´í¬ë„ŒíŠ¸ ë Œë”ë§ í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ íŒŒì¼.
"""

import time
import logging
import asyncio
import streamlit as st
from streamlit_pdf_viewer import pdf_viewer
from streamlit_mermaid import st_mermaid
import fitz  # PyMuPDF

from session import SessionManager
from model_loader import get_available_models
from config import (
    AVAILABLE_EMBEDDING_MODELS,
    OLLAMA_MODEL_NAME,
    MSG_PREPARING_ANSWER,
    MSG_THINKING,
    MSG_NO_THOUGHT_PROCESS,
    UI_CONTAINER_HEIGHT,
)


async def _stream_chat_response(qa_chain, user_input, chat_container) -> tuple[str, str]:
    """
    ìŠ¤íŠ¸ë¦¼ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ íŒŒì‹±í•˜ì—¬ UIì— í‘œì‹œí•˜ê³ , ìµœì¢… (ìƒê°, ë‹µë³€) íŠœí”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìƒì„± ì‹œê°„ì„ ë¶„ë¦¬í•˜ì—¬ ì¸¡ì •í•˜ê³  ë¡œê·¸ë¥¼ ë‚¨ê¹ë‹ˆë‹¤.
    """
    full_response = ""
    
    start_time = time.time()
    thought_end_time = None
    is_thought_complete = False
    
    with chat_container, st.chat_message("assistant"):
        expander = st.expander("ìƒê° ê³¼ì • ë³´ê¸°", expanded=False)
        thought_container = expander.empty()
        answer_container = st.empty()

        # --- ğŸ’¡ 1. ìˆ˜ì •ëœ ë¶€ë¶„: ì´ˆê¸° ë©”ì‹œì§€ë¥¼ ë™ì¼í•˜ê²Œ ì„¤ì • ---
        # í™•ì¥ íŒ¨ë„ê³¼ ë©”ì¸ ë‹µë³€ ì°½ ëª¨ë‘ì— 'ìƒê° ì¤‘' ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
        thought_container.markdown(MSG_THINKING + "â–Œ")
        answer_container.markdown(MSG_PREPARING_ANSWER)

        try:
            async for event in qa_chain.astream_events(
                {"input": user_input}, version="v1"
            ):
                kind = event["event"]
                name = event.get("name", "")
                
                if kind == "on_chain_stream" and name == "generate_response":
                    chunk_data = event["data"].get("chunk")
                    
                    if isinstance(chunk_data, dict) and "response" in chunk_data:
                        full_response += chunk_data["response"]
                        
                        think_content = ""
                        answer_content = ""
                        
                        think_start_tag = "<think>"
                        think_end_tag = "</think>"
                        
                        start_index = full_response.find(think_start_tag)
                        end_index = full_response.find(think_end_tag)

                        if start_index != -1:
                            if end_index != -1:
                                if not is_thought_complete:
                                    thought_end_time = time.time()
                                    is_thought_complete = True
                                
                                think_content = full_response[start_index + len(think_start_tag):end_index].strip()
                                answer_content = full_response[end_index + len(think_end_tag):].strip()
                            else:
                                think_content = full_response[start_index + len(think_start_tag):].strip()
                        else:
                            answer_content = full_response.strip()

                        # --- ğŸ’¡ 2. ìˆ˜ì •ëœ ë¶€ë¶„: ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ ë³€ê²½ ---
                        if answer_content:
                            # ë‹µë³€ ë‚´ìš©ì´ ì‹œì‘ë˜ë©´,
                            # 1. ìƒê° ê³¼ì •ì€ ì™„ì„±ëœ ë‚´ìš©ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ê³ ,
                            # 2. ë‹µë³€ ì°½ì€ ê³ ì • ë©”ì‹œì§€ë¥¼ ë®ì–´ì“°ë©° ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°ì„ ì‹œì‘í•©ë‹ˆë‹¤.
                            thought_container.markdown(think_content)
                            answer_container.markdown(answer_content + "â–Œ")
                        else:
                            # ìƒê° ê³¼ì •ë§Œ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì¼ ë•Œ,
                            # 1. ìƒê° ê³¼ì • ì°½ë§Œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
                            # 2. ë‹µë³€ ì°½(answer_container)ì€ ê±´ë“œë¦¬ì§€ ì•Šì•„ ê³ ì • ë©”ì‹œì§€ê°€ ìœ ì§€ë©ë‹ˆë‹¤.
                            thought_container.markdown(think_content + "â–Œ")
                            # answer_container.empty()  <- ì´ ì¤„ì„ ì œê±°í•˜ì—¬ ê³ ì • ë©”ì‹œì§€ê°€ ì‚¬ë¼ì§€ì§€ ì•Šë„ë¡ í•¨

        except Exception as e:
            error_msg = f"ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logging.error(error_msg, exc_info=True)
            answer_container.error(error_msg)
            return "", f"âŒ {error_msg}"

    # --- ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ìµœì¢… ë‚´ìš© ì •ë¦¬ ë° ë¡œê·¸ ì¶œë ¥ ---
    end_time = time.time()
    final_think_content = ""
    final_answer_content = ""

    start_index = full_response.find("<think>")
    end_index = full_response.find("</think>")

    if start_index != -1 and end_index != -1:
        final_think_content = full_response[start_index + len("<think>"):end_index].strip()
        final_answer_content = full_response[end_index + len("</think>"):].strip()
    else:
        final_answer_content = full_response.replace("<think>", "").replace("</think>", "").strip()

    thought_container.markdown(final_think_content or MSG_NO_THOUGHT_PROCESS)
    answer_container.markdown(final_answer_content)
    
    # --- ğŸ’¡ ìˆ˜ì •ëœ ë¶€ë¶„: ë¡œê·¸ ì¶œë ¥ ë¡œì§ ì •ë¦¬ ğŸ’¡ ---
    # ìƒì„¸ ì„±ëŠ¥ ì •ë³´ë¥¼ ë‹´ì„ ë³€ìˆ˜ ì´ˆê¸°í™”
    perf_details = ""
    
    if final_think_content and thought_end_time:
        # ìƒê° ê³¼ì •ê³¼ ë‹µë³€ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš°
        thought_duration = thought_end_time - start_time
        answer_duration = end_time - thought_end_time
        perf_details = (
            f"ìƒê°: {thought_duration:.2f}ì´ˆ ({len(final_think_content)}ì), "
            f"ë‹µë³€: {answer_duration:.2f}ì´ˆ ({len(final_answer_content)}ì)"
        )
    elif final_answer_content:
        # ë‹µë³€ë§Œ ìˆëŠ” ê²½ìš°
        total_duration = end_time - start_time
        perf_details = f"ë‹µë³€: {total_duration:.2f}ì´ˆ ({len(final_answer_content)}ì)"

    # ìƒì„¸ ì„±ëŠ¥ ì •ë³´ê°€ ìˆì„ ê²½ìš°ì—ë§Œ ë¡œê·¸ ì¶œë ¥
    if perf_details:
        logging.info(f"  [ì„±ëŠ¥ ìƒì„¸] {perf_details}")
    
    return final_think_content, final_answer_content


def render_sidebar(
    file_uploader_callback,
    model_selector_callback,
    embedding_selector_callback
):
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        st.file_uploader(
            "PDF íŒŒì¼ ì—…ë¡œë“œ",
            type="pdf",
            key="pdf_uploader",
            on_change=file_uploader_callback,
        )
        with st.spinner("LLM ëª¨ë¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
            available_models = get_available_models()
            actual_models = [m for m in available_models if "---" not in m]
            last_model = SessionManager.get("last_selected_model")
            if not last_model or last_model not in actual_models:
                if actual_models:
                    last_model = actual_models[0]
                    SessionManager.set("last_selected_model", last_model)
                else:
                    last_model = OLLAMA_MODEL_NAME
            current_model_index = (
                available_models.index(last_model)
                if last_model in available_models
                else 0
            )
            st.selectbox(
                "LLM ëª¨ë¸ ì„ íƒ",
                available_models,
                index=current_model_index,
                key="model_selector",
                on_change=model_selector_callback,
            )
        last_embedding_model = (
            SessionManager.get("last_selected_embedding_model")
            or AVAILABLE_EMBEDDING_MODELS[0]
        )
        current_embedding_model_index = (
            AVAILABLE_EMBEDDING_MODELS.index(last_embedding_model)
            if last_embedding_model in AVAILABLE_EMBEDDING_MODELS
            else 0
        )
        st.selectbox(
            "ì„ë² ë”© ëª¨ë¸ ì„ íƒ",
            AVAILABLE_EMBEDDING_MODELS,
            index=current_embedding_model_index,
            key="embedding_model_selector",
            on_change=embedding_selector_callback,
        )
        st.divider()
        st.header("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
        status_container = st.container()
        return status_container


def render_pdf_viewer():
    st.subheader("ğŸ“„ PDF ë¯¸ë¦¬ë³´ê¸°")
    pdf_bytes = SessionManager.get("pdf_file_bytes")
    if not pdf_bytes:
        st.info("ë¯¸ë¦¬ë³¼ PDFê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    try:
        pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
        total_pages = len(pdf_document)
        current_file_name = SessionManager.get("last_uploaded_file_name")
        if "current_page" not in st.session_state:
            st.session_state.current_page = 1
        if st.session_state.get("last_pdf_name") != current_file_name:
            st.session_state.current_page = 1
            st.session_state.last_pdf_name = current_file_name
        if st.session_state.current_page > total_pages:
            st.session_state.current_page = 1
        def go_to_previous_page():
            if st.session_state.current_page > 1:
                st.session_state.current_page -= 1
        def go_to_next_page():
            if st.session_state.current_page < total_pages:
                st.session_state.current_page += 1
        pdf_viewer(
            input=pdf_bytes,
            height=UI_CONTAINER_HEIGHT,
            pages_to_render=[st.session_state.current_page],
        )
        nav_cols = st.columns([1, 2, 1])
        with nav_cols[0]:
            st.button(
                "â† ì´ì „",
                on_click=go_to_previous_page,
                use_container_width=True,
                disabled=(st.session_state.current_page <= 1),
            )
        with nav_cols[1]:
            def sync_slider_and_input():
                st.session_state.current_page = st.session_state.current_page_slider
            st.slider(
                "í˜ì´ì§€ ì´ë™",
                min_value=1,
                max_value=total_pages,
                key="current_page_slider",
                label_visibility="collapsed",
                value=st.session_state.current_page,
                on_change=sync_slider_and_input,
            )
        with nav_cols[2]:
            st.button(
                "ë‹¤ìŒ â†’",
                on_click=go_to_next_page,
                use_container_width=True,
                disabled=(st.session_state.current_page >= total_pages),
            )
    except Exception as e:
        st.error(f"PDFë¥¼ í‘œì‹œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        logging.error("PDF ë·°ì–´ ì˜¤ë¥˜", exc_info=True)


def render_chat_column():
    """ì±„íŒ… ì»¬ëŸ¼ì„ ë Œë”ë§í•˜ê³  ì±„íŒ… ë¡œì§ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    # st.subheader("ğŸ’¬ ì±„íŒ…")
    chat_container = st.container(height=UI_CONTAINER_HEIGHT, border=True)

    # --- ğŸ’¡ 2. ëŒ€í™” ê¸°ë¡ í‘œì‹œ ë¡œì§ (ìˆ˜ì •ë¨) ğŸ’¡ ---
    messages = SessionManager.get_messages()
    for message in messages:
        with chat_container, st.chat_message(message["role"]):
            # ğŸ’¡ 'thought'ê°€ ì €ì¥ë˜ì–´ ìˆìœ¼ë©´ expanderì™€ í•¨ê»˜ ë Œë”ë§
            if message["role"] == "assistant" and "thought" in message and message["thought"]:
                with st.expander("ìƒê° ê³¼ì • ë³´ê¸°", expanded=False):
                    st.markdown(message["thought"], unsafe_allow_html=True)
            
            # ğŸ’¡ contentëŠ” í•­ìƒ ë Œë”ë§
            st.markdown(message["content"], unsafe_allow_html=True)

    # --- ğŸ’¡ 1. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë¡œì§ (ì´ ë¶€ë¶„ì€ ë³€ê²½ ì—†ìŒ) ğŸ’¡ ---
    if user_input := st.chat_input(
        "PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.", disabled=not SessionManager.is_ready_for_chat()
    ):
        # ğŸ’¡ ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” 'thought' ì—†ì´ ì¶”ê°€
        SessionManager.add_message("user", user_input)
        st.rerun()

    # --- ğŸ’¡ AI ì‘ë‹µ ìƒì„± ë° ì €ì¥ ë¡œì§ (ìˆ˜ì •ë¨) ğŸ’¡ ---
    if messages and messages[-1]["role"] == "user":
        last_user_input = messages[-1]["content"]
        qa_chain = SessionManager.get("qa_chain")
        
        if qa_chain:
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜ëŠ” ì´ì œ (ìƒê°, ë‹µë³€) íŠœí”Œì„ ë°˜í™˜
            final_thought, final_answer = loop.run_until_complete(
                _stream_chat_response(qa_chain, last_user_input, chat_container)
            )
            
            # ğŸ’¡ ìˆ˜ì •ëœ add_messageë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒê°ê³¼ ë‹µë³€ì„ ëª¨ë‘ ì €ì¥
            if final_answer or final_thought:
                SessionManager.add_message(
                    "assistant", 
                    content=final_answer, 
                    thought=final_thought
                )
                st.rerun()
        else:
            st.error("QA ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")

    if not SessionManager.get_messages():
        with chat_container:
            st.info(
                "**RAG-Chatì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!**\n\n"
                "ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•œ ëŒ€í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”."
            )
            st.markdown(
                """
                **ğŸ’¡ ì‚¬ìš© ê°€ì´ë“œ**
                - **PDF ì—…ë¡œë“œ:** ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•˜ê³  ì‹¶ì€ PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.
                - **ëª¨ë¸ ì„ íƒ:** ë¡œì»¬ `Ollama` ëª¨ë¸ ë˜ëŠ” `Gemini` API ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - **ì§ˆë¬¸í•˜ê¸°:** ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´, ë‚´ìš©ì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - **PDF ë·°ì–´:** ìš°ì¸¡ì—ì„œ ì›ë³¸ ë¬¸ì„œë¥¼ í•¨ê»˜ ë³´ë©° ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                
                **âš ï¸ ì•Œì•„ë‘ì‹¤ ì **
                - **ë‹µë³€ì˜ ì •í™•ì„±:** ë‹µë³€ì€ ì—…ë¡œë“œëœ PDF ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ë˜ë©°, ì‚¬ì‹¤ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - **ê°œì¸ì •ë³´:** Gemini ëª¨ë¸ ì‚¬ìš© ì‹œ, ì§ˆë¬¸ ë‚´ìš©ì´ Google ì„œë²„ë¡œ ì „ì†¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - **ì´ˆê¸° ë¡œë”©:** ì„ë² ë”© ëª¨ë¸ì„ ì²˜ìŒ ì‚¬ìš©í•˜ë©´ ë‹¤ìš´ë¡œë“œì— ëª‡ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                """
            )

    if error_msg := SessionManager.get("pdf_processing_error"):
        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}")
        if st.button("ì¬ì‹œë„"):
            SessionManager.reset_all_state()
            st.rerun()

def render_workflow_tab_content():
    """'ì›Œí¬í”Œë¡œìš°' íƒ­ì— ë“¤ì–´ê°ˆ ì½˜í…ì¸ ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    qa_chain = SessionManager.get("rag_workflow_graph")
    
    if not qa_chain:
        st.info("ì›Œí¬í”Œë¡œìš°ë¥¼ ë³´ë ¤ë©´ ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ PDFë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    # 1. LangGraphì—ì„œ Mermaid êµ¬ë¬¸ ì¶”ì¶œ
    try:
        mermaid_syntax = qa_chain.get_graph().draw_mermaid()
    except Exception as e:
        st.error(f"ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return

    # 2. Mermaid ê·¸ë˜í”„ ë Œë”ë§
    st.subheader("ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨")
    st_mermaid(mermaid_syntax, height="350px") # ì»¬ëŸ¼ì— ë§ê²Œ ë†’ì´ ì‚´ì§ ì¡°ì ˆ

    # 3. ê° ë…¸ë“œì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… ì¶”ê°€
    st.subheader("ê° ë‹¨ê³„ ì„¤ëª…")
    
    node_descriptions = {
        "retrieve": "**1. ë¬¸ì„œ ê²€ìƒ‰ (Retrieve):** ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œ ì¡°ê°ë“¤ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.",
        "format_context": "**2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (Format Context):** ê²€ìƒ‰ëœ ë¬¸ì„œ ì¡°ê°ë“¤ì„ LLMì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.",
        "generate_response": "**3. ë‹µë³€ ìƒì„± (Generate Response):** ì •ë¦¬ëœ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
    }

    graph_nodes = qa_chain.get_graph().nodes
    for node_name in graph_nodes:
        if node_name in node_descriptions:
            st.markdown(node_descriptions[node_name])
        elif node_name != "__end__":
            st.markdown(f"- **{node_name}**: ì»¤ìŠ¤í…€ ë…¸ë“œ")

def render_left_column_with_tabs():
    """
    ì™¼ìª½ ì»¬ëŸ¼ì— 'ì±„íŒ…'ê³¼ 'ì›Œí¬í”Œë¡œìš°' íƒ­ì„ ìƒì„±í•˜ê³ ,
    ê° íƒ­ì— ë§ëŠ” ì½˜í…ì¸ ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
    """
    # 1. íƒ­ ìƒì„±
    tab_chat, tab_workflow = st.tabs(["ğŸ’¬ ì±„íŒ…", "ğŸ“Š ì›Œí¬í”Œë¡œìš°"])

    # 2. 'ì±„íŒ…' íƒ­ ì½˜í…ì¸  êµ¬ì„±
    with tab_chat:
        # ê¸°ì¡´ ì±„íŒ… ë Œë”ë§ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        # (ì´í›„ ë‹¨ê³„ì—ì„œ ì´ í•¨ìˆ˜ë¥¼ ì•½ê°„ ìˆ˜ì •í•  ê²ƒì…ë‹ˆë‹¤.)
        render_chat_column()

    # 3. 'ì›Œí¬í”Œë¡œìš°' íƒ­ ì½˜í…ì¸  êµ¬ì„±
    with tab_workflow:
        # ê·¸ë˜í”„ ë·° ë Œë”ë§ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        # (ì´í›„ ë‹¨ê³„ì—ì„œ ì´ í•¨ìˆ˜ë¥¼ ì•½ê°„ ìˆ˜ì •í•  ê²ƒì…ë‹ˆë‹¤.)
        render_workflow_tab_content() # ìƒˆ ì´ë¦„ìœ¼ë¡œ ë³€ê²½