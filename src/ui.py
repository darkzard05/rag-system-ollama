"""
Streamlit UI ì»´í¬ë„ŒíŠ¸ ë Œë”ë§ í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ íŒŒì¼.
Clean & Minimal Version: ë¶€ê°€ ìš”ì†Œ ì œê±°, ì§ê´€ì ì¸ ë¡œë”© ë° ìŠ¤íŠ¸ë¦¬ë°.
"""

import time
import logging
from typing import Callable, Optional

import streamlit as st
from streamlit_pdf_viewer import pdf_viewer
import fitz  # PyMuPDF

from session import SessionManager
from model_loader import get_available_models
from utils import sync_run, apply_tooltips_to_response
from config import (
    AVAILABLE_EMBEDDING_MODELS,
    OLLAMA_MODEL_NAME,
    UI_CONTAINER_HEIGHT,
    MSG_SIDEBAR_TITLE,
    MSG_PDF_UPLOADER_LABEL,
    MSG_MODEL_SELECTOR_LABEL,
    MSG_EMBEDDING_SELECTOR_LABEL,
    MSG_SYSTEM_STATUS_TITLE,
    MSG_PDF_VIEWER_TITLE,
    MSG_PDF_VIEWER_NO_FILE,
    MSG_PDF_VIEWER_PREV_BUTTON,
    MSG_PDF_VIEWER_NEXT_BUTTON,
    MSG_PDF_VIEWER_ERROR,
    MSG_CHAT_TITLE,
    MSG_CHAT_INPUT_PLACEHOLDER,
    MSG_CHAT_NO_QA_SYSTEM,
    MSG_CHAT_WELCOME,
    MSG_ERROR_OLLAMA_NOT_RUNNING,
    MSG_PREPARING_ANSWER,
)

logger = logging.getLogger(__name__)


async def _stream_chat_response(qa_chain, user_input: str, chat_container) -> str:
    """
    ìµœì í™”ëœ ë‹µë³€ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜.
    [ìµœì í™”] ì¸ìœ„ì ì¸ ë”œë ˆì´(update_interval)ë¥¼ ì œê±°í•˜ì—¬ ë°˜ì‘ ì†ë„ ê·¹ëŒ€í™”.
    [ê°œì„ ] ë¬¸ì„œ ì¶œì²˜ë¥¼ RAG ê²°ê³¼ì—ì„œ ì§ì ‘ ê°€ì ¸ì™€ êµ¬ì¡°ì ìœ¼ë¡œ ë Œë”ë§.
    """
    full_response = ""
    retrieved_documents = [] # ê²€ìƒ‰ëœ ë¬¸ì„œ ì €ì¥ìš©
    start_time = time.time()
    
    current_llm = SessionManager.get("llm")
    if not current_llm:
        return "âŒ ì˜¤ë¥˜: LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    run_config = {"configurable": {"llm": current_llm}}
    SessionManager.set("is_generating_answer", True)

    try:
        with chat_container:
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                answer_container = st.empty()
                # ë¡œë”© ë©”ì‹œì§€ ë³µêµ¬
                answer_container.markdown(f"âŒ› {MSG_PREPARING_ANSWER}")

                # ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„
                async for event in qa_chain.astream_events(
                    {"input": user_input},
                    config=run_config,
                    version="v1"
                ):
                    kind = event["event"]
                    name = event.get("name", "Unknown")
                    data = event.get("data", {})
                    
                    # [Debug]
                    # logger.debug(f"[Stream Event] Kind: {kind} | Name: {name}")

                    chunk_text = None

                    # 1. í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
                    if kind == "on_parser_stream":
                        chunk = data.get("chunk")
                        if isinstance(chunk, str):
                            chunk_text = chunk
                    
                    elif kind == "on_chat_model_stream":
                        chunk = data.get("chunk")
                        if hasattr(chunk, "content"):
                            chunk_text = chunk.content
                        elif isinstance(chunk, dict) and "content" in chunk:
                            chunk_text = chunk["content"]
                        elif isinstance(chunk, str):
                            chunk_text = chunk
                    
                    elif kind == "on_custom_event" and name == "response_chunk":
                        chunk = data.get("chunk")
                        if isinstance(chunk, str):
                            chunk_text = chunk
                    
                    if chunk_text:
                        full_response += chunk_text
                        answer_container.markdown(full_response + "â–Œ", unsafe_allow_html=True)

                    # 2. ë¬¸ì„œ ìº¡ì²˜ (retrieve ë…¸ë“œ ì™„ë£Œ ì‹œì )
                    if kind == "on_chain_end" and name == "retrieve":
                        output = data.get("output")
                        if output and isinstance(output, dict) and "documents" in output:
                            retrieved_documents = output["documents"]
                            logger.info(f"[UI] ê²€ìƒ‰ëœ ë¬¸ì„œ ìº¡ì²˜: {len(retrieved_documents)}ê°œ")

                    # 3. ìµœì¢… ê²°ê³¼ ë³´ì • (generate_response ë…¸ë“œ ì™„ë£Œ ì‹œì )
                    if kind == "on_chain_end" and name == "generate_response":
                        output = data.get("output")
                        if isinstance(output, dict):
                            # ë§Œì•½ ë…¸ë“œê°€ documentsë¥¼ ë°˜í™˜í•œë‹¤ë©´ ì—¬ê¸°ì„œë„ ìº¡ì²˜ ì‹œë„ (ì•ˆì „ë§)
                            if "documents" in output and not retrieved_documents:
                                retrieved_documents = output["documents"]
                            
                            if "response" in output:
                                final_node_res = output["response"]
                                if len(final_node_res) > len(full_response):
                                    full_response = final_node_res

                # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ì²˜ë¦¬
                elapsed_time = time.time() - start_time
                
                if full_response:
                    # [ë³€ê²½] í•˜ë‹¨ ëª©ë¡ ì¶”ê°€ ëŒ€ì‹  ë³¸ë¬¸ì— íˆ´íŒ ì ìš©
                    if retrieved_documents:
                        # íˆ´íŒì´ ì ìš©ëœ HTMLë¡œ ë³€í™˜
                        final_html = apply_tooltips_to_response(full_response, retrieved_documents)
                        answer_container.markdown(final_html, unsafe_allow_html=True)
                        
                        # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥ì„ ìœ„í•´ ì›ë³¸ í…ìŠ¤íŠ¸(full_response)ê°€ ì•„ë‹Œ 
                        # HTML ë²„ì „(final_html)ì„ ë°˜í™˜í•´ì•¼ ë‚˜ì¤‘ì—ë„ íˆ´íŒì´ ë³´ì„.
                        # ë‹¨, SessionManagerì—ëŠ” êµ¬ì¡°ì  ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ HTMLì„ ì €ì¥í•´ì•¼ í•¨.
                        full_response = final_html 
                    else:
                        # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ í…ìŠ¤íŠ¸ ë Œë”ë§
                        answer_container.markdown(full_response, unsafe_allow_html=True)

                    logger.info(f"[UI] ë‹µë³€ ìƒì„± ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ")
                else:
                    logger.warning(f"[UI] ë‹µë³€ ìƒì„± ì‹¤íŒ¨. ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                    answer_container.error("âš ï¸ ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        return full_response

    except Exception as e:
        logger.error(f"Streaming error: {e}", exc_info=True)
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    finally:
        SessionManager.set("is_generating_answer", False)


def render_sidebar(
    file_uploader_callback: Callable,
    model_selector_callback: Callable,
    embedding_selector_callback: Callable
):
    """
    ìµœì†Œí•œì˜ ì„¤ì •ë§Œ ë‚¨ê¸´ ì‚¬ì´ë“œë°”ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
    ê³µê°„ íš¨ìœ¨ì„ ìœ„í•´ êµ¬ë¶„ì„ ì„ ì œê±°í•˜ê³  Expanderë¥¼ í™œìš©í•©ë‹ˆë‹¤.

    Args:
        file_uploader_callback: íŒŒì¼ ì—…ë¡œë“œ ì‹œ ì‹¤í–‰ë  ì½œë°± í•¨ìˆ˜
        model_selector_callback: LLM ëª¨ë¸ ë³€ê²½ ì‹œ ì‹¤í–‰ë  ì½œë°± í•¨ìˆ˜
        embedding_selector_callback: ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ì‹œ ì‹¤í–‰ë  ì½œë°± í•¨ìˆ˜
    """
    with st.sidebar:
        st.header(MSG_SIDEBAR_TITLE)

        # ë‹µë³€ ìƒì„± ì¤‘ì¸ì§€ í™•ì¸ (ì‚¬ì´ë“œë°” ì „ì²´ ì ê¸ˆìš©)
        is_generating = SessionManager.get("is_generating_answer")
        
        # 1. íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ (ê°€ì¥ ì¤‘ìš”í•˜ë¯€ë¡œ ìƒì‹œ ë…¸ì¶œ)
        st.file_uploader(
            MSG_PDF_UPLOADER_LABEL, 
            type="pdf", 
            key="pdf_uploader", 
            on_change=file_uploader_callback,
            disabled=is_generating  # ìƒì„± ì¤‘ ì—…ë¡œë“œ ë°©ì§€
        )

        # 2. ëª¨ë¸ ì„¤ì • ì„¹ì…˜ (ì ‘ì´ì‹ìœ¼ë¡œ ê³µê°„ ì ˆì•½)
        # expanded=Falseë¡œ ì„¤ì •í•˜ì—¬ ê¸°ë³¸ì ìœ¼ë¡œëŠ” ìˆ¨ê¹€ ì²˜ë¦¬ (ì‚¬ìš©ì ê²½í—˜ì— ë”°ë¼ ë³€ê²½ ê°€ëŠ¥)
        with st.expander("âš™ï¸ ëª¨ë¸ ì„¤ì •", expanded=False):
            # ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            available_models = get_available_models()
            is_ollama_error = (
                bool(available_models) and 
                available_models[0] == MSG_ERROR_OLLAMA_NOT_RUNNING
            )
            actual_models = [] if is_ollama_error else [m for m in available_models if "---" not in m]
            
            last_model = SessionManager.get("last_selected_model")
            if not last_model or (actual_models and last_model not in actual_models):
                last_model = actual_models[0] if actual_models else OLLAMA_MODEL_NAME
                SessionManager.set("last_selected_model", last_model)

            try:
                idx = available_models.index(last_model)
            except ValueError:
                idx = 0

            # ê¸´ ëª¨ë¸ ì´ë¦„ì´ ì˜ë¦¬ëŠ” ë¬¸ì œë¥¼ ìœ„í•´ help íˆ´íŒ ì¶”ê°€
            st.selectbox(
                MSG_MODEL_SELECTOR_LABEL, 
                available_models, 
                index=idx, 
                key="model_selector", 
                on_change=model_selector_callback, 
                disabled=(is_ollama_error or is_generating), # ì—ëŸ¬ê±°ë‚˜ ìƒì„± ì¤‘ì´ë©´ ë¹„í™œì„±
                help="ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”."
            )

            # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
            last_emb = SessionManager.get("last_selected_embedding_model") or AVAILABLE_EMBEDDING_MODELS[0]
            try:
                emb_idx = AVAILABLE_EMBEDDING_MODELS.index(last_emb)
            except ValueError:
                emb_idx = 0
                
            st.selectbox(
                MSG_EMBEDDING_SELECTOR_LABEL, 
                AVAILABLE_EMBEDDING_MODELS, 
                index=emb_idx, 
                key="embedding_model_selector", 
                on_change=embedding_selector_callback,
                disabled=is_generating, # ìƒì„± ì¤‘ ë³€ê²½ ë°©ì§€
                help="ë¬¸ì„œ ê²€ìƒ‰ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤."
            )
        
        # 3. ì‹œìŠ¤í…œ ìƒíƒœ ì„¹ì…˜ (êµ¬ë¶„ì„  ì—†ì´ ì—¬ë°±ìœ¼ë¡œ ë¶„ë¦¬)
        st.markdown("#### " + MSG_SYSTEM_STATUS_TITLE)
        
        # ìƒíƒœ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ë¹ˆ ì»¨í…Œì´ë„ˆ ë°˜í™˜
        return st.container()


def render_pdf_viewer():
    _pdf_viewer_fragment()


@st.fragment
def _pdf_viewer_fragment():
    """PDF ë·°ì–´ (Fragment) - ê°œì„ ëœ ë„¤ë¹„ê²Œì´ì…˜"""
    st.subheader(MSG_PDF_VIEWER_TITLE)
    
    pdf_bytes = SessionManager.get("pdf_file_bytes")
    if not pdf_bytes:
        st.info(MSG_PDF_VIEWER_NO_FILE)
        return
    
    try:
        with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
            total_pages = len(doc)
            
            # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            if "current_page" not in st.session_state: 
                st.session_state.current_page = 1
            
            # í˜ì´ì§€ ë²”ìœ„ ë³´ì •
            if st.session_state.current_page > total_pages:
                st.session_state.current_page = 1
            if st.session_state.current_page < 1:
                st.session_state.current_page = 1

            # ë‹µë³€ ìƒì„± ì¤‘ì¸ì§€ í™•ì¸
            is_generating = SessionManager.get("is_generating_answer")

            # --- PDF ë·°ì–´ ë Œë”ë§ ---
            pdf_viewer(
                input=pdf_bytes, 
                height=UI_CONTAINER_HEIGHT, 
                pages_to_render=[st.session_state.current_page]
            )

            # --- í•˜ë‹¨ ë„¤ë¹„ê²Œì´ì…˜ ---
            # í—¬í¼ í•¨ìˆ˜: í˜ì´ì§€ ë³€ê²½ ì½œë°±
            def go_prev():
                if st.session_state.current_page > 1:
                    st.session_state.current_page -= 1
            
            def go_next():
                if st.session_state.current_page < total_pages:
                    st.session_state.current_page += 1

            # ë ˆì´ì•„ì›ƒ: [ì´ì „] [í˜ì´ì§€ ì…ë ¥ / ì´í˜ì´ì§€] [ë‹¤ìŒ]
            c1, c2, c3 = st.columns([1, 1, 1])
            
            with c1:
                # ì´ì „ í˜ì´ì§€ ë²„íŠ¼ (on_click ì‚¬ìš©)
                st.button(
                    MSG_PDF_VIEWER_PREV_BUTTON, 
                    key="btn_pdf_prev", 
                    use_container_width=True,
                    disabled=(st.session_state.current_page <= 1 or is_generating),
                    on_click=go_prev
                )

            with c2:
                # ê°€ìš´ë° ë³‘í•©ëœ 'í˜„ì¬ / ì´ page' ë ˆì´ì•„ì›ƒ
                p1, p2 = st.columns([1, 1])
                with p1:
                    def update_page_input():
                        # number_inputì˜ ê°’ì´ ë³€ê²½ë˜ë©´ session_stateì— ë°˜ì˜
                        # keyê°€ 'num_input_page'ì´ë¯€ë¡œ st.session_state.num_input_pageì— ê°’ì´ ìˆìŒ
                        st.session_state.current_page = int(st.session_state.num_input_page)

                    st.number_input(
                        "í˜ì´ì§€ ì´ë™", 
                        min_value=1, 
                        max_value=total_pages, 
                        value=st.session_state.current_page, 
                        label_visibility="collapsed",
                        key="num_input_page",
                        disabled=is_generating,
                        on_change=update_page_input
                    )
                with p2:
                    st.markdown(
                        f"<div style='line-height: 2.3em; font-size: 1.0em;'>"
                        f"&nbsp;/ {total_pages} pages</div>", 
                        unsafe_allow_html=True
                    )

            with c3:
                # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ (on_click ì‚¬ìš©)
                st.button(
                    MSG_PDF_VIEWER_NEXT_BUTTON, 
                    key="btn_pdf_next", 
                    use_container_width=True,
                    disabled=(st.session_state.current_page >= total_pages or is_generating),
                    on_click=go_next
                )
            
    except Exception as e:
        logger.error(f"PDF ë·°ì–´ ì˜¤ë¥˜: {e}", exc_info=True)
        st.error(f"PDF ì˜¤ë¥˜: {e}")


def render_left_column():
    # [íˆ´íŒ CSS ì£¼ì…]
    st.markdown("""
    <style>
    /* íˆ´íŒ ì»¨í…Œì´ë„ˆ */
    .tooltip {
        position: relative;
        display: inline-block;
        border-bottom: 1px dotted #888; /* ì¸ìš©êµ¬ì„ì„ í‘œì‹œí•˜ëŠ” ì ì„  ë°‘ì¤„ */
        cursor: help;
        color: #0068c9; /* ë§í¬ ìƒ‰ìƒê³¼ ìœ ì‚¬í•˜ê²Œ */
        font-weight: bold;
    }

    /* íˆ´íŒ í…ìŠ¤íŠ¸ (ìˆ¨ê¹€ ìƒíƒœ) */
    .tooltip .tooltip-text {
        visibility: hidden;
        width: 350px;
        background-color: #333; /* ë‹¤í¬ ê·¸ë ˆì´ ë°°ê²½ */
        color: #fff;
        text-align: left;
        border-radius: 6px;
        padding: 10px;
        font-size: 0.9em;
        font-weight: normal;
        line-height: 1.5;
        
        /* ìœ„ì¹˜ ì¡°ì • */
        position: absolute;
        z-index: 1000;
        bottom: 125%; /* í…ìŠ¤íŠ¸ ìœ„ìª½ì— í‘œì‹œ */
        left: 50%;
        margin-left: -175px; /* ê°€ìš´ë° ì •ë ¬ */
        
        /* í˜ì´ë“œì¸ íš¨ê³¼ */
        opacity: 0;
        transition: opacity 0.3s;
        
        /* ìŠ¤í¬ë¡¤ ë° í¬ê¸° ì œí•œ */
        max-height: 200px;
        overflow-y: auto;
        box-shadow: 0px 4px 8px rgba(0,0,0,0.3);
    }

    /* í™”ì‚´í‘œ */
    .tooltip .tooltip-text::after {
        content: "";
        position: absolute;
        top: 100%;
        left: 50%;
        margin-left: -5px;
        border-width: 5px;
        border-style: solid;
        border-color: #333 transparent transparent transparent;
    }

    /* í˜¸ë²„ ì‹œ í‘œì‹œ */
    .tooltip:hover .tooltip-text {
        visibility: visible;
        opacity: 1;
    }
    
    /* ë‹¤í¬ëª¨ë“œ ëŒ€ì‘: ê¸€ììƒ‰ì„ ëª…í™•íˆ */
    @media (prefers-color-scheme: dark) {
        .tooltip { color: #4fa8ff; }
    }
    </style>
    """, unsafe_allow_html=True)
    
    _chat_fragment()


def render_message(role: str, content: str):
    """
    ë©”ì‹œì§€ë¥¼ ì—­í• ì— ë”°ë¼ ìŠ¤íƒ€ì¼ë§í•˜ì—¬ ë Œë”ë§í•©ë‹ˆë‹¤.
    Args:
        role: ë©”ì‹œì§€ ì‘ì„±ì ì—­í•  ('user' ë˜ëŠ” 'assistant')
        content: ë©”ì‹œì§€ ë³¸ë¬¸
    """
    # ì•„ë°”íƒ€ ì„¤ì •: ì‚¬ìš©ì(ğŸ‘¤), ì–´ì‹œìŠ¤í„´íŠ¸(ğŸ¤–)
    avatar_icon = "ğŸ¤–" if role == "assistant" else "ğŸ‘¤"
    
    with st.chat_message(role, avatar=avatar_icon):
        # [ê°œì„ ] íŒŒì‹± ë¡œì§ ì œê±° - contentì— ì´ë¯¸ í¬ë§·íŒ…ëœ ì¶œì²˜ê°€ í¬í•¨ë˜ì–´ ìˆìŒ
        # HTML íƒœê·¸(ì ‘ì´ì‹ ì¶œì²˜) ë Œë”ë§ì„ ìœ„í•´ unsafe_allow_html=True ì„¤ì •
        st.markdown(content, unsafe_allow_html=True)


@st.fragment
def _chat_fragment():
    """ì±„íŒ… êµ¬ì—­ (Fragment)"""
    st.subheader(MSG_CHAT_TITLE)
    chat_container = st.container(height=UI_CONTAINER_HEIGHT, border=True)

    messages = SessionManager.get_messages()
    for msg in messages:
        with chat_container:
            render_message(msg["role"], msg["content"])

    if not messages:
        with chat_container: 
            st.info(MSG_CHAT_WELCOME)

    is_gen = SessionManager.get("is_generating_answer")
    if user_input := st.chat_input(MSG_CHAT_INPUT_PLACEHOLDER, disabled=is_gen, key="chat_input_clean"):
        SessionManager.add_message("user", user_input)
        
        # ì¦‰ì‹œ ì‚¬ìš©ì ë©”ì‹œì§€ ë Œë”ë§
        with chat_container:
            render_message("user", user_input)

        qa_chain = SessionManager.get("qa_chain")
        if qa_chain:
            final_ans = sync_run(_stream_chat_response(qa_chain, user_input, chat_container))
            if final_ans and not final_ans.startswith("âŒ"):
                SessionManager.add_message("assistant", final_ans)
                st.rerun()
        else:
            st.toast(MSG_CHAT_NO_QA_SYSTEM, icon="âš ï¸")