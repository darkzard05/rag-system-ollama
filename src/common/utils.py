"""
í”„ë¡œì íŠ¸ ì „ë°˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ íŒŒì¼.
Utils Rebuild: ë³µì¡í•œ ë°ì½”ë ˆì´í„° ì œê±° ë° ë¹„ë™ê¸° í—¬í¼ ë‹¨ìˆœí™”.
"""

import asyncio
import functools
import logging
import os
import re
import time

logger = logging.getLogger(__name__)

# --- ì‚¬ì „ ì»´íŒŒì¼ëœ ì •ê·œí‘œí˜„ì‹ (ì„±ëŠ¥ ìµœì í™”) ---
_RE_LATEX_BLOCK = re.compile(r"\\\[(.*?)\\\]", re.DOTALL)
_RE_LATEX_INLINE = re.compile(r"\\\((.*?)\\\)", re.DOTALL)

# [ìˆ˜ì •] ë³µí•© ì¸ìš© íŒ¨í„´ ì§€ì›: [p.1, p.2] ë˜ëŠ” [p.1, 2, 3] ë˜ëŠ” [page 5] ë“± ì§€ì›
_RE_CITATION_BLOCK = re.compile(
    r"([\[\(])((?:[Pp](?:age)?\.?\s*)?\d+(?:[\s,]*)(?:(?:[Pp](?:age)?\.?\s*)?\d+(?:[\s,]*))*)([\]\)])",
    re.IGNORECASE,
)
_RE_EXTRACT_PAGES = re.compile(r"(\d+)")
_RE_WHITESPACE = re.compile(r"\s+")
_RE_CLEAN_LIST_NUM = re.compile(r"^\d+[\.\)]\s*")
_RE_CLEAN_LIST_BULLET = re.compile(r"^[\-\*â€¢]\s*")

# [ìˆ˜ì •] ì •ê·œì‹ ì™„í™”:
# 1. ^\d+[\.\)\s]+ : ë¬¸ë‘ì˜ ìˆ«ìì™€ ì /ê´„í˜¸ (ì˜ˆ: "1. ", "1) ")
# 2. ^\s*[\-\*\u2022]\s* : ë¬¸ë‘ì˜ ë¶ˆë › í¬ì¸íŠ¸ (ì˜ˆ: "- ", "* ")
# 3. ^["']+|["']+$ : ë¬¸ë‘/ë¬¸ë¯¸ì˜ ë”°ì˜´í‘œ
# 4. (?:^Example:|^Query:)\s* : "Example:" ê°™ì€ ì ‘ë‘ì‚¬ ì œê±°
_RE_QUERY_CLEAN_PREFIX = re.compile(
    r"^(?:\d+[\.\)\s]+|\s*[\-\*\u2022]\s*|(?:Example|Query|Question):\s*)+",
    re.IGNORECASE,
)
_RE_QUERY_CLEAN_QUOTES = re.compile(r'^["\']+|["\']+$')


def normalize_latex_delimiters(text: str) -> str:
    r"""
    LLMì´ ì¶œë ¥í•˜ëŠ” ë‹¤ì–‘í•œ LaTeX ìˆ˜ì‹ êµ¬ë¶„ìë¥¼ Streamlit í‘œì¤€($ ë˜ëŠ” $$)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - \( ... \) -> $ ... $ (ì¸ë¼ì¸)
    - \[ ... \] -> $$ ... $$ (ë¸”ë¡)
    - ê¸°í˜¸ ì•ë’¤ì˜ ë¶ˆí•„ìš”í•œ ì´ìŠ¤ì¼€ì´í”„ ì œê±°
    """
    if not text:
        return text

    # 1. ë¸”ë¡ ìˆ˜ì‹ ë³€í™˜: \[ ... \] -> $$ ... $$
    text = _RE_LATEX_BLOCK.sub(r"$$\1$$", text)

    # 2. ì¸ë¼ì¸ ìˆ˜ì‹ ë³€í™˜: \( ... \) -> $ ... $
    text = _RE_LATEX_INLINE.sub(r"$\1$", text)

    # 3. ì˜ëª»ëœ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì •ì œ (ì˜ˆ: \$ -> $)
    # ë‹¨, ì½”ë“œ ë¸”ë¡ ë‚´ì˜ ê¸°í˜¸ëŠ” ê±´ë“œë¦¬ì§€ ì•Šë„ë¡ ì£¼ì˜ê°€ í•„ìš”í•˜ë‚˜ ì¼ë°˜ ë‹µë³€ ê¸°ì¤€ ì²˜ë¦¬
    text = text.replace(r"\$", "$")

    return text


def apply_tooltips_to_response(response_text: str, documents: list) -> str:
    """
    [ìµœì í™”] LaTeX ì •ê·œí™”ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤. (íˆ´íŒ ë¡œì§ì€ UI ê³„ì¸µìœ¼ë¡œ ì´ë™ë¨)
    """
    if not response_text:
        return response_text

    return normalize_latex_delimiters(response_text)


# --- ì „ì²˜ë¦¬ìš© ê³ ì† í…Œì´ë¸” ---
# ë„ ë¬¸ì ë“± ì œì–´ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ëŠ” í…Œì´ë¸”
_CLEAN_TRANS_TABLE = str.maketrans({"\x00": " ", "\r": " ", "\n": " ", "\t": " "})


def preprocess_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ì œ: ì œì–´ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ê³  ì—°ì† ê³µë°±ì„ ê³ ì† ì •ê·œí™”"""
    if not text:
        return ""

    # 1. str.translateë¥¼ ì´ìš©í•œ ê³ ì† ë¬¸ì ì¹˜í™˜ (ë£¨í”„ë³´ë‹¤ ìˆ˜ì‹­ ë°° ë¹ ë¦„)
    text = text.translate(_CLEAN_TRANS_TABLE)

    # 2. ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ í†µí•©
    # [ìµœì í™”] ì´ë¯¸ translateë¡œ ì¤„ë°”ê¿ˆ ë“±ì´ ê³µë°±ì´ ë˜ì—ˆìœ¼ë¯€ë¡œ ë‹¨ìˆœ ê³µë°±ë§Œ ì²˜ë¦¬
    if "  " in text:
        text = _RE_WHITESPACE.sub(" ", text)

    return text.strip()


def clean_query_text(query: str) -> str:
    """ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê¸°í˜¸, ë²ˆí˜¸, ì ‘ë‘ì‚¬(Example:, Question: ë“±) ì œê±°"""
    if not query:
        return ""

    # 1. ë¬¸ë‘ì˜ ìˆ«ì, ë¶ˆë ›, ì ‘ë‘ì‚¬(Example:, Query: ë“±) ì¼ê´„ ì œê±°
    query = _RE_QUERY_CLEAN_PREFIX.sub("", query.strip())

    # 2. ë¬¸ë‘/ë¬¸ë¯¸ ë”°ì˜´í‘œ ì œê±°
    query = _RE_QUERY_CLEAN_QUOTES.sub("", query.strip())

    return query.strip()


import streamlit as st  # noqa: E402


@st.cache_data(ttl=5)  # 5ì´ˆ ë™ì•ˆ ë¦¬ì†ŒìŠ¤ ì •ë³´ ìºì‹±
def get_ollama_resource_usage(model_name: str) -> str:
    """
    Ollama APIë¥¼ í†µí•´ íŠ¹ì • ëª¨ë¸ì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ìƒíƒœ(GPU/CPU)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        import requests

        from common.config import OLLAMA_BASE_URL

        # Ollama ps API í˜¸ì¶œ
        response = requests.get(f"{OLLAMA_BASE_URL}/api/ps", timeout=2.0)
        if response.status_code == 200:
            data = response.json()
            models = data.get("models", [])

            for m in models:
                if model_name in m.get("name", ""):
                    size_vram = m.get("size_vram", 0)
                    size = m.get("size", 1)

                    # VRAM ì‚¬ìš© ë¹„ìœ¨ ê³„ì‚°
                    vram_ratio = (size_vram / size) * 100
                    if vram_ratio >= 90:
                        return f"GPU (VRAM {vram_ratio:.1f}%)"
                    elif vram_ratio > 0:
                        return f"Hybrid (VRAM {vram_ratio:.1f}%, CPU {100 - vram_ratio:.1f}%)"
                    else:
                        return "CPU (0% VRAM)"

            return "Unknown (Not running)"
        return "Unknown (API Error)"
    except Exception:
        return "Unknown (Connection Error)"


def format_error_message(e: Exception) -> str:
    """
    ë°œìƒí•œ ì˜ˆì™¸ ê°ì²´ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ì¹œì ˆí•œ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    from common.exceptions import (
        EmbeddingModelError,
        EmptyPDFError,
        InsufficientChunksError,
        LLMInferenceError,
    )

    err_type = type(e).__name__
    msg = str(e)

    # 1. ì»¤ìŠ¤í…€ ë„ë©”ì¸ ì˜ˆì™¸ ì²˜ë¦¬
    if isinstance(e, EmptyPDFError):
        return "ğŸ“„ PDF íŒŒì¼ì— í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ì´ë¯¸ì§€ë¡œë§Œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•´ ë³´ì„¸ìš”."
    elif isinstance(e, InsufficientChunksError):
        return "âš ï¸ ë¬¸ì„œì˜ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    elif isinstance(e, LLMInferenceError):
        return f"ğŸ¤– ì¶”ë¡  ëª¨ë¸ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {msg}"
    elif isinstance(e, EmbeddingModelError):
        return "ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìì›(VRAM/RAM)ì´ ë¶€ì¡±í•œì§€ í™•ì¸í•´ ì£¼ì„¸ìš”."

    # 2. ì¼ë°˜ ì‹œìŠ¤í…œ ì˜ˆì™¸ ì²˜ë¦¬
    if "ConnectionError" in err_type or "11434" in msg:
        return (
            "ğŸ”Œ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”."
        )
    elif "timeout" in msg.lower():
        return (
            "âŒ› ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        )
    elif "out of memory" in msg.lower() or "CUDA" in msg:
        return "ğŸš€ GPU ë©”ëª¨ë¦¬(VRAM)ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ê±°ë‚˜ ëª¨ë¸ì„ ì‘ì€ ê²ƒìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”."

    # 3. ê¸°ë³¸ê°’
    return f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ ({err_type}): {msg}"


import hashlib


def fast_hash(text: str, length: int = 16) -> str:
    """
    ë³´ì•ˆì´ í•„ìš” ì—†ëŠ” ë‹¨ìˆœ ì‹ë³„ìš© ê³ ì† í•´ì‹œ í•¨ìˆ˜.
    SHA256ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥¸ MD5ë¥¼ ì‚¬ìš©í•˜ê³  ê²°ê³¼ ê¸¸ì´ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.
    """
    if not text:
        return "0" * length
    # usedforsecurity=False: ë³´ì•ˆ ì§„ë‹¨ ë„êµ¬(Bandit ë“±)ì— ì´ í•´ì‹œê°€
    # ì•”í˜¸í™”ë‚˜ ë³´ì•ˆ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒì„ ì•Œë¦½ë‹ˆë‹¤.
    return hashlib.md5(text.encode(errors="ignore"), usedforsecurity=False).hexdigest()[
        :length
    ]


def count_tokens_rough(text: str) -> int:
    """
    í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ëŒ€ëµì ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    - ì˜ì–´: ì•½ 4ê¸€ìë‹¹ 1í† í°
    - í•œê¸€/íŠ¹ìˆ˜ë¬¸ì: ì•½ 1~2ê¸€ìë‹¹ 1í† í°
    ë³´ìˆ˜ì ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•´ (ê¸€ì ìˆ˜ / 2.5)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    if not text:
        return 0
    return int(len(text) / 2.5) + 1


@functools.lru_cache(maxsize=4)
def _get_cached_pdf_bytes(pdf_path: str) -> bytes | None:
    """PDF íŒŒì¼ ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì— ìºì‹±í•©ë‹ˆë‹¤. (I/O ì ˆê°)"""
    if os.path.exists(pdf_path):
        with open(pdf_path, "rb") as f:
            return f.read()
    return None


def _merge_rects(rects: list, threshold: float = 5.0) -> list:
    """
    ì¸ì ‘í•˜ê±°ë‚˜ ê²¹ì¹˜ëŠ” ì‚¬ê°í˜•ë“¤ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.
    [ìµœì í™”] ë‹¤ë‹¨ ë ˆì´ì•„ì›ƒ ë° í–‰ ê°„ê²©ì„ ê³ ë ¤í•œ ì •ë°€ ë³‘í•©.
    """
    if not rects:
        return []

    # 1. Y ì¢Œí‘œ ë° X ì¢Œí‘œ ê¸°ì¤€ ì •ë ¬
    sorted_rects = sorted(rects, key=lambda r: (r.y0, r.x0))
    merged = []

    if not sorted_rects:
        return []

    current_group = [sorted_rects[0]]

    for next_rect in sorted_rects[1:]:
        last = current_group[-1]

        # ìˆ˜ì§ ê²¹ì¹¨ ì •ë„ í™•ì¸ (í–‰ íŒë‹¨)
        y_overlap = max(0, min(last.y1, next_rect.y1) - max(last.y0, next_rect.y0))
        is_same_line = y_overlap > min(last.height, next_rect.height) * 0.6

        # ìˆ˜í‰ ê±°ë¦¬ í™•ì¸ (ë‹¨ì–´ ê°„ê²© íŒë‹¨)
        x_gap = next_rect.x0 - last.x1

        if is_same_line and x_gap < threshold * 10:  # ê°™ì€ í–‰ & ì¸ì ‘
            current_group.append(next_rect)
        else:
            # ê·¸ë£¹ ë³‘í•© ë° ìƒˆ ê·¸ë£¹ ì‹œì‘
            union_rect = current_group[0]
            for r in current_group[1:]:
                union_rect = union_rect | r
            merged.append(union_rect)
            current_group = [next_rect]

    # ë§ˆì§€ë§‰ ê·¸ë£¹ ì²˜ë¦¬
    if current_group:
        union_rect = current_group[0]
        for r in current_group[1:]:
            union_rect = union_rect | r
        merged.append(union_rect)

    return merged


def get_pdf_annotations(
    pdf_path: str, documents: list, color: str = "red"
) -> list[dict]:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œ ì¡°ê°ë“¤ì˜ í…ìŠ¤íŠ¸ ì¢Œí‘œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    [ê³ ë„í™”] ìŠ¤ë ˆë“œ ì•ˆì „í•œ í•¸ë“¤ë§ê³¼ PyMuPDF ìµœì‹  ê¸°ëŠ¥ í™œìš©.
    """
    import re

    import fitz

    annotations = []
    if not pdf_path or not documents:
        logger.info(f"[Utils] í•˜ì´ë¼ì´íŠ¸ ì¤‘ë‹¨: ê²½ë¡œ({pdf_path}) ë˜ëŠ” ë¬¸ì„œ ì—†ìŒ")
        return []

    try:
        # [ìµœì í™”] ë°”ì´íŠ¸ë¥¼ ê°€ì ¸ì™€ì„œ ê°œë³„ ìŠ¤ë ˆë“œìš© ë¬¸ì„œ ê°ì²´ ìƒì„± (Thread-safe)
        pdf_bytes = _get_cached_pdf_bytes(pdf_path)
        if not pdf_bytes:
            logger.warning(f"[Utils] PDF ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŒ: {pdf_path}")
            return []

        # ê° í˜¸ì¶œë§ˆë‹¤ ë…ë¦½ì ì¸ doc ê°ì²´ ì‚¬ìš©
        with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
            # ê³ ê¸‰ ê²€ìƒ‰ í”Œë˜ê·¸ ì„¤ì •
            search_flags = (
                fitz.TEXT_DEHYPHENATE
                | fitz.TEXT_PRESERVE_LIGATURES
                | fitz.TEXT_PRESERVE_WHITESPACE
            )

            for idx, doc_obj in enumerate(documents):
                page_num = doc_obj.metadata.get("page", 1) - 1
                if page_num < 0 or page_num >= len(doc):
                    continue

                page = doc[page_num]
                query_content = doc_obj.page_content.strip()

                # [ìˆ˜ì •] ë£¨í”„ ì‹œì‘ ì‹œ ìƒíƒœ ì´ˆê¸°í™” (ì˜¤ì—¼ ë°©ì§€)
                match_score = 0.0
                found_rects = []

                # 1. 1ì°¨ ì‹œë„: PyMuPDF ê¸°ë³¸ ê²€ìƒ‰ (ê³ ê¸‰ í”Œë˜ê·¸ ì ìš©)
                search_query = query_content[:100].replace("\n", " ")
                found_quads = page.search_for(
                    search_query, quads=True, flags=search_flags
                )

                if found_quads:
                    found_rects = [q.rect for q in found_quads]
                    match_score = 1.0  # ì§ì ‘ ë§¤ì¹­ ì„±ê³µ

                # 2. 2ì°¨ ì‹œë„: ì‹œí€€ìŠ¤ ë§¤ì¹­ í´ë°±
                if not found_rects:
                    q_words = [
                        re.sub(r"[^\w]", "", w).lower()
                        for w in query_content.split()
                        if len(w) > 1
                    ]
                    if len(q_words) >= 3:
                        p_words_raw = page.get_text("words", flags=search_flags)
                        p_words_norm = [
                            re.sub(r"[^\w]", "", w[4]).lower() for w in p_words_raw
                        ]

                        search_len = min(len(q_words), 12)
                        key_seq = q_words[:search_len]

                        best_start, max_match = -1, 0
                        for i in range(len(p_words_norm) - search_len + 1):
                            m_count = sum(
                                1
                                for j in range(search_len)
                                if p_words_norm[i + j] == key_seq[j]
                            )
                            if m_count > max_match:
                                max_match, best_start = m_count, i
                                if max_match == search_len:
                                    break

                        match_score = max_match / search_len if search_len > 0 else 0
                        if match_score >= 0.65:
                            match_limit = min(len(q_words) + 5, 80)
                            found_rects = [
                                fitz.Rect(p_words_raw[best_start + k][:4])
                                for k in range(match_limit)
                                if best_start + k < len(p_words_raw)
                            ]

                # 3. ê²°ê³¼ ì •ë¦¬ ë° ë³‘í•©
                if found_rects:
                    p_rect = page.rect
                    p_width, p_height = p_rect.width, p_rect.height

                    merged_rects = _merge_rects(found_rects)
                    chunk_annos_count = 0
                    for i, rect in enumerate(merged_rects[:3]):
                        # [ìµœì í™”] ë…¸ì´ì¦ˆ í•„í„°ë§ (ë„ˆë¬´ ì‘ì€ ì˜ì—­ ë¬´ì‹œ)
                        if rect.width < 5 or rect.height < 5:
                            continue

                        x = max(0, float(rect.x0))
                        y = max(0, float(rect.y0))
                        w = min(p_width - x, float(rect.width))
                        h = min(p_height - y, float(rect.height))

                        anno = {
                            "page": int(page_num + 1),
                            "x": x,
                            "y": y,
                            "width": w,
                            "height": h,
                            "color": color,
                            "id": f"ref_{idx}_{page_num}_{i}",
                        }
                        annotations.append(anno)
                        chunk_annos_count += 1

                    if chunk_annos_count > 0:
                        # [ìµœì í™”] ë¬¸ì„œ ì¡°ê°ë‹¹ 1ê°œì˜ í†µí•© ë¡œê·¸ë§Œ ì¶œë ¥
                        logger.info(
                            f"[Utils] í•˜ì´ë¼ì´íŠ¸ ìƒì„± ({chunk_annos_count}ê°œ) | Page: {page_num + 1} | Score: {match_score:.2f} | Text: {query_content[:50]}..."
                        )
                else:
                    if query_content:
                        logger.info(
                            f"[Utils] ë§¤ì¹­ ìµœì¢… ì‹¤íŒ¨ | Page: {page_num + 1} | Score: {match_score:.2f} | Query: {query_content[:30]}..."
                        )

    except Exception as e:
        logger.error(f"[Utils] PDF í•˜ì´ë¼ì´íŠ¸ ì¢Œí‘œ ì¶”ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)

    return annotations


def sync_run(coro):
    """
    Streamlit(ë™ê¸° í™˜ê²½)ì—ì„œ ë¹„ë™ê¸° ì½”ë£¨í‹´ì„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ í—¬í¼.
    ì „ì—­ì ìœ¼ë¡œ nest_asyncioê°€ ì ìš©ë˜ì–´ ìˆì–´ì•¼ ì‘ë™í•©ë‹ˆë‹¤.
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        return loop.run_until_complete(coro)

    return asyncio.run(coro)


def log_operation(operation_name):
    """
    ë‹¨ìˆœ ë™ê¸° í•¨ìˆ˜ìš© ë¡œê¹… ë°ì½”ë ˆì´í„°.
    GraphBuilderì˜ Node í•¨ìˆ˜ì—ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”! (config ì „ë‹¬ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥)
    """

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            logger.info(f"[SYSTEM] [TASK] {operation_name} ì‹œì‘")
            start = time.time()
            try:
                res = func(*args, **kwargs)
                dur = time.time() - start
                logger.info(f"[SYSTEM] [TASK] {operation_name} ì™„ë£Œ | ì†Œìš”: {dur:.2f}s")
                return res
            except Exception as e:
                logger.info(f"[SYSTEM] [TASK] {operation_name} ì‹¤íŒ¨ | {e}")
                raise

        return wrapper

    return decorator
