"""
í”„ë¡œì íŠ¸ ì „ë°˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ íŒŒì¼.
Utils Rebuild: ë³µì¡í•œ ë°ì½”ë ˆì´í„° ì œê±° ë° ë¹„ë™ê¸° í—¬í¼ ë‹¨ìˆœí™”.
"""

import asyncio
import functools
import hashlib
import logging
import os
import re
import time

import streamlit as st

logger = logging.getLogger(__name__)

# --- ì‚¬ì „ ì»´íŒŒì¼ëœ ì •ê·œí‘œí˜„ì‹ (ì„±ëŠ¥ ìµœì í™”) ---
_RE_LATEX_BLOCK = re.compile(r"\\\[(.*?)\\\]", re.DOTALL)
_RE_LATEX_INLINE = re.compile(r"\\\((.*?)\\\)", re.DOTALL)

# [ìˆ˜ì •] ë³µí•© ì¸ìš© íŒ¨í„´ ì§€ì›: [p.1, p.2] ë˜ëŠ” [p.1, 2, 3] ë˜ëŠ” [page 5] ë“± ì§€ì›
_RE_CITATION_BLOCK = re.compile(
    r"([\[\(])((?:[Pp](?:age)?\.?\s*)?\d+(?:[\s,]*)(?:(?:[Pp](?:age)?\.?\s*)?\d+(?:[\s,]*))*)([\]\)])",
    re.IGNORECASE,
)
_RE_EXTRACT_PAGES = re.compile(r"(\d+)")
_RE_WHITESPACE = re.compile(r"\s+")
_RE_CLEAN_LIST_NUM = re.compile(r"^\d+[\.\)]\s*")
_RE_CLEAN_LIST_BULLET = re.compile(r"^[\-\*â€¢]\s*")

# [ìˆ˜ì •] ì •ê·œì‹ ì™„í™”:
# 1. ^\d+[\.\)\s]+ : ë¬¸ë‘ì˜ ìˆ«ìì™€ ì /ê´„í˜¸ (ì˜ˆ: "1. ", "1) ")
# 2. ^\s*[\-\*\u2022]\s* : ë¬¸ë‘ì˜ ë¶ˆë › í¬ì¸íŠ¸ (ì˜ˆ: "- ", "* ")
# 3. ^["']+|["']+$ : ë¬¸ë‘/ë¬¸ë¯¸ì˜ ë”°ì˜´í‘œ
# 4. (?:^Example:|^Query:)\s* : "Example:" ê°™ì€ ì ‘ë‘ì‚¬ ì œê±°
_RE_QUERY_CLEAN_PREFIX = re.compile(
    r"^(?:\d+[\.\)\s]+|\s*[\-\*\u2022]\s*|(?:Example|Query|Question):\s*)+",
    re.IGNORECASE,
)
_RE_QUERY_CLEAN_QUOTES = re.compile(r'^["\']+|["\']+$')


def normalize_latex_delimiters(text: str) -> str:
    r"""
    LLMì´ ì¶œë ¥í•˜ëŠ” ë‹¤ì–‘í•œ LaTeX ìˆ˜ì‹ êµ¬ë¶„ìë¥¼ Streamlit í‘œì¤€($ ë˜ëŠ” $$)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - \( ... \) -> $ ... $ (ì¸ë¼ì¸)
    - \[ ... \] -> $$ ... $$ (ë¸”ë¡)
    - ê¸°í˜¸ ì•ë’¤ì˜ ë¶ˆí•„ìš”í•œ ì´ìŠ¤ì¼€ì´í”„ ì œê±°
    """
    if not text:
        return text

    # 1. ë¸”ë¡ ìˆ˜ì‹ ë³€í™˜: \[ ... \] -> $$ ... $$
    text = _RE_LATEX_BLOCK.sub(r"$$\1$$", text)

    # 2. ì¸ë¼ì¸ ìˆ˜ì‹ ë³€í™˜: \( ... \) -> $ ... $
    text = _RE_LATEX_INLINE.sub(r"$\1$", text)

    # 3. ì˜ëª»ëœ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì •ì œ (ì˜ˆ: \$ -> $)
    # ë‹¨, ì½”ë“œ ë¸”ë¡ ë‚´ì˜ ê¸°í˜¸ëŠ” ê±´ë“œë¦¬ì§€ ì•Šë„ë¡ ì£¼ì˜ê°€ í•„ìš”í•˜ë‚˜ ì¼ë°˜ ë‹µë³€ ê¸°ì¤€ ì²˜ë¦¬
    text = text.replace(r"\$", "$")

    return text


def extract_annotations_from_docs(documents: list) -> list[dict]:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ ì¢Œí‘œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬
    í˜„ì¬ ì²­í¬ì˜ í…ìŠ¤íŠ¸ì™€ ì¼ì¹˜í•˜ëŠ” ì˜ì—­ë§Œ ì¤„(Line) ë‹¨ìœ„ë¡œ í•˜ì´ë¼ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    annotations: list[dict] = []
    if not documents:
        return annotations

    logger.info(f"[HIGHLIGHT] Processing {len(documents)} docs for annotations")

    for _i, doc in enumerate(documents):
        meta = (
            getattr(doc, "metadata", {})
            if hasattr(doc, "metadata")
            else doc.get("metadata", {})
        )
        page_val = int(meta.get("page", 1))
        all_coords = meta.get("word_coords", [])
        content = doc.page_content.lower()

        if not all_coords:
            continue

        # [ê³ ë„í™”] ì—°ì†ì„± ê¸°ë°˜ í…ìŠ¤íŠ¸ ë§¤ì¹­ (Sequence Matching)
        # 1. ì²­í¬ í…ìŠ¤íŠ¸ì™€ PDF í…ìŠ¤íŠ¸ë¥¼ ìˆœìˆ˜ ë‹¨ì–´ í† í°ìœ¼ë¡œ ì •ê·œí™”
        content_tokens = re.findall(r"[\w\d]+", content)
        if not content_tokens:
            continue

        pdf_tokens = [re.sub(r"[^\w\d]", "", str(c[4]).lower()) for c in all_coords]

        # 2. PDF ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì—ì„œ í˜„ì¬ ì²­í¬ê°€ ì‹œì‘ë˜ëŠ” ìµœì ì˜ ì§€ì  ê²€ìƒ‰ (Sliding Window)
        best_start = -1
        max_match = 0
        window_size = min(20, len(content_tokens))  # ì‹œì‘ ë¶€ë¶„ 20ë‹¨ì–´ë¡œ ì§€ì  íƒìƒ‰

        for j in range(len(pdf_tokens) - len(content_tokens) + 1):
            current_match = 0
            for k in range(window_size):
                if pdf_tokens[j + k] == content_tokens[k]:
                    current_match += 1

            if current_match > max_match:
                max_match = current_match
                best_start = j

            # 80% ì´ìƒ ì¼ì¹˜í•˜ë©´ ì¦‰ì‹œ ì‹œì‘ì ìœ¼ë¡œ í™•ì • (ì„±ëŠ¥ ìµœì í™”)
            if current_match >= window_size * 0.8:
                best_start = j
                break

        # 3. ë§¤ì¹­ëœ ì§€ì ë¶€í„° ì²­í¬ ê¸¸ì´ë§Œí¼ì˜ ì¢Œí‘œë§Œ ì¶”ì¶œ
        if best_start != -1:
            # ì²­í¬ í…ìŠ¤íŠ¸ ë‚´ì˜ ì‹¤ì œ ë‹¨ì–´ ê°œìˆ˜ë§Œí¼ ì¢Œí‘œë¥¼ ê°€ì ¸ì˜´
            filtered_coords = all_coords[best_start : best_start + len(content_tokens)]
        else:
            # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œì—ë§Œ ê¸°ì¡´ì˜ ë£¨ì¦ˆí•œ í•„í„°ë§ìœ¼ë¡œ í´ë°± (ìµœì†Œ ê°€ì‹œì„±)
            filtered_coords = [
                c
                for c in all_coords
                if re.sub(r"[^\w\d]", "", str(c[4]).lower()) in content_tokens[:50]
            ]

        if not filtered_coords:
            continue

        # 4. ì¤„ ë‹¨ìœ„ ê·¸ë£¹í™” ë° ë°•ìŠ¤ ìƒì„±
        lines: dict[int, list] = {}
        for c in filtered_coords:
            y_key = round(c[1] / 8) * 8
            if y_key not in lines:
                lines[y_key] = []
            lines[y_key].append(c)

        doc_anno_count = 0
        for y_key in sorted(lines.keys()):
            line_coords = lines[y_key]
            x_min = min(c[0] for c in line_coords)
            y_min = min(c[1] for c in line_coords)
            x_max = max(c[2] for c in line_coords)
            y_max = max(c[3] for c in line_coords)

            if x_max > x_min and y_max > y_min:
                annotations.append(
                    {
                        "page": page_val,
                        "x": x_min,
                        "y": y_min,
                        "width": x_max - x_min,
                        "height": y_max - y_min,
                        "color": "red",
                        "thickness": 2,
                    }
                )
                doc_anno_count += 1

        logger.info(
            f"[HIGHLIGHT] Page {page_val}: Found chunk sequence at index {best_start}, created {doc_anno_count} line boxes"
        )

    return annotations


def apply_tooltips_to_response(
    response_text: str, documents: list | None = None, msg_index: int = 0
) -> str:
    """
    ë‹µë³€ ë‚´ì˜ ì¸ìš©êµ¬([1], [p.5] ë“±)ë¥¼ ì°¾ì•„ ë¬¸ì„œ ì •ë³´ íˆ´íŒì„ ì…í™ë‹ˆë‹¤.
    """
    if not response_text:
        return response_text

    # 1. LaTeX ì •ê·œí™” ë¨¼ì € ìˆ˜í–‰
    text = normalize_latex_delimiters(response_text)

    # 2. ë¬¸ì„œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
    if not documents:
        return text

    def replace_citation(match):
        full_match = match.group(0)
        inner_text = match.group(2)

        # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
        page_matches = _RE_EXTRACT_PAGES.findall(inner_text)
        if not page_matches:
            return full_match

        target_page = int(page_matches[0])

        # í•´ë‹¹ í˜ì´ì§€ì™€ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ì²­í¬ ì°¾ê¸°
        matching_doc = None
        for doc in documents:
            meta = (
                getattr(doc, "metadata", {})
                if hasattr(doc, "metadata")
                else doc.get("metadata", {})
            )
            doc_page = int(meta.get("page", -1))
            if doc_page == target_page:  # [ìˆ˜ì •] ë‘˜ ë‹¤ 1-indexedì´ë¯€ë¡œ ì§ì ‘ ë¹„êµ
                matching_doc = doc
                break

        if matching_doc:
            content = (
                getattr(matching_doc, "page_content", "")
                if hasattr(matching_doc, "page_content")
                else matching_doc.get("page_content", "")
            )
            # íˆ´íŒìš© í…ìŠ¤íŠ¸ ì •ì œ (HTML íƒœê·¸ ë° ë”°ì˜´í‘œ ì²˜ë¦¬)
            clean_content = (
                content.replace('"', "&quot;").replace("'", "&apos;")[:200] + "..."
            )

            # í•˜ì´ë¼ì´íŠ¸ ìŠ¤íƒ€ì¼ ì ìš©ëœ HTML ë°˜í™˜
            return f'<span class="citation-tooltip" title="{clean_content}" style="color: #1e88e5; font-weight: bold; cursor: help; border-bottom: 1px dotted #1e88e5;">{full_match}</span>'

        return full_match

    # ì¸ìš©êµ¬ íŒ¨í„´ ë§¤ì¹­ ë° ì¹˜í™˜
    try:
        text = _RE_CITATION_BLOCK.sub(replace_citation, text)
    except Exception as e:
        logger.error(f"[Utils] ì¸ìš©êµ¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    return text


# --- ì „ì²˜ë¦¬ìš© ê³ ì† í…Œì´ë¸” ---
# ë„ ë¬¸ì ë“± ì œì–´ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ëŠ” í…Œì´ë¸”
_CLEAN_TRANS_TABLE = str.maketrans({"\x00": " ", "\r": " ", "\n": " ", "\t": " "})


def preprocess_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì •ì œ: ì œì–´ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ê³  ì—°ì† ê³µë°±ì„ ê³ ì† ì •ê·œí™”
    [ìµœì í™”] ì •ê·œì‹ ì—”ì§„ ëŒ€ì‹  ë„¤ì´í‹°ë¸Œ split/joinì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
    """
    if not text:
        return ""

    # 1. str.translateë¥¼ ì´ìš©í•œ ê³ ì† ë¬¸ì ì¹˜í™˜
    text = text.translate(_CLEAN_TRANS_TABLE)

    # 2. ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ í†µí•© (split/joinì´ re.subë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„)
    return " ".join(text.split())


def clean_query_text(query: str) -> str:
    """ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê¸°í˜¸, ë²ˆí˜¸, ì ‘ë‘ì‚¬(Example:, Question: ë“±) ì œê±°"""
    if not query:
        return ""

    # 1. ë¬¸ë‘ì˜ ìˆ«ì, ë¶ˆë ›, ì ‘ë‘ì‚¬(Example:, Query: ë“±) ì¼ê´„ ì œê±°
    query = _RE_QUERY_CLEAN_PREFIX.sub("", query.strip())

    # 2. ë¬¸ë‘/ë¬¸ë¯¸ ë”°ì˜´í‘œ ì œê±°
    query = _RE_QUERY_CLEAN_QUOTES.sub("", query.strip())

    return query.strip()


def safe_cache_data(func=None, **kwargs):
    """Streamlit ëŸ°íƒ€ì„ì´ ìˆì„ ë•Œë§Œ cache_dataë¥¼ ì ìš©í•˜ê³ , ì—†ìœ¼ë©´ ì›ë³¸ í•¨ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if func is None:
        return lambda f: safe_cache_data(f, **kwargs)

    try:
        if st.runtime.exists():
            return st.cache_data(**kwargs)(func)
    except Exception:
        pass
    return func


def safe_cache_resource(func=None, **kwargs):
    """Streamlit ëŸ°íƒ€ì„ì´ ìˆì„ ë•Œë§Œ cache_resourceë¥¼ ì ìš©í•˜ê³ , ì—†ìœ¼ë©´ ì›ë³¸ í•¨ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if func is None:
        return lambda f: safe_cache_resource(f, **kwargs)

    try:
        if st.runtime.exists():
            return st.cache_resource(**kwargs)(func)
    except Exception:
        pass
    return func


@safe_cache_data(ttl=5)  # 5ì´ˆ ë™ì•ˆ ë¦¬ì†ŒìŠ¤ ì •ë³´ ìºì‹±
def get_ollama_resource_usage(model_name: str) -> str:
    """
    Ollama APIë¥¼ í†µí•´ íŠ¹ì • ëª¨ë¸ì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ìƒíƒœ(GPU/CPU)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        import requests

        from common.config import OLLAMA_BASE_URL

        # Ollama ps API í˜¸ì¶œ
        response = requests.get(f"{OLLAMA_BASE_URL}/api/ps", timeout=2.0)
        if response.status_code == 200:
            data = response.json()
            models = data.get("models", [])

            for m in models:
                if model_name in m.get("name", ""):
                    size_vram = m.get("size_vram", 0)
                    size = m.get("size", 1)

                    # VRAM ì‚¬ìš© ë¹„ìœ¨ ê³„ì‚°
                    vram_ratio = (size_vram / size) * 100
                    if vram_ratio >= 90:
                        return f"GPU (VRAM {vram_ratio:.1f}%)"
                    elif vram_ratio > 0:
                        return f"Hybrid (VRAM {vram_ratio:.1f}%, CPU {100 - vram_ratio:.1f}%)"
                    else:
                        return "CPU (0% VRAM)"

            return "Unknown (Not running)"
        return "Unknown (API Error)"
    except Exception:
        return "Unknown (Connection Error)"


def format_error_message(e: Exception) -> str:
    """
    ë°œìƒí•œ ì˜ˆì™¸ ê°ì²´ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ì¹œì ˆí•œ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    from common.exceptions import (
        EmbeddingModelError,
        EmptyPDFError,
        InsufficientChunksError,
        LLMInferenceError,
    )

    err_type = type(e).__name__
    msg = str(e)

    # 1. ì»¤ìŠ¤í…€ ë„ë©”ì¸ ì˜ˆì™¸ ì²˜ë¦¬
    if isinstance(e, EmptyPDFError):
        return "ğŸ“„ PDF íŒŒì¼ì— í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ì´ë¯¸ì§€ë¡œë§Œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•´ ë³´ì„¸ìš”."
    elif isinstance(e, InsufficientChunksError):
        return "âš ï¸ ë¬¸ì„œì˜ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    elif isinstance(e, LLMInferenceError):
        return f"ğŸ¤– ì¶”ë¡  ëª¨ë¸ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {msg}"
    elif isinstance(e, EmbeddingModelError):
        return "ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìì›(VRAM/RAM)ì´ ë¶€ì¡±í•œì§€ í™•ì¸í•´ ì£¼ì„¸ìš”."

    # 2. ì¼ë°˜ ì‹œìŠ¤í…œ ì˜ˆì™¸ ì²˜ë¦¬
    if "ConnectionError" in err_type or "11434" in msg:
        return (
            "ğŸ”Œ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”."
        )
    elif "timeout" in msg.lower():
        return (
            "âŒ› ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        )
    elif "out of memory" in msg.lower() or "CUDA" in msg:
        return "ğŸš€ GPU ë©”ëª¨ë¦¬(VRAM)ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ê±°ë‚˜ ëª¨ë¸ì„ ì‘ì€ ê²ƒìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”."

    # 3. ê¸°ë³¸ê°’
    return f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ ({err_type}): {msg}"


def fast_hash(text: str, length: int = 16) -> str:
    """
    ë³´ì•ˆì´ í•„ìš” ì—†ëŠ” ë‹¨ìˆœ ì‹ë³„ìš© ê³ ì† í•´ì‹œ í•¨ìˆ˜.
    SHA256ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥¸ MD5ë¥¼ ì‚¬ìš©í•˜ê³  ê²°ê³¼ ê¸¸ì´ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.
    """
    if not text:
        return "0" * length
    # usedforsecurity=False: ë³´ì•ˆ ì§„ë‹¨ ë„êµ¬(Bandit ë“±)ì— ì´ í•´ì‹œê°€
    # ì•”í˜¸í™”ë‚˜ ë³´ì•ˆ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒì„ ì•Œë¦½ë‹ˆë‹¤.
    return hashlib.md5(text.encode(errors="ignore"), usedforsecurity=False).hexdigest()[
        :length
    ]


def count_tokens_rough(text: str) -> int:
    """
    í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ëŒ€ëµì ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    - ì˜ì–´: ì•½ 4ê¸€ìë‹¹ 1í† í°
    - í•œê¸€/íŠ¹ìˆ˜ë¬¸ì: ì•½ 1~2ê¸€ìë‹¹ 1í† í°
    ë³´ìˆ˜ì ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•´ (ê¸€ì ìˆ˜ / 2.5)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    if not text:
        return 0
    return int(len(text) / 2.5) + 1


@safe_cache_data(ttl=4)
def _get_cached_pdf_bytes(pdf_path: str) -> bytes | None:
    """PDF íŒŒì¼ ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì— ìºì‹±í•©ë‹ˆë‹¤. (I/O ì ˆê°)"""
    if os.path.exists(pdf_path):
        with open(pdf_path, "rb") as f:
            return f.read()
    return None


def sync_run(coro):
    """
    Streamlit(ë™ê¸° í™˜ê²½)ì—ì„œ ë¹„ë™ê¸° ì½”ë£¨í‹´ì„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ í—¬í¼.
    ì „ì—­ì ìœ¼ë¡œ nest_asyncioê°€ ì ìš©ë˜ì–´ ìˆì–´ì•¼ ì‘ë™í•©ë‹ˆë‹¤.
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        return loop.run_until_complete(coro)

    return asyncio.run(coro)


def log_operation(operation_name):
    """
    ë™ê¸° ë° ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ë¡œê¹… ë°ì½”ë ˆì´í„°.
    GraphBuilderì˜ Node í•¨ìˆ˜ì—ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”! (config ì „ë‹¬ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥)
    """

    def decorator(func):
        if asyncio.iscoroutinefunction(func):

            @functools.wraps(func)
            async def async_wrapper(*args, **kwargs):
                logger.info(f"[SYSTEM] [TASK] {operation_name} ì‹œì‘")
                start = time.time()
                try:
                    res = await func(*args, **kwargs)
                    dur = time.time() - start
                    logger.info(
                        f"[SYSTEM] [TASK] {operation_name} ì™„ë£Œ | ì†Œìš”: {dur:.2f}s"
                    )
                    return res
                except Exception as e:
                    logger.info(f"[SYSTEM] [TASK] {operation_name} ì‹¤íŒ¨ | {e}")
                    raise

            return async_wrapper
        else:

            @functools.wraps(func)
            def sync_wrapper(*args, **kwargs):
                logger.info(f"[SYSTEM] [TASK] {operation_name} ì‹œì‘")
                start = time.time()
                try:
                    res = func(*args, **kwargs)
                    dur = time.time() - start
                    logger.info(
                        f"[SYSTEM] [TASK] {operation_name} ì™„ë£Œ | ì†Œìš”: {dur:.2f}s"
                    )
                    return res
                except Exception as e:
                    logger.info(f"[SYSTEM] [TASK] {operation_name} ì‹¤íŒ¨ | {e}")
                    raise

            return sync_wrapper

    return decorator
