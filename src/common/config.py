"""
config.yml íŒŒì¼ê³¼ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤.
"""

import logging
import os
from collections.abc import Callable
from pathlib import Path
from typing import Any, Union

import yaml
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì • (í˜„ì¬ íŒŒì¼ ê¸°ì¤€ 3ë‹¨ê³„ ìƒìœ„ ë””ë ‰í† ë¦¬: src/common/config.py -> root)
PROJECT_ROOT = Path(__file__).parent.parent.parent
CONFIG_PATH = PROJECT_ROOT / "config.yml"


def _load_config() -> dict[str, Any]:
    """YAML ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        if not CONFIG_PATH.exists():
            raise FileNotFoundError(f"Config file not found at: {CONFIG_PATH}")

        with open(CONFIG_PATH, encoding="utf-8") as f:
            return yaml.safe_load(f) or {}  # ë¹ˆ íŒŒì¼ì¼ ê²½ìš° ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    except Exception as e:
        # ì„¤ì • ë¡œë“œ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ë¯€ë¡œ ë¡œê·¸ ë‚¨ê¸°ê³  ì¬ë°œìƒ
        logger.critical(f"Failed to load configuration: {e}")
        raise RuntimeError(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}") from e


def _get_env(key: str, default: Any, cast_type: Callable[[Any], Any] = str) -> Any:
    """í™˜ê²½ ë³€ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê³  íƒ€ì… ë³€í™˜í•©ë‹ˆë‹¤."""
    value = os.getenv(key)
    if value is None:
        return default
    try:
        return cast_type(value)
    except (ValueError, TypeError):
        cast_type_name = getattr(cast_type, "__name__", str(cast_type))
        logger.warning(
            f"í™˜ê²½ ë³€ìˆ˜ '{key}'ì˜ ê°’ '{value}'ì„(ë¥¼) {cast_type_name} íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            f"ê¸°ë³¸ê°’ '{default}'ì„(ë¥¼) ì‚¬ìš©í•©ë‹ˆë‹¤."
        )
        return default


# ì„¤ì • ë¡œë“œ
_config = _load_config()

# --- ëª¨ë¸ ë° ì„¤ì • ìƒìˆ˜ ---
_models_config = _config.get("models", {})
DEFAULT_OLLAMA_MODEL: str = os.getenv(
    "DEFAULT_OLLAMA_MODEL",
    _models_config.get("default_ollama", "qwen3:4b-instruct-2507-q4_K_M"),
)

# Ollama ì„œë²„ ì£¼ì†Œ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ ìš°ì„ )
OLLAMA_BASE_URL: str = _get_env(
    "OLLAMA_BASE_URL", _models_config.get("base_url", "http://127.0.0.1:11434")
)

# ì˜ˆì¸¡ íŒŒë¼ë¯¸í„° (í™˜ê²½ ë³€ìˆ˜ ìš°ì„ , ì‹¤íŒ¨ ì‹œ config.yml, ë§ˆì§€ë§‰ìœ¼ë¡œ í•˜ë“œì½”ë”©ëœ ê¸°ë³¸ê°’)
OLLAMA_NUM_PREDICT: int = _get_env(
    "OLLAMA_NUM_PREDICT", _models_config.get("ollama_num_predict", 4096), int
)
OLLAMA_TEMPERATURE: float = _get_env(
    "OLLAMA_TEMPERATURE", _models_config.get("temperature", 0.5), float
)
OLLAMA_NUM_CTX: int = _get_env(
    "OLLAMA_NUM_CTX", _models_config.get("num_ctx", 4096), int
)
OLLAMA_TOP_P: float = _get_env("OLLAMA_TOP_P", _models_config.get("top_p", 0.9), float)
OLLAMA_TIMEOUT: float = _get_env(
    "OLLAMA_TIMEOUT", _models_config.get("timeout", 900.0), float
)
# [ìµœì í™”] ì‹œìŠ¤í…œ ì½”ì–´ ìˆ˜ë¥¼ í™œìš©í•œ ì“°ë ˆë“œ ì„¤ì • (ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ë³´ìˆ˜ì  í• ë‹¹)
OLLAMA_NUM_THREAD: int = _get_env(
    "OLLAMA_NUM_THREAD",
    max(1, (os.cpu_count() or 4) // 2),
    int,  # ì½”ì–´ì˜ ì ˆë°˜ë§Œ ì‚¬ìš©
)

# --- ì„ë² ë”© ì„¤ì • ---
DEFAULT_EMBEDDING_MODEL: str = os.getenv(
    "DEFAULT_EMBEDDING_MODEL",
    _models_config.get("default_embedding", "nomic-embed-text"),
)
# ì´ì œ ì„ë² ë”© ëª¨ë¸ ëª©ë¡ì€ ê¸°ë³¸ì ìœ¼ë¡œ default_embedding í•˜ë‚˜ë§Œ í¬í•¨í•©ë‹ˆë‹¤.
# (UI ë“±ì—ì„œ Ollama ëª¨ë¸ ëª©ë¡ì„ ë³‘í•©í•˜ì—¬ ë™ì ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥)
AVAILABLE_EMBEDDING_MODELS: list[str] = [DEFAULT_EMBEDDING_MODEL]
CACHE_DIR: str = str(PROJECT_ROOT / _models_config.get("cache_dir", ".model_cache"))
EMBEDDING_BATCH_SIZE: Union[int, str] = _models_config.get(
    "embedding_batch_size",
    16,  # auto ëŒ€ì‹  ëª…ì‹œì  ê°’ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì œí•œ
)
EMBEDDING_DEVICE: str = _models_config.get("embedding_device", "auto")

# --- RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • ---
_rag_config = _config.get("rag", {})
RETRIEVER_CONFIG: dict = _rag_config.get("retriever", {})
RERANKER_CONFIG: dict = _rag_config.get("reranker", {})
TEXT_SPLITTER_CONFIG: dict = _rag_config.get("text_splitter", {})
SEMANTIC_CHUNKER_CONFIG: dict = _rag_config.get("semantic_chunker", {})
VECTOR_STORE_CACHE_DIR: str = str(
    PROJECT_ROOT
    / _rag_config.get("vector_store_cache_dir", ".model_cache/vector_store_cache")
)
QUERY_EXPANSION_CONFIG: dict = _rag_config.get("query_expansion", {"enabled": True})
INTENT_ANALYSIS_ENABLED: bool = _rag_config.get("intent_analysis_enabled", False)
INTENT_PARAMETERS: dict = _rag_config.get("intent_parameters", {})
_prompts_config = _rag_config.get("prompts") or {}
ANALYSIS_PROTOCOL: str = _prompts_config.get("analysis_protocol", "")
RESEARCH_SYSTEM_PROMPT: str = _prompts_config.get("research_system_prompt", "")
FACTOID_SYSTEM_PROMPT: str = _prompts_config.get("factoid_system_prompt", "")
GREETING_SYSTEM_PROMPT: str = _prompts_config.get("greeting_system_prompt", "")
OUT_OF_CONTEXT_SYSTEM_PROMPT: str = _prompts_config.get(
    "out_of_context_system_prompt", ""
)
QA_SYSTEM_PROMPT: str = _prompts_config.get("qa_system_prompt", "")
QA_HUMAN_PROMPT: str = _prompts_config.get("qa_human_prompt", "")
QUERY_EXPANSION_PROMPT: str = _prompts_config.get("query_expansion_prompt", "")

# --- ìºì‹œ ë³´ì•ˆ ì„¤ì • ---
_cache_security_config = _config.get("cache_security", {})

# ë³´ì•ˆ ë ˆë²¨ (environment variable ìš°ì„ )
CACHE_SECURITY_LEVEL: str = _get_env(
    "CACHE_SECURITY_LEVEL", _cache_security_config.get("security_level", "medium"), str
)

# HMAC ë¹„ë°€ (environment variable ìš°ì„ )
CACHE_HMAC_SECRET: str | None = _get_env(
    "CACHE_HMAC_SECRET", _cache_security_config.get("hmac_secret"), str
)

# ì‹ ë¢° ê²½ë¡œ (í™˜ê²½ë³€ìˆ˜ê°€ ìˆìœ¼ë©´ ì‰¼í‘œë¡œ ë¶„ë¦¬)
_trusted_paths_env = os.getenv("TRUSTED_CACHE_PATHS")
CACHE_TRUSTED_PATHS: list[str]
if _trusted_paths_env:
    CACHE_TRUSTED_PATHS = [p.strip() for p in _trusted_paths_env.split(",")]
else:
    CACHE_TRUSTED_PATHS = _cache_security_config.get("trusted_paths", [])

# ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë™ì‘
CACHE_VALIDATION_ON_FAILURE: str = _get_env(
    "CACHE_VALIDATION_ON_FAILURE",
    _cache_security_config.get("on_validation_failure", "regenerate"),
    str,
)

# íŒŒì¼ ê¶Œí•œ ê²€ì‚¬
CACHE_CHECK_PERMISSIONS: bool = _get_env(
    "CACHE_CHECK_PERMISSIONS",
    _cache_security_config.get("check_permissions", True),
    lambda x: x.lower() == "true" if isinstance(x, str) else x,
)

# ì˜ˆìƒ íŒŒì¼ ê¶Œí•œ
_expected_file_mode = _cache_security_config.get("expected_file_mode", 0o644)
CACHE_EXPECTED_FILE_MODE: int = (
    int(_expected_file_mode, 0)
    if isinstance(_expected_file_mode, str)
    else _expected_file_mode
)

_expected_dir_mode = _cache_security_config.get("expected_dir_mode", 0o755)
CACHE_EXPECTED_DIR_MODE: int = (
    int(_expected_dir_mode, 0)
    if isinstance(_expected_dir_mode, str)
    else _expected_dir_mode
)

# --- ì „ì—­ ìºì‹œ í™œì„±í™” ì„¤ì • ---
_cache_toggle_config = _config.get("global_cache", {})

ENABLE_VECTOR_CACHE: bool = _get_env(
    "ENABLE_VECTOR_CACHE",
    _cache_toggle_config.get("enable_vector_cache", True),
    lambda x: x.lower() == "true" if isinstance(x, str) else x,
)

ENABLE_RESPONSE_CACHE: bool = _get_env(
    "ENABLE_RESPONSE_CACHE",
    _cache_toggle_config.get("enable_response_cache", True),
    lambda x: x.lower() == "true" if isinstance(x, str) else x,
)


# --- ì±„íŒ… UI ìƒìˆ˜ ---
_ui_config = _config.get("ui", {})
UI_CONTAINER_HEIGHT: int = _ui_config.get("container_height", 650)
_ui_messages = _ui_config.get("messages", {})

# UI ë©”ì‹œì§€ (get ë©”ì„œë“œë¡œ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°)
MSG_PREPARING_ANSWER = _ui_messages.get("preparing_answer", "ë‹µë³€ ìƒì„± ì¤€ë¹„ ì¤‘...")
MSG_NO_RELATED_INFO = _ui_messages.get(
    "no_related_info", "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
)
MSG_SIDEBAR_TITLE = _ui_messages.get("sidebar_title", "âš™ï¸ ì„¤ì •")
MSG_PDF_UPLOADER_LABEL = _ui_messages.get("pdf_uploader_label", "PDF íŒŒì¼ ì—…ë¡œë“œ")
MSG_MODEL_SELECTOR_LABEL = _ui_messages.get("model_selector_label", "LLM ëª¨ë¸ ì„ íƒ")
MSG_EMBEDDING_SELECTOR_LABEL = _ui_messages.get(
    "embedding_selector_label", "ì„ë² ë”© ëª¨ë¸ ì„ íƒ"
)
MSG_SYSTEM_STATUS_TITLE = _ui_messages.get("system_status_title", "ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
MSG_LOADING_MODELS = _ui_messages.get(
    "loading_models", "LLM ëª¨ë¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."
)
MSG_PDF_VIEWER_TITLE = _ui_messages.get("pdf_viewer_title", "ğŸ“„ PDF ë¯¸ë¦¬ë³´ê¸°")
MSG_PDF_VIEWER_NO_FILE = _ui_messages.get(
    "pdf_viewer_no_file", "ë¯¸ë¦¬ë³¼ PDFê°€ ì—†ìŠµë‹ˆë‹¤."
)
MSG_PDF_VIEWER_PREV_BUTTON = _ui_messages.get("pdf_viewer_prev_button", "â† ì´ì „")
MSG_PDF_VIEWER_NEXT_BUTTON = _ui_messages.get("pdf_viewer_next_button", "ë‹¤ìŒ â†’")
MSG_PDF_VIEWER_PAGE_SLIDER = _ui_messages.get("pdf_viewer_page_slider", "í˜ì´ì§€ ì´ë™")
MSG_PDF_VIEWER_ERROR = _ui_messages.get("pdf_viewer_error", "PDF ì˜¤ë¥˜: {e}")
MSG_CHAT_TITLE = _ui_messages.get("chat_title", "ğŸ’¬ ì±„íŒ…")
MSG_CHAT_INPUT_PLACEHOLDER = _ui_messages.get(
    "chat_input_placeholder", "PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
)
MSG_CHAT_NO_QA_SYSTEM = _ui_messages.get("chat_no_qa_system", "QA ì‹œìŠ¤í…œ ë¯¸ì¤€ë¹„")
MSG_CHAT_GUIDE = _ui_messages.get("chat_guide", "ì‚¬ìš© ê°€ì´ë“œ")
MSG_STREAMING_ERROR = _ui_messages.get("streaming_error", "ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
MSG_GENERIC_ERROR = _ui_messages.get("generic_error", "ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
MSG_RETRY_BUTTON = _ui_messages.get("retry_button", "ì¬ì‹œë„")

_ui_errors = _ui_messages.get("errors", {})
MSG_ERROR_OLLAMA_NOT_RUNNING = _ui_errors.get(
    "ollama_not_running", "Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨"
)
