"""
ì¸ë±ìŠ¤ ìµœì í™” ëª¨ë“ˆ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±, ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ.
"""

import logging
from dataclasses import dataclass, field
from enum import Enum
from threading import RLock
from typing import Any

import numpy as np
from langchain_core.documents import Document

logger = logging.getLogger(__name__)


class CompressionMethod(Enum):
    """ë²¡í„° ì••ì¶• ë°©ë²•."""

    NONE = "none"
    QUANTIZATION_INT8 = "quantization_int8"
    QUANTIZATION_INT4 = "quantization_int4"
    PRODUCT_QUANTIZATION = "product_quantization"


class IndexOptimizationStrategy(Enum):
    """ì¸ë±ìŠ¤ ìµœì í™” ì „ëµ."""

    STANDARD = "standard"
    MEMORY_EFFICIENT = "memory_efficient"
    SPEED_OPTIMIZED = "speed_optimized"
    BALANCED = "balanced"


@dataclass
class VectorQuantizationConfig:
    """ë²¡í„° ì–‘ìí™” ì„¤ì •."""

    compression_method: CompressionMethod = CompressionMethod.QUANTIZATION_INT8
    target_bits: int = 8  # 4 ë˜ëŠ” 8
    preserve_norm: bool = True
    enable_scaling: bool = True


@dataclass
class IndexOptimizationConfig:
    """ì¸ë±ìŠ¤ ìµœì í™” ì„¤ì •."""

    strategy: IndexOptimizationStrategy = IndexOptimizationStrategy.BALANCED
    quantization_config: VectorQuantizationConfig = field(
        default_factory=VectorQuantizationConfig
    )
    enable_lru_cache: bool = True
    max_cache_size: int = 1000
    enable_doc_pruning: bool = True  # ğŸš€ ê¸°ë³¸ í™œì„±í™” (NumPy ìµœì í™” ì™„ë£Œë¡œ ë¶€í•˜ ì—†ìŒ)
    min_doc_similarity: float = 0.98  # ë” ì—„ê²©í•˜ê³  ì•ˆì „í•œ ì¤‘ë³µ ê¸°ì¤€
    enable_metadata_indexing: bool = True


@dataclass
class IndexStats:
    """ì¸ë±ìŠ¤ í†µê³„."""

    total_documents: int = 0
    total_vectors: int = 0
    original_size_mb: float = 0.0
    compressed_size_mb: float = 0.0
    compression_ratio: float = 0.0
    avg_search_time_ms: float = 0.0
    num_unique_metadata_fields: int = 0
    pruned_documents: int = 0


class VectorQuantizer:
    """ë²¡í„° ì–‘ìí™” ì—”ì§„."""

    def __init__(self, config: VectorQuantizationConfig):
        self.config = config
        self._lock = RLock()

    def quantize_vectors(
        self, vectors: list[np.ndarray]
    ) -> tuple[list[np.ndarray], dict[str, Any]]:
        """
        ë²¡í„° ì–‘ìí™”.

        Args:
            vectors: ì›ë³¸ ë²¡í„° ë¦¬ìŠ¤íŠ¸ (shape: [n, d])

        Returns:
            (ì–‘ìí™”ëœ ë²¡í„°, ë©”íƒ€ë°ì´í„°)
        """
        if self.config.compression_method == CompressionMethod.NONE:
            return vectors, {"method": "none"}

        if self.config.compression_method == CompressionMethod.QUANTIZATION_INT8:
            return self._quantize_int8(vectors)
        elif self.config.compression_method == CompressionMethod.QUANTIZATION_INT4:
            return self._quantize_int4(vectors)
        elif self.config.compression_method == CompressionMethod.PRODUCT_QUANTIZATION:
            return self._product_quantization(vectors)

    def _quantize_int8(
        self, vectors: list[np.ndarray]
    ) -> tuple[list[np.ndarray], dict]:
        """INT8 ì–‘ìí™”."""
        quantized = []
        scales = []
        offsets = []

        for vec in vectors:
            if self.config.preserve_norm:
                norm = np.linalg.norm(vec)

            # Min-max ì •ê·œí™”
            vec_min: float = float(np.min(vec))
            vec_max: float = float(np.max(vec))
            scale: float = (
                (vec_max - vec_min) / 255.0 if (vec_max - vec_min) > 0 else 1.0
            )
            offset: float = vec_min

            # ì–‘ìí™”
            quantized_vec = np.round((vec - offset) / scale).astype(np.uint8)
            quantized.append(quantized_vec)

            scales.append(float(scale))
            offsets.append(float(offset))

            if self.config.preserve_norm:
                scales[-1] = float(scales[-1] * norm)

        metadata = {
            "method": "quantization_int8",
            "scales": scales,
            "offsets": offsets,
            "compression_ratio": 4.0,  # float32 â†’ int8
        }

        return quantized, metadata

    def _quantize_int4(
        self, vectors: list[np.ndarray]
    ) -> tuple[list[np.ndarray], dict]:
        """INT4 ì–‘ìí™” (ë” ë†’ì€ ì••ì¶•)."""
        quantized = []
        scales = []
        offsets = []

        for vec in vectors:
            # Min-max ì •ê·œí™”
            vec_min: float = float(np.min(vec))
            vec_max: float = float(np.max(vec))
            scale: float = (
                (vec_max - vec_min) / 15.0 if (vec_max - vec_min) > 0 else 1.0
            )
            offset: float = vec_min

            # ì–‘ìí™” (4-bitëŠ” uint8ë¡œ í‘œí˜„, ì‹¤ì œë¡œëŠ” ë‘ ê°œì˜ 4-bit ê°’)
            quantized_vec = np.round((vec - offset) / scale).astype(np.uint8)
            quantized.append(quantized_vec)

            scales.append(scale)
            offsets.append(offset)

        metadata = {
            "method": "quantization_int4",
            "scales": scales,
            "offsets": offsets,
            "compression_ratio": 8.0,  # float32 â†’ int4 (packed as uint8)
        }

        return quantized, metadata

    def _product_quantization(
        self, vectors: list[np.ndarray]
    ) -> tuple[list[np.ndarray], dict]:
        """Product Quantization (PQ)."""
        # ê°„ë‹¨í•œ PQ êµ¬í˜„
        quantized = []
        assignments = []

        num_subspaces = 4

        for vec in vectors:
            # ë¶€ë¶„ê³µê°„ë³„ë¡œ ë‚˜ëˆ„ê¸°
            subspace_size = len(vec) // num_subspaces
            pq_codes = []

            for i in range(num_subspaces):
                start_idx = i * subspace_size
                end_idx = (i + 1) * subspace_size if i < num_subspaces - 1 else len(vec)
                subvec = vec[start_idx:end_idx]

                # K-means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ì½”ë“œë¶ ì°¾ê¸°
                code = np.argmin(np.abs(subvec - np.mean(subvec)))
                pq_codes.append(int(code % 256))  # 256 í´ëŸ¬ìŠ¤í„°

            quantized.append(np.array(pq_codes, dtype=np.uint8))
            assignments.append(pq_codes)

        metadata = {
            "method": "product_quantization",
            "num_subspaces": num_subspaces,
            "compression_ratio": 2.0,
        }

        return quantized, metadata

    def dequantize_vectors(
        self,
        quantized_vectors: list[np.ndarray],
        metadata: dict[str, Any],
    ) -> list[np.ndarray]:
        """ì–‘ìí™”ëœ ë²¡í„° ë³µì›."""
        method = metadata.get("method", "none")

        if method == "none":
            return quantized_vectors
        elif method == "quantization_int8":
            return self._dequantize_int8(quantized_vectors, metadata)
        elif method == "quantization_int4":
            return self._dequantize_int4(quantized_vectors, metadata)

        return quantized_vectors

    def _dequantize_int8(
        self,
        quantized_vectors: list[np.ndarray],
        metadata: dict,
    ) -> list[np.ndarray]:
        """INT8 ë³µì›."""
        scales = metadata.get("scales", [])
        offsets = metadata.get("offsets", [])

        dequantized = []
        for q_vec, scale, offset in zip(
            quantized_vectors, scales, offsets, strict=False
        ):
            vec = (q_vec.astype(np.float32) * scale) + offset
            dequantized.append(vec)

        return dequantized

    def _dequantize_int4(
        self,
        quantized_vectors: list[np.ndarray],
        metadata: dict,
    ) -> list[np.ndarray]:
        """INT4 ë³µì›."""
        scales = metadata.get("scales", [])
        offsets = metadata.get("offsets", [])

        dequantized = []
        for q_vec, scale, offset in zip(
            quantized_vectors, scales, offsets, strict=False
        ):
            vec = (q_vec.astype(np.float32) * scale) + offset
            dequantized.append(vec)

        return dequantized


class DocumentPruner:
    """ì¤‘ë³µ/ìœ ì‚¬ ë¬¸ì„œ ì œê±°ê¸°."""

    def __init__(self, min_similarity: float = 0.95):
        self.min_similarity = min_similarity
        self._lock = RLock()

    def prune_similar_documents(
        self,
        documents: list[Document],
        vectors: list[np.ndarray] | None = None,
    ) -> tuple[list[Document], list[int]]:
        """
        [ìµœì í™”] ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ì œê±° (NumPy ë²¡í„°í™” ì—°ì‚° ë²„ì „).

        ê¸°ì¡´ O(N^2) ë£¨í”„ë¥¼ ì œê±°í•˜ê³  í–‰ë ¬ ì—°ì‚°(Matrix Multiplication)ì„ ì‚¬ìš©í•˜ì—¬
        ìˆ˜ì²œ ê°œì˜ ë¬¸ì„œë¥¼ 0.1ì´ˆ ë‚´ì— ë¹„êµ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Returns:
            (ì œê±° í›„ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ì œê±°ëœ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸)
        """
        if not documents or vectors is None or len(vectors) == 0:
            return documents, []

        with self._lock:
            try:
                # 1. ë²¡í„° í–‰ë ¬í™” ë° ì •ê·œí™”
                v_matrix = np.array(vectors)
                # L2 Norm ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
                norms = np.linalg.norm(v_matrix, axis=1, keepdims=True)
                v_norm = v_matrix / np.where(norms == 0, 1e-10, norms)

                # 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° (N x N)
                # sim_matrix[i, j]ëŠ” ië²ˆì§¸ì™€ jë²ˆì§¸ ë¬¸ì„œ ê°„ì˜ ìœ ì‚¬ë„
                sim_matrix = np.dot(v_norm, v_norm.T)

                # 3. ì¤‘ë³µ ë° ìê¸° ìì‹  ë¹„êµ ì œì™¸ (ìƒì‚¼ê° í–‰ë ¬ë§Œ ì‚¬ìš©)
                # k=1 ì„¤ì •ìœ¼ë¡œ ëŒ€ê° ì„±ë¶„(ìê¸° ìì‹ , í•­ìƒ 1.0)ë„ ì œì™¸
                sim_matrix = np.triu(sim_matrix, k=1)

                # 4. ì„ê³„ê°’(min_similarity)ì„ ì´ˆê³¼í•˜ëŠ” ì¤‘ë³µ ìŒ ì°¾ê¸°
                # í–‰(i)ê³¼ ì—´(j) ì¸ë±ìŠ¤ ì¤‘ 'ì—´(j)' ì¸ë±ìŠ¤ê°€ ë’¤ì— ë‚˜ì˜¤ëŠ” ì¤‘ë³µ ë¬¸ì„œì„
                row_indices, col_indices = np.where(sim_matrix >= self.min_similarity)

                # 5. ì œê±°í•  ì¸ë±ìŠ¤ í™•ì • (ì¤‘ë³µëœ ì—´ ì¸ë±ìŠ¤ë“¤)
                removed_indices = sorted(set(col_indices.tolist()))
                kept_indices = [
                    i for i in range(len(documents)) if i not in removed_indices
                ]

                pruned_docs = [documents[i] for i in kept_indices]

                logger.info(
                    f"[Pruner] ìµœì í™” ì™„ë£Œ: {len(documents)} -> {len(pruned_docs)} "
                    f"({len(removed_indices)}ê°œ ì¤‘ë³µ ì œê±°)"
                )

                return pruned_docs, removed_indices

            except Exception as e:
                logger.error(f"[Pruner] ë²¡í„°í™” ì—°ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ê¸°ë³¸ ë°˜í™˜): {e}")
                return documents, []

    def _cosine_similarity(self, vec_a: np.ndarray, vec_b: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°."""
        norm_a = np.linalg.norm(vec_a)
        norm_b = np.linalg.norm(vec_b)

        if norm_a == 0 or norm_b == 0:
            return 0.0

        return np.dot(vec_a, vec_b) / (norm_a * norm_b)


class MetadataIndexer:
    """ë©”íƒ€ë°ì´í„° ì¸ë±ì‹± ì—”ì§„."""

    def __init__(self):
        self.indexes: dict[str, dict[Any, set[int]]] = {}
        self._lock = RLock()

    def build_indexes(self, documents: list[Document]):
        """ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ êµ¬ì¶•."""
        with self._lock:
            self.indexes.clear()

            # ëª¨ë“  ë©”íƒ€ë°ì´í„° í•„ë“œ ì¸ë±ì‹±
            for doc_idx, doc in enumerate(documents):
                for key, value in doc.metadata.items():
                    if key not in self.indexes:
                        self.indexes[key] = {}

                    if value not in self.indexes[key]:
                        self.indexes[key][value] = set()

                    self.indexes[key][value].add(doc_idx)

    def search_by_metadata(
        self,
        field: str,
        value: Any,
    ) -> set[int]:
        """ë©”íƒ€ë°ì´í„°ë¡œ ë¬¸ì„œ ê²€ìƒ‰."""
        with self._lock:
            if field not in self.indexes:
                return set()

            if value not in self.indexes[field]:
                return set()

            return self.indexes[field][value].copy()

    def get_stats(self) -> dict[str, int]:
        """ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ í†µê³„."""
        with self._lock:
            return {field: len(values) for field, values in self.indexes.items()}


class LRUVectorCache:
    """LRU ë²¡í„° ìºì‹œ."""

    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self.cache: dict[str, np.ndarray] = {}
        self.access_order: list[str] = []
        self._lock = RLock()

    def get(self, key: str) -> np.ndarray | None:
        """ìºì‹œì—ì„œ ë²¡í„° ì¡°íšŒ."""
        with self._lock:
            if key not in self.cache:
                return None

            # LRU ìˆœì„œ ì—…ë°ì´íŠ¸
            self.access_order.remove(key)
            self.access_order.append(key)

            return self.cache[key].copy()

    def put(self, key: str, vector: np.ndarray):
        """ìºì‹œì— ë²¡í„° ì €ì¥."""
        with self._lock:
            if key in self.cache:
                self.access_order.remove(key)

            # ìš©ëŸ‰ ì´ˆê³¼ì‹œ LRU ì œê±°
            if len(self.cache) >= self.max_size and key not in self.cache:
                lru_key = self.access_order.pop(0)
                del self.cache[lru_key]

            self.cache[key] = vector.copy()
            self.access_order.append(key)

    def clear(self):
        """ìºì‹œ ì´ˆê¸°í™”."""
        with self._lock:
            self.cache.clear()
            self.access_order.clear()

    def get_stats(self) -> dict[str, Any]:
        """ìºì‹œ í†µê³„."""
        with self._lock:
            return {
                "size": len(self.cache),
                "max_size": self.max_size,
                "utilization": float(len(self.cache) / self.max_size)
                if self.max_size > 0
                else 0.0,
            }


class IndexOptimizer:
    """ì¸ë±ìŠ¤ ìµœì í™” í†µí•© ê´€ë¦¬ì."""

    def __init__(self, config: IndexOptimizationConfig):
        self.config = config
        self.quantizer = VectorQuantizer(config.quantization_config)
        self.pruner = DocumentPruner(config.min_doc_similarity)
        self.metadata_indexer = MetadataIndexer()
        self.vector_cache = (
            LRUVectorCache(config.max_cache_size) if config.enable_lru_cache else None
        )

        self._lock = RLock()
        self.stats = IndexStats()

    def optimize_index(
        self,
        documents: list[Document],
        vectors: list[np.ndarray],
    ) -> tuple[list[Document], list[np.ndarray], dict[str, Any], IndexStats]:
        """
        ì¸ë±ìŠ¤ ìµœì í™”.

        Returns:
            (ìµœì í™”ëœ ë¬¸ì„œ, ìµœì í™”ëœ ë²¡í„°, ì–‘ìí™” ë©”íƒ€ë°ì´í„°, í†µê³„)
        """
        self.stats = IndexStats(
            total_documents=len(documents),
            total_vectors=len(vectors),
        )

        # 1. ë¬¸ì„œ í”„ë£¨ë‹
        if self.config.enable_doc_pruning:
            documents, pruned_indices = self.pruner.prune_similar_documents(
                documents, vectors
            )
            vectors = [v for i, v in enumerate(vectors) if i not in pruned_indices]
            self.stats.pruned_documents = len(pruned_indices)

        # 2. ë²¡í„° ì–‘ìí™”
        original_size = sum(v.nbytes for v in vectors)
        vectors, quant_metadata = self.quantizer.quantize_vectors(vectors)
        compressed_size = sum(v.nbytes for v in vectors)

        self.stats.original_size_mb = original_size / (1024 * 1024)
        self.stats.compressed_size_mb = compressed_size / (1024 * 1024)
        self.stats.compression_ratio = (
            original_size / compressed_size if compressed_size > 0 else 1.0
        )

        # 3. ë©”íƒ€ë°ì´í„° ì¸ë±ì‹±
        if self.config.enable_metadata_indexing:
            self.metadata_indexer.build_indexes(documents)
            self.stats.num_unique_metadata_fields = len(self.metadata_indexer.indexes)

        return documents, vectors, quant_metadata, self.stats

    def search_with_metadata(
        self,
        field: str,
        value: Any,
        documents: list[Document],
    ) -> list[tuple[int, Document]]:
        """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰."""
        doc_indices = self.metadata_indexer.search_by_metadata(field, value)
        return [(idx, documents[idx]) for idx in sorted(doc_indices)]

    def get_stats(self) -> IndexStats:
        """í˜„ì¬ ì¸ë±ìŠ¤ í†µê³„."""
        return self.stats


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_optimizer_instance: IndexOptimizer | None = None


def get_index_optimizer(
    config: IndexOptimizationConfig | None = None,
) -> IndexOptimizer:
    """ì „ì—­ ì¸ë±ìŠ¤ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ."""
    global _optimizer_instance
    if _optimizer_instance is None:
        _optimizer_instance = IndexOptimizer(config or IndexOptimizationConfig())
    return _optimizer_instance


def reset_index_optimizer():
    """ì¸ë±ìŠ¤ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ë¦¬ì…‹ (í…ŒìŠ¤íŠ¸ìš©)."""
    global _optimizer_instance
    _optimizer_instance = None
