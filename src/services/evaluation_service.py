import logging
import time
from datetime import datetime

from langchain_ollama import ChatOllama
from ragas import EvaluationDataset, RunConfig, evaluate
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.llms import LangchainLLMWrapper

# [ì£¼ì˜] FaithfulnesswithHHEMì€ v0.4.3 ê¸°ì¤€ collectionsì— ëˆ„ë½ë˜ì–´ ìˆì–´ ì§ì ‘ ì„í¬íŠ¸
from ragas.metrics import FaithfulnesswithHHEM
from ragas.metrics.collections import AnswerRelevancy

from common.config import (
    EVAL_JUDGE_MODEL,
    EVAL_TIMEOUT,
    OLLAMA_BASE_URL,
    PROJECT_ROOT,
)
from core.model_loader import load_embedding_model

logger = logging.getLogger(__name__)


class EvaluationService:
    """RAG ì‹œìŠ¤í…œì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ (Ragas 0.4+ ê· í˜• ëª¨ë“œ)"""

    def __init__(self):
        self.report_dir = PROJECT_ROOT / "reports"
        self.report_dir.mkdir(exist_ok=True)

    def _setup_metrics(self):
        """í‰ê°€ ì§€í‘œ ì´ˆê¸°í™” (ê· í˜• ëª¨ë“œ: í• ë£¨ì‹œë„¤ì´ì…˜ + ë‹µë³€ ê´€ë ¨ì„±)"""
        # [ìµœì í™”] í‰ê°€ìš© LLM ì„¤ì • (ë©”ëª¨ë¦¬ ì ìœ ìœ¨ê³¼ ì»¨í…ìŠ¤íŠ¸ì˜ ê· í˜•)
        eval_llm = ChatOllama(
            model=EVAL_JUDGE_MODEL,
            base_url=OLLAMA_BASE_URL,
            timeout=EVAL_TIMEOUT,
            temperature=0,
            num_ctx=6144,  # 4096 -> 6144ë¡œ í™•ì¥í•˜ì—¬ ë” ë§ì€ ê·¼ê±° í™•ì¸ ê°€ëŠ¥
        )

        evaluator_llm = LangchainLLMWrapper(eval_llm)
        embedder = load_embedding_model()
        evaluator_embeddings = LangchainEmbeddingsWrapper(embedder)

        # [ìµœì í™”] FaithfulnesswithHHEM: LLM í˜¸ì¶œì„ ìµœì†Œí™”í•˜ëŠ” ë‹¨ì¼ ì§€í‘œ
        faithfulness = FaithfulnesswithHHEM(llm=evaluator_llm)

        # [ìµœì í™”] AnswerRelevancy: strictness=1ë¡œ ìì› íš¨ìœ¨ì  ê´€ë ¨ì„± í‰ê°€
        answer_relevancy = AnswerRelevancy(
            llm=evaluator_llm, embeddings=evaluator_embeddings, strictness=1
        )

        logger.info(
            "Successfully initialized balanced evaluation metrics (HHEM + Relevancy)."
        )
        return [faithfulness, answer_relevancy]

    async def run_evaluation(
        self, data_points: list[dict], report_prefix: str = "e2e_eval_report"
    ):
        """
        ë°ì´í„°ì…‹ì— ëŒ€í•´ ê· í˜• ì¡íŒ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        logger.info(f"Starting balanced evaluation for {len(data_points)} cases...")

        # 1. ë°ì´í„°ì…‹ êµ¬ì„±
        eval_data = []
        for d in data_points:
            raw_context = d.get("context", "")

            # [ìµœì í™”] ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ í™•ì¥ (ìƒìœ„ 5ê°œ ì²­í¬ë¡œ ì¦ì„¤í•˜ì—¬ ì •í™•ë„ í–¥ìƒ)
            if isinstance(raw_context, str) and "### [ìë£Œ" in raw_context:
                chunks = [
                    c.strip() for c in raw_context.split("### [ìë£Œ") if c.strip()
                ]
                processed_contexts = [f"### [ìë£Œ {c}" for c in chunks[:5]]
            elif isinstance(raw_context, list):
                processed_contexts = raw_context[:5]
            else:
                processed_contexts = [str(raw_context)[:3000]]

            eval_data.append(
                {
                    "user_input": d["query"],
                    "response": d["response"],
                    "retrieved_contexts": processed_contexts,
                }
            )

        dataset = EvaluationDataset.from_list(eval_data)
        metrics = self._setup_metrics()

        # 2. í‰ê°€ ì‹¤í–‰
        start_time = time.time()
        # [ì•ˆì •ì„±] max_workers=1ë¡œ ìˆœì°¨ ì‹¤í–‰ ìœ ì§€í•˜ì—¬ ì‹œìŠ¤í…œ ë©ˆì¶¤ ë°©ì§€
        run_config = RunConfig(timeout=EVAL_TIMEOUT, max_workers=1)

        results = evaluate(dataset=dataset, metrics=metrics, run_config=run_config)

        duration = time.time() - start_time
        logger.info(f"Evaluation finished in {duration:.2f}s")

        # 3. ë¦¬í¬íŠ¸ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.report_dir / f"{report_prefix}_{timestamp}.md"

        summary = results.to_pandas().mean(numeric_only=True).to_dict()

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(
                f"# RAG Evaluation Report ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n\n"
            )
            f.write(f"**Judge Model:** {EVAL_JUDGE_MODEL}\n")
            f.write(
                "**Optimization:** Balanced Mode (HHEM + Relevancy, Sequential, context-5)\n"
            )
            f.write(f"**Duration:** {duration:.2f}s\n\n")

            f.write("## ğŸ“Š Summary Scores\n\n")
            for m, s in summary.items():
                f.write(f"- **{m}:** {s:.4f}\n")

            f.write("\n## ğŸ” Detailed Analysis\n\n")
            f.write(results.to_pandas().to_markdown(index=False))

        logger.info(f"Report saved to: {report_path}")
        return summary, str(report_path)
