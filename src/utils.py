import os
import torch
import time
import subprocess
import logging
import functools
from typing import List, Optional, Dict
import streamlit as st

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain

from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# ëª¨ë¸ ë° ì„¤ì • ìƒìˆ˜
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ìƒìˆ˜
RETRIEVER_CONFIG: Dict = {
    'search_type': "similarity",
    'search_kwargs': {
        'k': 5,
    }
}

# í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
TEXT_SPLITTER_CONFIG: Dict = {
    'chunk_size': 4000,
    'chunk_overlap': 200,
}

class SessionManager:
    """ì„¸ì…˜ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    # ì„¸ì…˜ ìƒíƒœì˜ ê¸°ë³¸ê°’ì„ í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ì •ì˜
    DEFAULT_SESSION_STATE = {
        "messages": [],
        "last_selected_model": None,
        "last_uploaded_file_name": None,
        "pdf_processed": False,
        "pdf_processing_error": None,
        "qa_chain": None,
        "vector_store": None,
        "llm": None,
        "temp_pdf_path": None,
        "pdf_is_processing": False,
        "processing_step": None,
        "source_documents": {}, # source_documents ì´ˆê¸°í™” ì¶”ê°€
        "processed_document_splits": None, # ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ ì €ì¥ìš©
    }
    
    @classmethod
    def init_session(cls):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” - í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨"""
        if not st.session_state.get("_initialized", False):
            logging.info("ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì¤‘...")
            for key, value in cls.DEFAULT_SESSION_STATE.items():
                if key not in st.session_state:
                    st.session_state[key] = value
            st.session_state._initialized = True
    
    @classmethod
    def reset_session_state(cls, keys=None):
        """ì§€ì •ëœ í‚¤ë“¤ì˜ ì„¸ì…˜ ìƒíƒœë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ë¦¬ì…‹"""
        keys_to_reset = keys if keys is not None else cls.DEFAULT_SESSION_STATE.keys()
        for key in keys_to_reset:
            if key in cls.DEFAULT_SESSION_STATE:
                st.session_state[key] = cls.DEFAULT_SESSION_STATE[key]
    
    @classmethod
    def reset_for_new_file(cls, uploaded_file):
        """ìƒˆ íŒŒì¼ ì—…ë¡œë“œì‹œ ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹"""
        logging.info("ìƒˆ íŒŒì¼ ì—…ë¡œë“œë¡œ ì¸í•œ ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹ ì¤‘...")
        
        # ëª¨ë¸ ë³€ê²½ ë©”ì‹œì§€ ì €ì¥
        last_model_change_message = st.session_state.get("last_model_change_message")
        
        file_related_keys = [
            "last_uploaded_file_name",
            "pdf_processed",
            "pdf_processing_error",
            "qa_chain",
            "vector_store",
            "pdf_is_processing",
            "processing_step",
            "messages",
            "processed_document_splits" # ì¶”ê°€
        ]
        cls.reset_session_state(file_related_keys)
        st.session_state.last_uploaded_file_name = uploaded_file.name
        
        # ëª¨ë¸ ë³€ê²½ ë©”ì‹œì§€ ë³µì›
        if last_model_change_message:
            st.session_state.last_model_change_message = last_model_change_message
            cls.add_message("assistant", last_model_change_message)
    
    @classmethod
    def add_message(cls, role: str, content: str):
        """ë©”ì‹œì§€ ì¶”ê°€"""
        if not st.session_state.get("messages"):
            st.session_state.messages = []
        st.session_state.messages.append({"role": role, "content": content})
    
    @classmethod
    def update_progress(cls, step: str, message: str):
        """ì²˜ë¦¬ ë‹¨ê³„ ì—…ë°ì´íŠ¸ ë° ì§„í–‰ ìƒí™© ë©”ì‹œì§€ í‘œì‹œ"""
        st.session_state.processing_step = step
        cls.add_message("assistant", f"ğŸ”„ {message}")
    
    @staticmethod
    def is_ready_for_chat():
        """ì±„íŒ… ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
        return (st.session_state.get("pdf_processed") and 
                not st.session_state.get("pdf_processing_error") and 
                st.session_state.get("qa_chain") is not None)
    
    @classmethod
    def update_model(cls, new_model: str):
        """ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        old_model = st.session_state.get("last_selected_model", "N/A")
        model_related_keys = ["last_selected_model", "llm", "qa_chain"]
        cls.reset_session_state(model_related_keys)
        st.session_state.last_selected_model = new_model
        
        cls.add_message(
            "assistant", 
            f"ğŸ”„ ëª¨ë¸ì„ {old_model}ì—ì„œ {new_model}ë¡œ ë³€ê²½í•˜ëŠ” ì¤‘..."
        )
        return old_model

    @classmethod
    def handle_error(cls, error: Exception, error_context: str, affected_states: list = None):
        """ì—ëŸ¬ ì²˜ë¦¬ ë° ìƒíƒœ ì—…ë°ì´íŠ¸"""
        error_msg = f"{error_context}: {str(error)}"
        logging.error(error_msg, exc_info=True)
        
        if affected_states:
            cls.reset_session_state(affected_states)
            
        cls.add_message("assistant", f"âŒ {error_msg}")
        return error_msg
    
    @classmethod
    def set_error_state(cls, error_message: str, error_context: str = None):
        """ì—ëŸ¬ ìƒíƒœ ì„¤ì •"""
        st.session_state.pdf_processing_error = error_message
        if error_context:
            logging.error(f"{error_context}: {error_message}")
        cls.add_message("assistant", f"âŒ {error_message}")
    
    @classmethod
    def clear_error_state(cls):
        """ì—ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”"""
        st.session_state.pdf_processing_error = None

# ë¡œê¹… ë°ì½”ë ˆì´í„° ìˆ˜ì •
def log_operation(operation_name):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            logging.info(f"{operation_name} ì‹œì‘...")
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                logging.info(f"{operation_name} ì™„ë£Œ (ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
                return result
            except Exception as e:
                logging.error(f"{operation_name} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                raise
        return wrapper
    return decorator

@st.cache_data(show_spinner=False)
@log_operation("Ollama ëª¨ë¸ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°")
def get_ollama_models() -> List[str]:
    result = subprocess.run(["ollama", "list"], capture_output=True, text=True, check=True)
    return [line.split()[0] for line in result.stdout.split("\n")[1:] if line]

@st.cache_resource(show_spinner=False)
@log_operation("PDF íŒŒì¼ ë¡œë“œ")
def load_pdf_docs(pdf_file_path: str) -> List:
    if not os.path.exists(pdf_file_path):
        raise FileNotFoundError(f"PDF íŒŒì¼ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_file_path}")
    loader = PyMuPDFLoader(
        pdf_file_path,
        mode="page",
        )
    return loader.load()

@st.cache_resource(show_spinner=False)
@log_operation("ì„ë² ë”© ëª¨ë¸ ë¡œë”©")
def load_embedding_model() -> HuggingFaceEmbeddings:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logging.info(f"ì„ë² ë”© ëª¨ë¸ìš© ì¥ì¹˜: {device}")
    embedder = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={
            "device": device,
            "trust_remote_code": False,
            },
        encode_kwargs={
            "device": device,
            "batch_size": 64,
            "normalize_embeddings": True,
            },
    )
    return embedder

@st.cache_data(show_spinner=False)
@log_operation("ë¬¸ì„œ ë¶„í• ")
def split_documents(_docs: List) -> List:
    chunker = RecursiveCharacterTextSplitter(
        chunk_size=TEXT_SPLITTER_CONFIG['chunk_size'],
        chunk_overlap=TEXT_SPLITTER_CONFIG['chunk_overlap'],
        length_function=len,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
        is_separator_regex=False,
        add_start_index=True,
    )
    return chunker.split_documents(_docs)

@st.cache_resource(show_spinner=False)
@log_operation("FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±")
def create_vector_store(_documents, _embedder) -> Optional[FAISS]:
    return FAISS.from_documents(
        documents=_documents,
        embedding=_embedder,
    )

@st.cache_resource(show_spinner=False)
@log_operation("Ollama LLM ë¡œë”©")
def load_llm(model_name: str) -> OllamaLLM:
    return OllamaLLM(
        model=model_name,
        num_predict=-1,
        )

# QA í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ìˆ˜ ì™¸ë¶€ì—ì„œ ì •ì˜í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ import ê°€ëŠ¥í•˜ê²Œ í•¨
QA_PROMPT = ChatPromptTemplate.from_messages([
    ("system", (
     """
     ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì§€ì‹ì´ë‚˜ ì •ë³´ë¥¼ ì‚¬ìš©í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

     **ì»¨í…ìŠ¤íŠ¸**
     {context}

     **ë‹µë³€ ìƒì„± ì§€ì¹¨**
     1.  **ì–¸ì–´** ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ë™ì¼í•œ ì–¸ì–´ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
     2.  **ë‹µë³€ í˜•ì‹**
        - ë‹µë³€ì€ ë°˜ë“œì‹œ ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
        - ë‹µë³€ ë‚´ìš©ì´ ì—¬ëŸ¬ í•­ëª©, ë‹¨ê³„ ë˜ëŠ” ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±ë  ê²½ìš°, ë§ˆí¬ë‹¤ìš´ì˜ ì¤„ ë°”ê¿ˆ(ì˜ˆ: ë¹ˆ ì¤„ ì‚½ì…)ì´ë‚˜ ëª©ë¡(ìˆ«ì ëª©ë¡, ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ëª©ë¡)ì„ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì—¬ì•¼ í•©ë‹ˆë‹¤.
        - ê° ì •ë³´ ë‹¨ìœ„ê°€ ëª…í™•íˆ êµ¬ë¶„ë˜ë„ë¡ í‘œí˜„í•´ì•¼ í•©ë‹ˆë‹¤.
     """
        )),
    ("human", "Question: {input}")
    ])


def update_qa_chain(llm, vector_store):
    """QA ì²´ì¸ ì—…ë°ì´íŠ¸"""
    try:
        # í—¬í¼ í•¨ìˆ˜ ì •ì˜ (st.session_state ì§ì ‘ ì ‘ê·¼ ì œê±°)
        def add_doc_number_to_metadata(docs: List[Dict]) -> List[Dict]:
            """
            ê²€ìƒ‰ëœ ê° ë¬¸ì„œì— 'doc_number' ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ê³ ,
            í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            st.session_state ì ‘ê·¼ì€ ì´ í•¨ìˆ˜ ì™¸ë¶€ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            """
            # ì´ í•¨ìˆ˜ëŠ” ìˆœìˆ˜í•˜ê²Œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€/ìˆ˜ì •í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
            # st.session_state.source_documents ê´€ë ¨ ë¡œì§ì€ í˜¸ì¶œí•˜ëŠ” ìª½(ë©”ì¸ ìŠ¤ë ˆë“œ)ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            for i, doc in enumerate(docs, 1):
                doc.metadata["doc_number"] = i # ë‚˜ì¤‘ì— document_promptì—ì„œ ì‚¬ìš©

                # í˜ì´ì§€ ë²ˆí˜¸ ì²˜ë¦¬: 0-indexedë¥¼ 1-indexed ë¬¸ìì—´ë¡œ ë³€í™˜ ë˜ëŠ” 'N/A'
                # PyMuPDFLoaderëŠ” 'page' ë©”íƒ€ë°ì´í„°ë¥¼ 0-indexed ì •ìˆ˜ë¡œ ì œê³µ
                page_number_raw = doc.metadata.get('page')
                if page_number_raw is not None:
                    try:
                        # Ensure page_number_raw is an integer before arithmetic operation
                        current_page_int = int(page_number_raw)
                        doc.metadata['page'] = str(current_page_int + 1) # Convert to 1-indexed string
                    except ValueError:
                        # Handle cases where page_number_raw is a string that cannot be converted to int
                        logging.warning(f"Could not convert page metadata '{page_number_raw}' to an integer. Setting page to 'N/A'.")
                        doc.metadata['page'] = 'N/A'
                else:
                    doc.metadata['page'] = 'N/A'
            return docs

        def rename_documents_key(data_dict: Dict) -> Dict:
            """'processed_documents' í‚¤ë¥¼ 'documents'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤."""
            if "processed_documents" in data_dict:
                data_dict["documents"] = data_dict.pop("processed_documents")
            return data_dict

        # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        faiss_retriever = vector_store.as_retriever(
            search_type=RETRIEVER_CONFIG['search_type'],
            search_kwargs=RETRIEVER_CONFIG['search_kwargs']
        )

        # BM25 ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • (ë¶„í• ëœ ë¬¸ì„œê°€ ì„¸ì…˜ì— ì €ì¥ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
        final_retriever = faiss_retriever # ê¸°ë³¸ê°’ì€ FAISS ë¦¬íŠ¸ë¦¬ë²„
        if st.session_state.get("processed_document_splits"):
            try:
                bm25_retriever = BM25Retriever.from_documents(
                    st.session_state.processed_document_splits
                )
                bm25_retriever.k = RETRIEVER_CONFIG['search_kwargs'].get('k', 5) # FAISSì™€ ë™ì¼í•œ k ì‚¬ìš©

                # EnsembleRetriever ì„¤ì • (ê°€ì¤‘ì¹˜ëŠ” ì‹¤í—˜ì„ í†µí•´ ì¡°ì •)
                final_retriever = EnsembleRetriever(
                    retrievers=[bm25_retriever, faiss_retriever],
                    weights=[0.4, 0.6] 
                )
                logging.info("EnsembleRetriever (BM25 + FAISS) ì‚¬ìš©.")
            except Exception as e:
                logging.warning(f"BM25 ë¦¬íŠ¸ë¦¬ë²„ ë˜ëŠ” EnsembleRetriever ìƒì„± ì‹¤íŒ¨: {e}. FAISS ë¦¬íŠ¸ë¦¬ë²„ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            logging.info("ë¶„í• ëœ ë¬¸ì„œê°€ ì—†ì–´ FAISS ë¦¬íŠ¸ë¦¬ë²„ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        # ê° ë¬¸ì„œë¥¼ LLM í”„ë¡¬í”„íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ ë¶€ë¶„ì— ë§ê²Œ í¬ë§·íŒ…í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸
        # add_doc_number_to_metadata_and_save í•¨ìˆ˜ì—ì„œ doc.metadataì— 'doc_number'ì™€ 'page'ê°€ ì„¤ì •ë¨
        document_prompt = PromptTemplate.from_template(
            "[{doc_number}] {page_content} (p.{page})"
        )

        # LLMì— ìµœì¢…ì ìœ¼ë¡œ ì „ë‹¬ë  í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²°í•© ì²´ì¸ ìƒì„±
        # create_stuff_documents_chainì€ QA_PROMPTì˜ {context}ë¥¼ document_promptë¡œ í¬ë§·ëœ ë¬¸ì„œë“¤ë¡œ ì±„ì›€
        combine_docs_chain = create_stuff_documents_chain(
            llm=llm,
            prompt=QA_PROMPT,
            document_prompt=document_prompt,
            document_separator="\n\n",
            document_variable_name="context" # Explicitly use "context" from QA_PROMPT
        )

        # LCELì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ RAG ì²´ì¸ êµ¬ì„±
        # 1. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” -> 2. ì…ë ¥ í†µê³¼ ë° ë¬¸ì„œ ê²€ìƒ‰/ì²˜ë¦¬ -> 3. í‚¤ ì´ë¦„ ë³€ê²½ -> 4. LLM í˜¸ì¶œ
        # retrieverê°€ ë¬¸ìì—´ ì…ë ¥ì„ ë°›ë„ë¡ RunnableLambdaë¥¼ ì‚¬ìš©í•˜ì—¬ 'input' í‚¤ì˜ ê°’ì„ ì¶”ì¶œ
        retrieval_chain_with_processing = RunnablePassthrough.assign(
            processed_documents=RunnableLambda(lambda x: x["input"]) # 'input' í‚¤ì˜ ê°’ë§Œ final_retrieverë¡œ ì „ë‹¬
                                | final_retriever # FAISS ë˜ëŠ” Ensemble ë¦¬íŠ¸ë¦¬ë²„
                                | RunnableLambda(add_doc_number_to_metadata) # st.session_state ì ‘ê·¼ ì œê±°
        )

        final_qa_chain = (
            # RunnableLambda(init_source_docs_and_pass_input) # st.session_state ì ‘ê·¼ ì œê±°
            RunnablePassthrough() # ì…ë ¥: {"input": "question"}
            | retrieval_chain_with_processing # ì¶œë ¥: {"input": "question", "processed_documents": [docs_with_metadata]}
            # Ensure the key for documents matches what combine_docs_chain expects via QA_PROMPT's {context}
            | RunnableLambda(lambda x: {"input": x["input"], "context": x.pop("processed_documents")}) 
            # ì´ì „ rename_documents_key ëŒ€ì‹  contextë¡œ ì§ì ‘ ë§¤í•‘
            # | RunnableLambda(rename_documents_key) # ì¶œë ¥: {"input": "question", "documents": [docs_with_metadata]}
            | combine_docs_chain # ì…ë ¥: {"input": "question", "documents": [docs]}, ì¶œë ¥: LLM ë‹µë³€ ë¬¸ìì—´ (ìŠ¤íŠ¸ë¦¬ë° ì‹œ ì²­í¬)
        )
        return final_qa_chain

    except Exception as e:
        raise ValueError(f"QA ì²´ì¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def process_pdf(uploaded_file, selected_model: str, temp_pdf_path: str):
    """PDF ì²˜ë¦¬ ë° QA ì²´ì¸ ìƒì„±."""
    try:
        # ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.pdf_is_processing = True
        st.session_state.pdf_processed = False
        st.session_state.qa_chain = None
        
        # ê° ë‹¨ê³„ ì²˜ë¦¬
        docs = load_pdf_docs(temp_pdf_path)
        embedder = load_embedding_model()
        documents = split_documents(docs)
        st.session_state.processed_document_splits = documents # ë¶„í• ëœ ë¬¸ì„œ ì €ì¥
        vector_store = create_vector_store(documents, embedder)
        llm = load_llm(selected_model)
        
        # QA ì²´ì¸ ìƒì„±
        qa_chain = update_qa_chain(llm, vector_store)
        
        # ì„¸ì…˜ ìƒíƒœ í•œë²ˆì— ì—…ë°ì´íŠ¸
        st.session_state.update({
            'vector_store': vector_store,
            'llm': llm,
            'qa_chain': qa_chain,
            'pdf_processed': True,
            'pdf_processing_error': None
        })
        
        # ì„±ê³µ ë©”ì‹œì§€
        success_message = (
            f"âœ… '{uploaded_file.name}' ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
            "ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ë“¤ì„ í•´ë³´ì„¸ìš”:\n\n"
            "[ë¬¸ì„œ ì „ì²´ ì´í•´í•˜ê¸°]\n"
            "- ì´ ë¬¸ì„œë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”\n"
            "- ì´ ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì¥ê³¼ ê·¼ê±°ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n"
            "- ì´ ë¬¸ì„œì˜ í•µì‹¬ ìš©ì–´ 3ê°€ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n\n"
            "[ì„¸ë¶€ ë‚´ìš© íŒŒì•…í•˜ê¸°]\n"
            "- ì´ ë¬¸ì„œê°€ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n"
            "- ë¬¸ì„œì—ì„œ ì œì‹œëœ í•´ê²°ì±…ì´ë‚˜ ì œì•ˆì€ ë¬´ì—‡ì¸ê°€ìš”?\n"
            "- ì´ ì—°êµ¬ì˜ í•œê³„ì ì´ë‚˜ í–¥í›„ ì—°êµ¬ ë°©í–¥ì€ ë¬´ì—‡ì¸ê°€ìš”?\n\n"
            "ììœ ë¡­ê²Œ ë¬¸ì„œì˜ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
        )
        SessionManager.add_message("assistant", success_message)

    except Exception as e:
        SessionManager.handle_error(
            error=e,
            error_context="PDF ì²˜ë¦¬",
            affected_states=['pdf_processed', 'qa_chain', 'vector_store', 'llm']
        )
        raise
    finally:
        st.session_state.pdf_is_processing = False
        st.session_state.processing_step = None