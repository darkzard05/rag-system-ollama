import os
import torch
import time
import subprocess
import logging
import functools
from typing import List, Optional, Dict
import streamlit as st

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain

from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# ëª¨ë¸ ë° ì„¤ì • ìƒìˆ˜
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ìƒìˆ˜
RETRIEVER_CONFIG: Dict = {
    'search_type': "similarity",
    'search_kwargs': {
        'k': 5,
    }
}
# í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
TEXT_SPLITTER_CONFIG: Dict = {
    'chunk_size': 4000,
    'chunk_overlap': 200,
}

class SessionManager:
    """ì„¸ì…˜ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
      # ì„¸ì…˜ ìƒíƒœì˜ ê¸°ë³¸ê°’ì„ í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ì •ì˜
    DEFAULT_SESSION_STATE = {
        # ê¸°ë³¸ ìƒíƒœ
        "model_update_initiated_message": None, # ëª¨ë¸ ë³€ê²½ ì‹œì‘ ë©”ì‹œì§€
        "messages": [],
        "last_selected_model": None,
        "last_uploaded_file_name": None,
        "last_model_change_message": None,
        
        # PDF ì²˜ë¦¬ ìƒíƒœ
        "pdf_processed": False,
        "pdf_processing_error": None,
        "pdf_is_processing": False,
        "temp_pdf_path": None,
        "processing_step": None,
        
        # ë¬¸ì„œ ì²˜ë¦¬ ê´€ë ¨
        "processed_document_splits": None,
        "source_documents": {},
        "current_file_path": None,
        
        # LLM ë° ê²€ìƒ‰ ê´€ë ¨
        "qa_chain": None,
        "vector_store": None,
        "llm": None,
        "bm25_retriever": None,
        
        # ìºì‹œ ê´€ë ¨
        "_faiss_index": None,
        "_pdf_text_cache": None,
        "last_retriever_key": None,
    }
    
    # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ ì‹œ ë³´ì¡´í•  ì„¸ì…˜ ìƒíƒœ í‚¤ ëª©ë¡
    PRESERVE_ON_NEW_FILE_KEYS = [
        "_initialized",
        "last_selected_model",
        "model_update_initiated_message",
        "last_model_change_message"
    ]
    
    @classmethod
    def init_session(cls):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” - í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨"""
        if not st.session_state.get("_initialized", False):
            logging.info("ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì¤‘...")
            for key, value in cls.DEFAULT_SESSION_STATE.items():
                if key not in st.session_state:
                    st.session_state[key] = value
            st.session_state._initialized = True
    
    @classmethod
    def reset_session_state(cls, keys=None):
        """ì§€ì •ëœ í‚¤ë“¤ì˜ ì„¸ì…˜ ìƒíƒœë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ë¦¬ì…‹"""
        keys_to_reset = keys if keys is not None else cls.DEFAULT_SESSION_STATE.keys()
        for key in keys_to_reset:
            if key in cls.DEFAULT_SESSION_STATE:
                st.session_state[key] = cls.DEFAULT_SESSION_STATE[key]
    @classmethod
    def reset_for_new_file(cls, uploaded_file: str):
        """ìƒˆ íŒŒì¼ ì—…ë¡œë“œì‹œ ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹"""
        logging.info("ìƒˆ íŒŒì¼ ì—…ë¡œë“œë¡œ ì¸í•œ ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹ ì¤‘...")
        
        # ë³´ì¡´í•  ìƒíƒœ ì €ì¥
        preserved_states = {
            "model_update_initiated_message": st.session_state.get("model_update_initiated_message"),
            "last_model_change_message": st.session_state.get("last_model_change_message"),
            "last_selected_model": st.session_state.get("last_selected_model"),
            "_initialized": st.session_state.get("_initialized", False)
        }
        
        # ëª¨ë“  ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë³´ì¡´í•  ìƒíƒœ ì œì™¸)
        exclude_keys = ["last_selected_model", "_initialized"]
        all_keys = [key for key in st.session_state.keys() if key not in exclude_keys]
        cls.reset_session_state(all_keys)
        
        # ìƒˆ íŒŒì¼ ì •ë³´ ì„¤ì •
        st.session_state.last_uploaded_file_name = uploaded_file.name
        st.session_state.current_file_path = None  # ìƒˆë¡œìš´ ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì„¤ì •ë¨
        
        # 1. ë³´ì¡´ëœ ì„¸ì…˜ ìƒíƒœ ê°’ ë³µì›
        if preserved_states.get("model_update_initiated_message") is not None:
            st.session_state.model_update_initiated_message = preserved_states["model_update_initiated_message"]
        if preserved_states.get("last_model_change_message") is not None:
            st.session_state.last_model_change_message = preserved_states["last_model_change_message"]
        if preserved_states.get("last_selected_model") is not None:
            st.session_state.last_selected_model = preserved_states["last_selected_model"]
        st.session_state._initialized = preserved_states.get("_initialized", False)

        # 2. ë³´ì¡´ëœ ë©”ì‹œì§€ë“¤ì„ (ì´ˆê¸°í™”ëœ) messages ëª©ë¡ì— ìˆœì„œëŒ€ë¡œ ë‹¤ì‹œ ì¶”ê°€
        # ëª¨ë¸ ë³€ê²½ ì‹œì‘ ë©”ì‹œì§€ ì¶”ê°€
        initiated_msg = st.session_state.get("model_update_initiated_message")
        if initiated_msg:
            cls.add_message("assistant", initiated_msg)
        
        # ëª¨ë¸ ë³€ê²½ ì™„ë£Œ/ê²°ê³¼ ë©”ì‹œì§€ ì¶”ê°€ (ì‹œì‘ ë©”ì‹œì§€ì™€ ë‹¤ë¥¼ ê²½ìš°ì—ë§Œ)
        completed_msg = st.session_state.get("last_model_change_message")
        if completed_msg and completed_msg != initiated_msg:
            cls.add_message("assistant", completed_msg)
            
        # ì´ˆê¸°í™” ì™„ë£Œ ë¡œê¹…
        logging.info(f"ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ - ìƒˆ íŒŒì¼: {uploaded_file.name}")
    
    @classmethod
    def add_message(cls, role: str, content: str):
        """ë©”ì‹œì§€ ì¶”ê°€"""
        if not st.session_state.get("messages"):
            st.session_state.messages = []
        st.session_state.messages.append({"role": role, "content": content})
    
    @classmethod
    def update_progress(cls, step: str, message: str):
        """ì²˜ë¦¬ ë‹¨ê³„ ì—…ë°ì´íŠ¸ ë° ì§„í–‰ ìƒí™© ë©”ì‹œì§€ í‘œì‹œ"""
        st.session_state.processing_step = step
        cls.add_message("assistant", f"ğŸ”„ {message}")
    
    @staticmethod
    def is_ready_for_chat():
        """ì±„íŒ… ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
        return (st.session_state.get("pdf_processed") and 
                not st.session_state.get("pdf_processing_error") and 
                st.session_state.get("qa_chain") is not None)
    
    @classmethod
    def update_model(cls, new_model: str):
        """ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        old_model = st.session_state.get("last_selected_model", "N/A")
        model_related_keys = ["last_selected_model", "llm", "qa_chain"]
        cls.reset_session_state(model_related_keys)
        st.session_state.last_selected_model = new_model
        
        model_update_msg = f"ğŸ”„ ëª¨ë¸ì„ {new_model}ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤."
        cls.add_message("assistant", model_update_msg)
        st.session_state.model_update_initiated_message = model_update_msg # ì‹œì‘ ë©”ì‹œì§€ ì €ì¥
        return old_model

    @classmethod
    def handle_error(cls, error: Exception, error_context: str, affected_states: list = None):
        """ì—ëŸ¬ ì²˜ë¦¬ ë° ìƒíƒœ ì—…ë°ì´íŠ¸"""
        error_msg = f"{error_context}: {str(error)}"
        logging.error(error_msg, exc_info=True)
        
        if affected_states:
            cls.reset_session_state(affected_states)
            
        cls.add_message("assistant", f"âŒ {error_msg}")
        return error_msg
    
    @classmethod
    def set_error_state(cls, error_message: str, error_context: str = None):
        """ì—ëŸ¬ ìƒíƒœ ì„¤ì •"""
        st.session_state.pdf_processing_error = error_message
        if error_context:
            logging.error(f"{error_context}: {error_message}")
        cls.add_message("assistant", f"âŒ {error_message}")
    
    @classmethod
    def clear_error_state(cls):
        """ì—ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”"""
        st.session_state.pdf_processing_error = None
    
    @classmethod
    def get_file_specific_cache_key(cls, base_key: str) -> str:
        """íŒŒì¼ë³„ ê³ ìœ  ìºì‹œ í‚¤ ìƒì„±"""
        current_file = st.session_state.get('current_file_path', '')
        current_model = st.session_state.get('last_selected_model', '')
        return f"{base_key}_{current_file}_{current_model}"

# ë¡œê¹… ë°ì½”ë ˆì´í„° ìˆ˜ì •
def log_operation(operation_name):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            logging.info(f"{operation_name} ì‹œì‘...")
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                logging.info(f"{operation_name} ì™„ë£Œ (ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
                return result
            except Exception as e:
                logging.error(f"{operation_name} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                raise
        return wrapper
    return decorator

@st.cache_data(show_spinner=False)
@log_operation("Ollama ëª¨ë¸ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°")
def get_ollama_models() -> List[str]:
    result = subprocess.run(["ollama", "list"], capture_output=True, text=True, check=True)
    return [line.split()[0] for line in result.stdout.split("\n")[1:] if line]

@log_operation("PDF íŒŒì¼ ë¡œë“œ")
def load_pdf_docs(pdf_file_path: str) -> List:
    if not os.path.exists(pdf_file_path):
        raise FileNotFoundError(f"PDF íŒŒì¼ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_file_path}")
    loader = PyMuPDFLoader(
        pdf_file_path,
        mode="page",
        )
    return loader.load()

@st.cache_resource(show_spinner=False)
@log_operation("ì„ë² ë”© ëª¨ë¸ ë¡œë”©")
def load_embedding_model() -> HuggingFaceEmbeddings:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logging.info(f"ì„ë² ë”© ëª¨ë¸ìš© ì¥ì¹˜: {device}")
      # GPU ë©”ëª¨ë¦¬ ìµœì í™”
    if device == "cuda":
        # CUDA ìºì‹œ ì •ë¦¬
        torch.cuda.empty_cache()
        # ë©”ëª¨ë¦¬ í• ë‹¹ì ì„¤ì •
        torch.backends.cudnn.benchmark = True
        # GPU ë©”ëª¨ë¦¬ í• ë‹¹ì ìµœì í™”
        # ì´ ê°’ì€ ì‚¬ìš©ìì˜ GPU ë©”ëª¨ë¦¬ í¬ê¸° ë° ëª¨ë¸ì— ë”°ë¼ ì¡°ì •ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        torch.backends.cuda.max_split_size_mb = 512 
    
    # ëª¨ë¸ ì„¤ì • (SentenceTransformerëŠ” torch_dtypeë¥¼ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ)
    model_kwargs = {
        "device": device,
        "trust_remote_code": False,
    }
    
    # ì¸ì½”ë”© ì„¤ì • ìµœì í™”
    encode_kwargs = {
        "device": device,
        "batch_size": 128,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        "normalize_embeddings": True,
        "convert_to_numpy": True,  # numpy ë³€í™˜ ìµœì í™”
        "convert_to_tensor": False  # ë¶ˆí•„ìš”í•œ í…ì„œ ë³€í™˜ ë°©ì§€
    }
    
    embedder = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs,
        cache_folder=".model_cache"  # ëª¨ë¸ ìºì‹œ í´ë” ì§€ì •
    )
    
    # ì´ˆê¸° ì›Œë°ì—… ì‹¤í–‰
    try:
        logging.info("ì„ë² ë”© ëª¨ë¸ ì›Œë°ì—… ì‹¤í–‰ ì¤‘...")
        _ = embedder.embed_documents(["ì›Œë°ì—… í…ìŠ¤íŠ¸"])
        logging.info("ì„ë² ë”© ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
    except Exception as e:
        logging.warning(f"ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return embedder

@log_operation("ë¬¸ì„œ ë¶„í• ")
def split_documents(_docs: List) -> List:
    chunker = RecursiveCharacterTextSplitter(
        chunk_size=TEXT_SPLITTER_CONFIG['chunk_size'],
        chunk_overlap=TEXT_SPLITTER_CONFIG['chunk_overlap'],
        length_function=len,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
        is_separator_regex=False,
        add_start_index=True,
    )
    return chunker.split_documents(_docs)

@log_operation("FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±")
def create_vector_store(_documents, _embedder) -> Optional[FAISS]:
    return FAISS.from_documents(
        documents=_documents,
        embedding=_embedder,
    )
    
@log_operation("BM25 ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±")
def create_bm25_retriever(_documents: List, k: int) -> BM25Retriever:
    """ìºì‹œëœ BM25 ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë°˜í™˜í•©ë‹ˆë‹¤."""
    retriever = BM25Retriever.from_documents(_documents)
    retriever.k = k
    return retriever

@log_operation("Ensemble ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±")
def create_ensemble_retriever(_faiss_retriever: FAISS, _bm25_retriever: BM25Retriever, weights: List[float]):
    """ìºì‹œëœ Ensemble ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return EnsembleRetriever(
        retrievers=[_bm25_retriever, _faiss_retriever], weights=weights
    )

@st.cache_resource(show_spinner=False)
@log_operation("Ollama LLM ë¡œë”©")
def load_llm(model_name: str) -> OllamaLLM:
    return OllamaLLM(
        model=model_name,
        num_predict=-1,
        )

# QA í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ìˆ˜ ì™¸ë¶€ì—ì„œ ì •ì˜í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ import ê°€ëŠ¥í•˜ê²Œ í•¨
QA_PROMPT = ChatPromptTemplate.from_messages([
    ("system", (
     """
     ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
     ë‹¤ë¥¸ ì§€ì‹ì´ë‚˜ ì •ë³´ë¥¼ ì‚¬ìš©í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

     **ì»¨í…ìŠ¤íŠ¸**
     {context}

     **ë‹µë³€ ìƒì„± ì§€ì¹¨**
     1.  **ì–¸ì–´** ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ë™ì¼í•œ ì–¸ì–´ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
     2.  **ë‹µë³€ í˜•ì‹**
        - ë‹µë³€ì€ ë°˜ë“œì‹œ ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
        - ë‹µë³€ ë‚´ìš©ì´ ì—¬ëŸ¬ í•­ëª©, ë‹¨ê³„ ë˜ëŠ” ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±ë  ê²½ìš°,
        ë§ˆí¬ë‹¤ìš´ì˜ ì¤„ ë°”ê¿ˆ(ì˜ˆ: ë¹ˆ ì¤„ ì‚½ì…)ì´ë‚˜ ëª©ë¡(ìˆ«ì ëª©ë¡, ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ëª©ë¡)ì„ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì—¬ì•¼ í•©ë‹ˆë‹¤.
        - ê° ì •ë³´ ë‹¨ìœ„ê°€ ëª…í™•íˆ êµ¬ë¶„ë˜ë„ë¡ í‘œí˜„í•´ì•¼ í•©ë‹ˆë‹¤.
     """
        )),
    ("human", "Question: {input}")
    ])

# í—¬í¼ í•¨ìˆ˜ ì •ì˜ (st.session_state ì§ì ‘ ì ‘ê·¼ ì œê±°)
def add_doc_number_to_metadata(docs: List[Dict]) -> List[Dict]:
    """
    ê²€ìƒ‰ëœ ê° ë¬¸ì„œì— 'doc_number' ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ê³ ,
    í˜ì´ì§€ ë²ˆí˜¸ë¥¼ 1-indexed ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ìˆœìˆ˜ í•¨ìˆ˜ë¡œ, st.session_stateì— ì§ì ‘ ì ‘ê·¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    LLM ì‘ë‹µ í›„ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ê¸° ìœ„í•´ st.session_state.source_documents ë“±ì„ í™œìš©í•˜ëŠ” ë¡œì§ì€
    ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” UI ì¸¡ ì½”ë“œì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    # ì´ í•¨ìˆ˜ëŠ” ìˆœìˆ˜í•˜ê²Œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€/ìˆ˜ì •í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    # st.session_state.source_documents ê´€ë ¨ ë¡œì§ì€ í˜¸ì¶œí•˜ëŠ” ìª½(ë©”ì¸ ìŠ¤ë ˆë“œ)ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    for i, doc in enumerate(docs, 1):
        doc.metadata["doc_number"] = i

        # í˜ì´ì§€ ë²ˆí˜¸ ì²˜ë¦¬: 0-indexedë¥¼ 1-indexed ë¬¸ìì—´ë¡œ ë³€í™˜ ë˜ëŠ” 'N/A'
        # PyMuPDFLoaderëŠ” 'page' ë©”íƒ€ë°ì´í„°ë¥¼ 0-indexed ì •ìˆ˜ë¡œ ì œê³µ
        page_number_raw = doc.metadata.get('page')
        if page_number_raw is not None:
            try:
                # í˜ì´ì§€ ë²ˆí˜¸ê°€ ë¬¸ìì—´ë¡œ ë˜ì–´ ìˆì„ ê²½ìš° ì •ìˆ˜ë¡œ ë³€í™˜
                current_page_int = int(page_number_raw)
                doc.metadata['page'] = str(current_page_int + 1) # Convert to 1-indexed string
            except ValueError:
                # í˜ì´ì§€ ë²ˆí˜¸ê°€ ì •ìˆ˜ë¡œ ë³€í™˜ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
                logging.warning(f"Could not convert page metadata '{page_number_raw}' to an integer. Setting page to 'N/A'.")
                doc.metadata['page'] = 'N/A'
        else:
            doc.metadata['page'] = 'N/A'
    return docs

def update_qa_chain(llm, vector_store):
    """QA ì²´ì¸ ì—…ë°ì´íŠ¸"""
    try:
        # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        faiss_retriever = vector_store.as_retriever(
            search_type=RETRIEVER_CONFIG['search_type'],
            search_kwargs=RETRIEVER_CONFIG['search_kwargs']
        )
        
        # BM25 ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • (ë¶„í• ëœ ë¬¸ì„œê°€ ì„¸ì…˜ì— ì €ì¥ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
        final_retriever = faiss_retriever # ê¸°ë³¸ê°’ì€ FAISS ë¦¬íŠ¸ë¦¬ë²„
        
        if st.session_state.get("processed_document_splits"):
            try:
                # BM25 ë° Ensemble ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìœ„í•œ k ê°’ (FAISSì™€ ë™ì¼í•˜ê²Œ ì„¤ì •)
                k_val = RETRIEVER_CONFIG['search_kwargs'].get('k', 5)

                # ìºì‹œëœ BM25 ë¦¬íŠ¸ë¦¬ë²„ ì‚¬ìš©
                bm25_retriever_instance = create_bm25_retriever(
                    _documents=st.session_state.processed_document_splits,
                    k=k_val
                )
                
                # ìºì‹œëœ Ensemble ë¦¬íŠ¸ë¦¬ë²„ ì‚¬ìš©
                final_retriever = create_ensemble_retriever(
                    _faiss_retriever=faiss_retriever,
                    _bm25_retriever=bm25_retriever_instance,
                    weights=[0.4, 0.6]  # ì´ ê°€ì¤‘ì¹˜ëŠ” ì„¤ì •ìœ¼ë¡œ ê´€ë¦¬ ê°€ëŠ¥
                )
                logging.info("EnsembleRetriever (BM25 + FAISS) ìƒì„± ë° ì‚¬ìš©.")
            except Exception as e:
                logging.warning(f"BM25 ë¦¬íŠ¸ë¦¬ë²„ ë˜ëŠ” EnsembleRetriever ìƒì„± ì‹¤íŒ¨: {e}. FAISS ë¦¬íŠ¸ë¦¬ë²„ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            logging.info("ë¶„í• ëœ ë¬¸ì„œê°€ ì—†ì–´ FAISS ë¦¬íŠ¸ë¦¬ë²„ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        # ê° ë¬¸ì„œë¥¼ LLM í”„ë¡¬í”„íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ ë¶€ë¶„ì— ë§ê²Œ í¬ë§·íŒ…í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸
        # add_doc_number_to_metadata_and_save í•¨ìˆ˜ì—ì„œ doc.metadataì— 'doc_number'ì™€ 'page'ê°€ ì„¤ì •ë¨
        document_prompt = PromptTemplate.from_template(
            "[{doc_number}] {page_content} (p.{page})"
        )

        # LLMì— ìµœì¢…ì ìœ¼ë¡œ ì „ë‹¬ë  í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²°í•© ì²´ì¸ ìƒì„±
        # create_stuff_documents_chainì€ QA_PROMPTì˜ {context}ë¥¼ document_promptë¡œ í¬ë§·ëœ ë¬¸ì„œë“¤ë¡œ ì±„ì›€
        combine_docs_chain = create_stuff_documents_chain(
            llm=llm,
            prompt=QA_PROMPT,
            document_prompt=document_prompt,
            document_separator="\n\n",
            document_variable_name="context" # Explicitly use "context" from QA_PROMPT
        )

        # LCELì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ RAG ì²´ì¸ êµ¬ì„±
        # 1. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” -> 2. ì…ë ¥ í†µê³¼ ë° ë¬¸ì„œ ê²€ìƒ‰/ì²˜ë¦¬ -> 3. í‚¤ ì´ë¦„ ë³€ê²½ -> 4. LLM í˜¸ì¶œ
        # retrieverê°€ ë¬¸ìì—´ ì…ë ¥ì„ ë°›ë„ë¡ RunnableLambdaë¥¼ ì‚¬ìš©í•˜ì—¬ 'input' í‚¤ì˜ ê°’ì„ ì¶”ì¶œ
        retrieval_chain_with_processing = RunnablePassthrough.assign(
            processed_documents=RunnableLambda(lambda x: x["input"]) # 'input' í‚¤ì˜ ê°’ë§Œ final_retrieverë¡œ ì „ë‹¬
                                | final_retriever # FAISS ë˜ëŠ” Ensemble ë¦¬íŠ¸ë¦¬ë²„
                                | RunnableLambda(add_doc_number_to_metadata) # st.session_state ì ‘ê·¼ ì œê±°
        )

        final_qa_chain = (
            RunnablePassthrough() # ì…ë ¥: {"input": "question"}
            | retrieval_chain_with_processing # ì¶œë ¥: {"input": "question", "processed_documents": [docs_with_metadata]}
            | RunnableLambda(lambda x: {"input": x["input"], "context": x.pop("processed_documents")}) 
            | combine_docs_chain # ì…ë ¥: {"input": "question", "documents": [docs]}, ì¶œë ¥: LLM ë‹µë³€ ë¬¸ìì—´ (ìŠ¤íŠ¸ë¦¬ë° ì‹œ ì²­í¬)
        )
        return final_qa_chain

    except Exception as e:
        raise ValueError(f"QA ì²´ì¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def process_pdf(uploaded_file, selected_model: str, temp_pdf_path: str):
    """PDF ì²˜ë¦¬ ë° QA ì²´ì¸ ìƒì„±."""
    try:
        # ìƒíƒœ ë° ìºì‹œ ì´ˆê¸°í™”
        st.session_state.pdf_is_processing = True
        st.session_state.pdf_processed = False
        st.session_state.qa_chain = None
        
        # í˜„ì¬ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        st.session_state.current_file_path = temp_pdf_path
        logging.info(f"PDF ì²˜ë¦¬ ì‹œì‘: {temp_pdf_path}")

        # PDF íŒŒì¼ ì €ì¥
        docs = load_pdf_docs(temp_pdf_path)
        
        embedder = load_embedding_model()
        documents = split_documents(docs)
        st.session_state.processed_document_splits = documents # ë¶„í• ëœ ë¬¸ì„œ ì €ì¥
        vector_store = create_vector_store(documents, embedder)
        llm = load_llm(selected_model)
        
        # QA ì²´ì¸ ìƒì„±
        qa_chain = update_qa_chain(llm, vector_store)
        
        # ì„¸ì…˜ ìƒíƒœ í•œë²ˆì— ì—…ë°ì´íŠ¸
        st.session_state.update({
            'vector_store': vector_store,
            'llm': llm,
            'qa_chain': qa_chain,
            'pdf_processed': True,
            'pdf_processing_error': None
        })
        
        # ì„±ê³µ ë©”ì‹œì§€
        success_message = (
            f"âœ… '{uploaded_file.name}' ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
            "ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ë“¤ì„ í•´ë³´ì„¸ìš”:\n\n"
            "- ì´ ë¬¸ì„œë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”\n"
            "- ì´ ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì¥ê³¼ ê·¼ê±°ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n"
            "- ì´ ë¬¸ì„œì˜ í•µì‹¬ ìš©ì–´ 3ê°€ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n\n"
            "ììœ ë¡­ê²Œ ë¬¸ì„œì˜ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
        )
        SessionManager.add_message("assistant", success_message)

    except Exception as e:
        SessionManager.handle_error(
            error=e,
            error_context="PDF ì²˜ë¦¬",
            affected_states=['pdf_processed', 'qa_chain', 'vector_store', 'llm']
        )
        raise
    finally:
        st.session_state.pdf_is_processing = False
        st.session_state.processing_step = None