import os
import torch
import time
import subprocess
import logging
import functools
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
import streamlit as st
from typing import List, Optional, Dict

# ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ìƒìˆ˜
RETRIEVER_CONFIG: Dict = {
    'search_type': "mmr",
    'search_kwargs': {
        'k': 5,           # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ìµœì í™”
        'fetch_k': 20,    # í›„ë³´ ìˆ˜ ì¦ê°€
        'lambda_mult': 0.8 # MMR ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜ ì¦ê°€
    }
}

class SessionManager:
    """ì„¸ì…˜ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    # ì„¸ì…˜ ìƒíƒœì˜ ê¸°ë³¸ê°’ì„ í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ì •ì˜
    DEFAULT_SESSION_STATE = {
        "messages": [],
        "last_selected_model": None,
        "last_uploaded_file_name": None,
        "pdf_processed": False,
        "pdf_processing_error": None,
        "qa_chain": None,
        "vector_store": None,
        "llm": None,
        "temp_pdf_path": None,
        "pdf_is_processing": False,
        "processing_step": None
    }
    
    @classmethod
    def init_session(cls):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” - í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨"""
        if not st.session_state.get("_initialized", False):
            logging.info("ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì¤‘...")
            for key, value in cls.DEFAULT_SESSION_STATE.items():
                if key not in st.session_state:
                    st.session_state[key] = value
            st.session_state._initialized = True
    
    @classmethod
    def reset_session_state(cls, keys=None):
        """ì§€ì •ëœ í‚¤ë“¤ì˜ ì„¸ì…˜ ìƒíƒœë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ë¦¬ì…‹"""
        keys_to_reset = keys if keys is not None else cls.DEFAULT_SESSION_STATE.keys()
        for key in keys_to_reset:
            if key in cls.DEFAULT_SESSION_STATE:
                st.session_state[key] = cls.DEFAULT_SESSION_STATE[key]
    
    @classmethod
    def reset_for_new_file(cls, uploaded_file):
        """ìƒˆ íŒŒì¼ ì—…ë¡œë“œì‹œ ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹"""
        logging.info("ìƒˆ íŒŒì¼ ì—…ë¡œë“œë¡œ ì¸í•œ ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹ ì¤‘...")
        file_related_keys = [
            "last_uploaded_file_name",
            "pdf_processed",
            "pdf_processing_error",
            "qa_chain",
            "vector_store",
            "pdf_is_processing",
            "processing_step",
            "messages"
        ]
        cls.reset_session_state(file_related_keys)
        st.session_state.last_uploaded_file_name = uploaded_file.name
        
        # Streamlit ìºì‹œ ì´ˆê¸°í™”
        st.cache_data.clear()
        st.cache_resource.clear()
    
    @classmethod
    def add_message(cls, role: str, content: str):
        """ë©”ì‹œì§€ ì¶”ê°€"""
        if not st.session_state.get("messages"):
            st.session_state.messages = []
        st.session_state.messages.append({"role": role, "content": content})
    
    @classmethod
    def update_progress(cls, step: str, message: str):
        """ì²˜ë¦¬ ë‹¨ê³„ ì—…ë°ì´íŠ¸ ë° ì§„í–‰ ìƒí™© ë©”ì‹œì§€ í‘œì‹œ"""
        st.session_state.processing_step = step
        cls.add_message("assistant", f"ğŸ”„ {message}")
    
    @staticmethod
    def is_ready_for_chat():
        """ì±„íŒ… ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
        return (st.session_state.get("pdf_processed") and 
                not st.session_state.get("pdf_processing_error") and 
                st.session_state.get("qa_chain") is not None)
    
    @classmethod
    def update_model(cls, new_model: str):
        """ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        old_model = st.session_state.get("last_selected_model", "N/A")
        model_related_keys = ["last_selected_model", "llm", "qa_chain"]
        cls.reset_session_state(model_related_keys)
        st.session_state.last_selected_model = new_model
        
        cls.add_message(
            "assistant", 
            f"ğŸ”„ ëª¨ë¸ì„ {new_model}ë¡œ ë³€ê²½í•©ë‹ˆë‹¤."
        )
        return old_model

    @classmethod
    def handle_error(cls, error: Exception, error_context: str, affected_states: list = None):
        """ì—ëŸ¬ ì²˜ë¦¬ ë° ìƒíƒœ ì—…ë°ì´íŠ¸"""
        error_msg = f"{error_context}: {str(error)}"
        logging.error(error_msg, exc_info=True)
        
        if affected_states:
            cls.reset_session_state(affected_states)
            
        cls.add_message("assistant", f"âŒ {error_msg}")
        return error_msg
    
    @classmethod
    def set_error_state(cls, error_message: str, error_context: str = None):
        """ì—ëŸ¬ ìƒíƒœ ì„¤ì •"""
        st.session_state.pdf_processing_error = error_message
        if error_context:
            logging.error(f"{error_context}: {error_message}")
        cls.add_message("assistant", f"âŒ {error_message}")
    
    @classmethod
    def clear_error_state(cls):
        """ì—ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”"""
        st.session_state.pdf_processing_error = None

# ë¡œê¹… ë°ì½”ë ˆì´í„° ìˆ˜ì •
def log_operation(operation_name):
    def decorator(func):
        @functools.wraps(func)  # í•¨ìˆ˜ ë©”íƒ€ë°ì´í„° ë³´ì¡´
        def wrapper(*args, **kwargs):
            logging.info(f"{operation_name} ì‹œì‘...")
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                logging.info(f"{operation_name} ì™„ë£Œ (ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
                return result
            except Exception as e:
                logging.error(f"{operation_name} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                raise
        return wrapper
    return decorator

# Streamlit ìºì‹œ ë°ì½”ë ˆì´í„°ë¥¼ í•­ìƒ ë°”ê¹¥ìª½ì— ë°°ì¹˜
@st.cache_data(show_spinner=False)
@log_operation("Ollama ëª¨ë¸ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°")
def get_ollama_models() -> List[str]:
    result = subprocess.run(["ollama", "list"], capture_output=True, text=True, check=True)
    return [line.split()[0] for line in result.stdout.split("\n")[1:] if line]

@st.cache_resource(show_spinner=False)
@log_operation("PDF íŒŒì¼ ë¡œë“œ")
def load_pdf_docs(pdf_file_path: str) -> List:
    if not os.path.exists(pdf_file_path):
        raise FileNotFoundError(f"PDF íŒŒì¼ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_file_path}")
    loader = PyMuPDFLoader(pdf_file_path)
    return loader.load()

@st.cache_resource(show_spinner=False)
@log_operation("ì„ë² ë”© ëª¨ë¸ ë¡œë”©")
def load_embedding_model() -> HuggingFaceEmbeddings:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logging.info(f"ì„ë² ë”© ëª¨ë¸ìš© ì¥ì¹˜: {device}")
    embedder = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': device, 'trust_remote_code': True},
        encode_kwargs={'device': device, 'batch_size': 32},
    )
    return embedder

@st.cache_data(show_spinner=False)
@log_operation("ë¬¸ì„œ ë¶„í• ")
def split_documents(_docs: List) -> List:
    chunker = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
        is_separator_regex=False,
        add_start_index=True,
    )
    return chunker.split_documents(_docs)

@st.cache_resource(show_spinner=False)
@log_operation("FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±")
def create_vector_store(_documents, _embedder) -> Optional[FAISS]:
    return FAISS.from_documents(
        documents=_documents,
        embedding=_embedder,
    )

@st.cache_resource(show_spinner=False)
@log_operation("Ollama LLM ë¡œë”©")
def load_llm(model_name: str) -> OllamaLLM:
    return OllamaLLM(model=model_name)

# QA í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ìˆ˜ ì™¸ë¶€ì—ì„œ ì •ì˜í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ import ê°€ëŠ¥í•˜ê²Œ í•¨
QA_PROMPT = ChatPromptTemplate.from_messages([
    ("system", (
        "You are an AI assistant. Your primary task is to answer questions based *solely* on the provided 'Context'.\n\n"
        "**CRITICAL: You MUST respond in the EXACT same language as the user's question.** This is the most important instruction.\n\n"
        "Context:\n"
        "{context}\n"
        "Follow these instructions carefully:\n"
        "1. **Language of Response:**\n"
        "   - ALWAYS use the same language as the user's question for your entire response.\n"
        "   - For example, if the question is in Korean, your answer MUST be in Korean. If the question is in English, your answer MUST be in English.\n\n"
        "2. **Answer Formulation:**\n"
        "   - Construct your answer based *strictly* on the information found within the 'Context'.\n"
        "   - Your answer should be clear and detailed.\n"
        "   - Do NOT use any external knowledge or information not present in the 'Context'.\n\n"
        "3. **Handling Missing Information:**\n"
        "   - If the 'Context' does not contain the information to answer the question, you MUST state (in the same language as the question) that the information is not available in the provided document.\n"
        "   - Do not invent an answer or use external knowledge."
        )),
    ("human", "Question: {input}")
    ])

def process_pdf(uploaded_file, selected_model: str, temp_pdf_path: str):
    """PDF ì²˜ë¦¬ ë° QA ì²´ì¸ ìƒì„±."""
    try:
        # ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.pdf_is_processing = True
        st.session_state.pdf_processed = False
        st.session_state.qa_chain = None
        
        # ê° ë‹¨ê³„ ì²˜ë¦¬
        docs = load_pdf_docs(temp_pdf_path)
        embedder = load_embedding_model()
        documents = split_documents(docs)
        vector_store = create_vector_store(documents, embedder)
        llm = load_llm(selected_model)
        
        # QA ì²´ì¸ ìƒì„±
        combine_chain = create_stuff_documents_chain(llm, QA_PROMPT)
        retriever = vector_store.as_retriever(
            search_type=RETRIEVER_CONFIG['search_type'],
            search_kwargs=RETRIEVER_CONFIG['search_kwargs']
        )
        qa_chain = create_retrieval_chain(retriever, combine_chain)
        
        # ì„¸ì…˜ ìƒíƒœ í•œë²ˆì— ì—…ë°ì´íŠ¸
        st.session_state.update({
            'vector_store': vector_store,
            'llm': llm,
            'qa_chain': qa_chain,
            'pdf_processed': True,
            'pdf_processing_error': None
        })
        
        # ì„±ê³µ ë©”ì‹œì§€
        success_message = (
            f"âœ… '{uploaded_file.name}' ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
            "ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ë“¤ì„ í•´ë³´ì„¸ìš”:\n\n"
            "[ë¬¸ì„œ ì „ì²´ ì´í•´í•˜ê¸°]\n"
            "- ì´ ë¬¸ì„œë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”\n"
            "- ì´ ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì¥ê³¼ ê·¼ê±°ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n"
            "- ì´ ë¬¸ì„œì˜ í•µì‹¬ ìš©ì–´ 3ê°€ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n\n"
            "[ì„¸ë¶€ ë‚´ìš© íŒŒì•…í•˜ê¸°]\n"
            "- ì´ ë¬¸ì„œê°€ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n"
            "- ë¬¸ì„œì—ì„œ ì œì‹œëœ í•´ê²°ì±…ì´ë‚˜ ì œì•ˆì€ ë¬´ì—‡ì¸ê°€ìš”?\n"
            "- ì´ ì—°êµ¬ì˜ í•œê³„ì ì´ë‚˜ í–¥í›„ ì—°êµ¬ ë°©í–¥ì€ ë¬´ì—‡ì¸ê°€ìš”?\n\n"
            "ììœ ë¡­ê²Œ ë¬¸ì„œì˜ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
        )
        SessionManager.add_message("assistant", success_message)

    except Exception as e:
        SessionManager.handle_error(
            error=e,
            error_context="PDF ì²˜ë¦¬",
            affected_states=['pdf_processed', 'qa_chain', 'vector_store', 'llm']
        )
        raise
    finally:
        st.session_state.pdf_is_processing = False
        st.session_state.processing_step = None
    
    st.rerun()