import os
import time
import tempfile
import subprocess
import logging
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM
from langchain_core.messages import HumanMessage, AIMessage
import streamlit as st

from typing import List, Optional, Dict, Any


def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜"""
    logging.info("ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì¤‘...")
    defaults = {
        "messages": [],
        "last_selected_model": None,
        "last_uploaded_file_name": None,
        "pdf_processed": False,
        "pdf_processing_error": None,
        "qa_chain": None,
        "vector_store": None,
        "llm": None,
        "temp_pdf_path": None # ì„ì‹œ PDF íŒŒì¼ ê²½ë¡œ ì €ì¥ìš©
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value
            
def reset_session_state(uploaded_file):
    """ì„¸ì…˜ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    st.session_state.last_uploaded_file_name = uploaded_file.name
    st.session_state.pdf_processed = False
    st.session_state.pdf_processing_error = None
    st.session_state.qa_chain = None
    st.session_state.vector_store = None
    st.session_state.messages = []  # ìƒˆ íŒŒì¼ì´ë¯€ë¡œ ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
    load_pdf_docs.clear()
    get_embedder.clear()
    split_documents.clear()
    create_vector_store.clear()
    init_llm.clear()

def prepare_chat_history():
    """ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì¤€ë¹„í•©ë‹ˆë‹¤."""
    chat_history = []
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            chat_history.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "assistant":
            chat_history.append(AIMessage(content=msg["content"]))
    return chat_history

def generate_example_questions():
    """ì˜ˆì‹œ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        with st.spinner("ğŸ’¡ ë¬¸ì„œ ê¸°ë°˜ ì˜ˆì‹œ ì§ˆë¬¸ ìƒì„± ì¤‘..."):
            logging.info("ì˜ˆì‹œ ì§ˆë¬¸ ìƒì„± ì‹œì‘...")
            example_question_prompt = "ì´ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•  ë§Œí•œ í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ 5ê°€ì§€ë¥¼ í•œêµ­ì–´ë¡œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”. ì§ˆë¬¸ë§Œ ëª©ë¡ í˜•íƒœë¡œ ì œì‹œí•´ ì£¼ì„¸ìš”."
            chat_history = prepare_chat_history()
            response = st.session_state.qa_chain.invoke({
                "input": example_question_prompt,
                "chat_history": chat_history
            })
            example_questions = response.get("answer", "âš ï¸ ì˜ˆì‹œ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
            st.session_state.messages.append({
                "role": "assistant",
                "content": f"ğŸ’¡ ë‹¤ìŒì€ ì´ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³¼ ìˆ˜ ìˆëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤:\n\n{example_questions}"
            })
            logging.info("ì˜ˆì‹œ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ.")
    except Exception as e:
        logging.warning(f"ì˜ˆì‹œ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.session_state.messages.append({
            "role": "assistant",
            "content": "âš ï¸ ì˜ˆì‹œ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        })

@st.cache_data(show_spinner=False)
def get_ollama_models() -> List[str]:
    """Ollama ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    logging.info("Ollama ëª¨ë¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True, check=True)
        models = [line.split()[0] for line in result.stdout.split("\n")[1:] if line]
        return models
    except Exception as e:
        logging.error(f"Ollama ëª¨ë¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"Ollama ëª¨ë¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") from e

@st.cache_data(show_spinner=False)
def load_pdf_docs(file_path) -> List:
    """PDF íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    logging.info("PDF íŒŒì¼ ë¡œë“œ ì¤‘...")
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(file_path)
            temp_path = tmp_file.name
        loader = PyMuPDFLoader(temp_path)
        docs = loader.load()
        os.remove(temp_path)
        return docs
    except Exception as e:
        logging.error(f"PDF ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"PDF ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") from e

@st.cache_resource(show_spinner=False)
def get_embedder(model_name, model_kwargs=None, encode_kwargs=None) -> HuggingFaceEmbeddings:
    """HuggingFaceEmbeddings ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜"""
    try:
        return HuggingFaceEmbeddings(model_name=model_name,
                                    model_kwargs=model_kwargs,
                                    encode_kwargs=encode_kwargs)
    except Exception as e:
        logging.error(f"ì„ë² ë” ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"ì„ë² ë” ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") from e

@st.cache_data(show_spinner=False)
def split_documents(_docs: List, _embedder) -> List:
    """ë¬¸ì„œë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜"""
    logging.info("ë¬¸ì„œ ë¶„í•  ì‹œì‘...")
    start_time = time.time()
    try:
        chunker = SemanticChunker(_embedder)
        docs = chunker.split_documents(_docs)
        logging.info(f"ë¬¸ì„œ {len(docs)} í˜ì´ì§€ ë¶„í•  ì™„ë£Œ (ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
        return docs
    except Exception as e:
        logging.error(f"ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") from e

@st.cache_resource(show_spinner=False)
def create_vector_store(_documents, _embedder) -> Optional[FAISS]:
    """ë¬¸ì„œì—ì„œ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    logging.info("ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
    start_time = time.time()
    try:
        vector_space = FAISS.from_documents(_documents, _embedder)
        logging.info(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ (ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
        return vector_space
    except Exception as e:
        logging.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") from e

@st.cache_resource(show_spinner=False)
def init_llm(model_name) -> Optional[OllamaLLM]:
    """LLMì„ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜"""
    logging.info("LLM ì´ˆê¸°í™” ì¤‘...")
    try:
        return OllamaLLM(model=model_name, additional_settings={"output_format": "plain_text"})
    except Exception as e:
        logging.error(f"LLM ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"LLM ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") from e