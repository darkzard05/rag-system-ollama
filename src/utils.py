import os
import torch
import time
import subprocess
import logging
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
import streamlit as st
from typing import List, Optional, Dict

# ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ìƒìˆ˜
RETRIEVER_CONFIG: Dict = {
    'search_type': "mmr",
    'search_kwargs': {
        'k': 5,           # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ìµœì í™”
        'fetch_k': 20,    # í›„ë³´ ìˆ˜ ì¦ê°€
        'lambda_mult': 0.8 # MMR ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜ ì¦ê°€
    }
}

class SessionManager:
    """ì„¸ì…˜ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    # ì„¸ì…˜ ìƒíƒœì˜ ê¸°ë³¸ê°’ì„ í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ì •ì˜
    DEFAULT_SESSION_STATE = {
        "messages": [],
        "last_selected_model": None,
        "last_uploaded_file_name": None,
        "pdf_processed": False,
        "pdf_processing_error": None,
        "qa_chain": None,
        "vector_store": None,
        "llm": None,
        "temp_pdf_path": None,
        "pdf_is_processing": False,
        "processing_step": None,
        "needs_rerun": False  # ë¦¬ëŸ° í•„ìš” ì—¬ë¶€ë¥¼ ì¶”ì í•˜ëŠ” ìƒˆë¡œìš´ ìƒíƒœ
    }
    
    @classmethod
    def init_session(cls):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” - í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨"""
        if not st.session_state.get("_initialized", False):
            logging.info("ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì¤‘...")
            for key, value in cls.DEFAULT_SESSION_STATE.items():
                if key not in st.session_state:
                    st.session_state[key] = value
            st.session_state._initialized = True
    
    @classmethod
    def reset_session_state(cls, keys=None):
        """ì§€ì •ëœ í‚¤ë“¤ì˜ ì„¸ì…˜ ìƒíƒœë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ë¦¬ì…‹"""
        keys_to_reset = keys if keys is not None else cls.DEFAULT_SESSION_STATE.keys()
        for key in keys_to_reset:
            if key in cls.DEFAULT_SESSION_STATE:
                st.session_state[key] = cls.DEFAULT_SESSION_STATE[key]
    
    @classmethod
    def reset_for_new_file(cls, uploaded_file):
        """ìƒˆ íŒŒì¼ ì—…ë¡œë“œì‹œ ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹"""
        logging.info("ìƒˆ íŒŒì¼ ì—…ë¡œë“œë¡œ ì¸í•œ ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹ ì¤‘...")
        file_related_keys = [
            "last_uploaded_file_name",
            "pdf_processed",
            "pdf_processing_error",
            "qa_chain",
            "vector_store",
            "pdf_is_processing",
            "processing_step",
            "messages"
        ]
        cls.reset_session_state(file_related_keys)
        st.session_state.last_uploaded_file_name = uploaded_file.name
        
        # ìºì‹œëœ í•¨ìˆ˜ë“¤ ì´ˆê¸°í™”
        load_pdf_docs.clear()
        split_documents.clear()
        create_vector_store.clear()
        cls.request_rerun()
    
    @classmethod
    def add_message(cls, role: str, content: str, replace_last: bool = False):
        """ë©”ì‹œì§€ ì¶”ê°€ ë˜ëŠ” ë§ˆì§€ë§‰ ë©”ì‹œì§€ êµì²´"""
        if replace_last and st.session_state.messages:
            st.session_state.messages[-1] = {"role": role, "content": content}
        else:
            st.session_state.messages.append({"role": role, "content": content})
        cls.request_rerun()
    
    @classmethod
    def update_progress(cls, step: str, message: str):
        """ì²˜ë¦¬ ë‹¨ê³„ ì—…ë°ì´íŠ¸ ë° ì§„í–‰ ìƒí™© ë©”ì‹œì§€ í‘œì‹œ"""
        st.session_state.processing_step = step
        cls.add_message("assistant", f"ğŸ”„ {message}", replace_last=True)
    
    @staticmethod
    def is_ready_for_chat():
        """ì±„íŒ… ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
        return (st.session_state.get("pdf_processed") and 
                not st.session_state.get("pdf_processing_error") and 
                st.session_state.get("qa_chain") is not None)
    
    @classmethod
    def update_model(cls, new_model: str):
        """ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        old_model = st.session_state.get("last_selected_model", "N/A")
        model_related_keys = ["last_selected_model", "llm", "qa_chain"]
        cls.reset_session_state(model_related_keys)
        st.session_state.last_selected_model = new_model
        
        cls.add_message(
            "assistant", 
            f"ğŸ”„ ëª¨ë¸ì„ {new_model}ë¡œ ë³€ê²½í•©ë‹ˆë‹¤."
        )
        return old_model

    @classmethod
    def request_rerun(cls):
        """ë¦¬ëŸ°ì´ í•„ìš”í•¨ì„ í‘œì‹œ"""
        st.session_state.needs_rerun = True

    @classmethod
    def check_and_clear_rerun(cls):
        """ë¦¬ëŸ°ì´ í•„ìš”í•œì§€ í™•ì¸í•˜ê³  ìƒíƒœë¥¼ ì´ˆê¸°í™”"""
        needs_rerun = st.session_state.get("needs_rerun", False)
        st.session_state.needs_rerun = False
        return needs_rerun

    @classmethod
    def handle_error(cls, error: Exception, error_context: str, affected_states: list = None):
        """ì—ëŸ¬ ì²˜ë¦¬ ë° ìƒíƒœ ì—…ë°ì´íŠ¸"""
        error_msg = f"{error_context}: {str(error)}"
        logging.error(error_msg, exc_info=True)
        
        if affected_states:
            cls.reset_session_state(affected_states)
            
        cls.add_message("assistant", f"âŒ {error_msg}")
        return error_msg
    
    @classmethod
    def set_error_state(cls, error_message: str, error_context: str = None):
        """ì—ëŸ¬ ìƒíƒœ ì„¤ì •"""
        st.session_state.pdf_processing_error = error_message
        if error_context:
            logging.error(f"{error_context}: {error_message}")
        cls.add_message("assistant", f"âŒ {error_message}")
    
    @classmethod
    def clear_error_state(cls):
        """ì—ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”"""
        st.session_state.pdf_processing_error = None

@st.cache_data(show_spinner=False)
def get_ollama_models() -> List[str]:
    """Ollama ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    logging.info("Ollama ëª¨ë¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ê¸° ì‹œì‘...")
    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True, check=True)
        models = [line.split()[0] for line in result.stdout.split("\n")[1:] if line]
        return models
    except Exception as e:
        logging.error(f"Ollama ëª¨ë¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"Ollama ëª¨ë¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") from e

@st.cache_resource(show_spinner=False)
def load_pdf_docs(pdf_file_path: str) -> List:
    """PDF íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    logging.info("PDF íŒŒì¼ ë¡œë“œ ì‹œì‘...")
    if not os.path.exists(pdf_file_path):
        logging.error(f"PDF íŒŒì¼ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_file_path}")
        raise FileNotFoundError(f"PDF íŒŒì¼ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_file_path}")
    try:
        start_time = time.time()
        loader = PyMuPDFLoader(
            pdf_file_path,
            )
        docs = loader.load()
        logging.info(f"PDF íŒŒì¼ ë¡œë“œ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
        return docs
    except Exception as e:
        logging.error(f"PDF ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"PDF ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") from e

@st.cache_resource(show_spinner=False)
def load_embedding_model() -> HuggingFaceEmbeddings:
    """HuggingFace ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤."""
    logging.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹œì‘...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logging.info(f"ì„ë² ë”© ëª¨ë¸ìš© ì¥ì¹˜: {device}")
    try:
        embedder = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={
                'device': device,
            },
            encode_kwargs={
                'normalize_embeddings': True,
                'device': device,
            },
        )
        logging.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
        return embedder
    except Exception as e:
        logging.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise ValueError(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}") from e

@st.cache_data(show_spinner=False)
def split_documents(_docs: List) -> List:
    """ë¬¸ì„œë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜"""
    logging.info("ë¬¸ì„œ ë¶„í•  ì‹œì‘...")
    start_time = time.time()
    try:
        chunker = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # ì²­í¬ í¬ê¸° ìµœì í™” (ë” ì‘ì€ í¬ê¸°ë¡œ ì¡°ì •)
            chunk_overlap=200,  # ì˜¤ë²„ë© í¬ê¸° ì¦ê°€ë¡œ ë¬¸ë§¥ ìœ ì§€ ê°•í™”
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],  # ë¶„í•  ìš°ì„ ìˆœìœ„
            is_separator_regex=False,
            add_start_index=True,  # ì‹œì‘ ì¸ë±ìŠ¤ ì¶”ê°€ë¡œ ì¶”ì ì„± í–¥ìƒ
        )
        docs = chunker.split_documents(_docs)
        logging.info(f"ë¬¸ì„œ {len(docs)} í˜ì´ì§€ ë¶„í•  ì™„ë£Œ (ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
        return docs
    except Exception as e:
        logging.error(f"ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") from e

@st.cache_resource(show_spinner=False)
def create_vector_store(_documents, _embedder) -> Optional[FAISS]:
    """ë¬¸ì„œì—ì„œ FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    logging.info("FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹œì‘...")
    start_time = time.time()
    try:
        vector_space = FAISS.from_documents(
            documents=_documents,
            embedding=_embedder,
        )   
        logging.info(f"FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ (ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
        return vector_space
    except Exception as e:
        logging.error(f"FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") from e
    
@st.cache_resource(show_spinner=False)
def load_llm(model_name: str) -> OllamaLLM:
    """ì„ íƒëœ Ollama LLMì„ ë¡œë“œí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤."""
    logging.info(f"Ollama LLM ë¡œë”© ì‹œì‘: {model_name}")
    try:
        llm = OllamaLLM(
            model=model_name,
            )
        logging.info(f"Ollama LLM ë¡œë”© ì™„ë£Œ: {model_name}")
        return llm
    except Exception as e:
        logging.error(f"Ollama LLM ({model_name}) ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise ValueError(f"Ollama LLM ({model_name}) ë¡œë”© ì‹¤íŒ¨: {e}") from e
    
# QA í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ìˆ˜ ì™¸ë¶€ì—ì„œ ì •ì˜í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ import ê°€ëŠ¥í•˜ê²Œ í•¨
QA_PROMPT = ChatPromptTemplate.from_messages([
    ("system", (
        "You are an AI assistant. Your primary task is to answer questions based *solely* on the provided 'Context'.\n\n"
        "**CRITICAL: You MUST respond in the EXACT same language as the user's question.** This is the most important instruction.\n\n"
        "Context:\n"
        "{context}\n"
        "Follow these instructions carefully:\n"
        "1. **Language of Response:**\n"
        "   - ALWAYS use the same language as the user's question for your entire response.\n"
        "   - For example, if the question is in Korean, your answer MUST be in Korean. If the question is in English, your answer MUST be in English.\n\n"
        "2. **Answer Formulation:**\n"
        "   - Construct your answer based *strictly* on the information found within the 'Context'.\n"
        "   - Your answer should be clear and detailed.\n"
        "   - Do NOT use any external knowledge or information not present in the 'Context'.\n\n"
        "3. **Handling Missing Information:**\n"
        "   - If the 'Context' does not contain the information to answer the question, you MUST state (in the same language as the question) that the information is not available in the provided document.\n"
        "   - Do not invent an answer or use external knowledge."
        )),
    ("human", "Question: {input}")
    ])

@st.cache_resource(show_spinner=False, ttl=3600)
def process_pdf(uploaded_file, selected_model: str, temp_pdf_path: str):
    """PDF ì²˜ë¦¬ ë° QA ì²´ì¸ ìƒì„±."""
    try:
        # ì´ˆê¸° ì§„í–‰ ìƒíƒœ ë©”ì‹œì§€
        SessionManager.update_progress("start", "PDF ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        # 1. PDF ë¬¸ì„œ ë¡œë“œ
        SessionManager.update_progress("loading", "PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        docs = load_pdf_docs(temp_pdf_path)
        if not docs:
            raise ValueError("PDF ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        SessionManager.update_progress("embedding", "ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        embedder = load_embedding_model()
        if not embedder:
            raise ValueError("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 3. ë¬¸ì„œ ë¶„í• 
        SessionManager.update_progress("splitting", "ë¬¸ì„œë¥¼ ë¶„í• í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        documents = split_documents(docs)
        if not documents:
            raise ValueError("ë¬¸ì„œë¥¼ ë¶„í• í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 4. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        SessionManager.update_progress("vectorizing", "ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        vector_store = create_vector_store(documents, embedder)
        if not vector_store:
            raise ValueError("ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.session_state.vector_store = vector_store

        # 5. LLM ì´ˆê¸°í™”
        SessionManager.update_progress("llm_init", f"{selected_model} ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        if not isinstance(selected_model, str):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë¸ëª…ì…ë‹ˆë‹¤.")
            
        llm = load_llm(selected_model)
        if not llm:
            raise ValueError("LLMì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.session_state.llm = llm

        # 6. QA ì²´ì¸ ìƒì„±
        SessionManager.update_progress("qa_chain", "QA ì²´ì¸ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        logging.info("QA ì²´ì¸ ìƒì„± ì‹œì‘...")
        combine_chain = create_stuff_documents_chain(
            st.session_state.llm,
            QA_PROMPT
        )
        
        retriever = st.session_state.vector_store.as_retriever(
            search_type=RETRIEVER_CONFIG['search_type'],
            search_kwargs=RETRIEVER_CONFIG['search_kwargs']
        )
        
        qa_chain = create_retrieval_chain(retriever, combine_chain)
        if not qa_chain:
            raise ValueError("QA ì²´ì¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        st.session_state.qa_chain = qa_chain
        st.session_state.pdf_processed = True
        logging.info("PDF ì²˜ë¦¬ ì™„ë£Œ.")
        
        # ì„±ê³µ ë©”ì‹œì§€ ìƒì„±
        success_message = (
            f"âœ… '{uploaded_file.name}' ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
            "ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ë“¤ì„ í•´ë³´ì„¸ìš”:\n\n"
            "[ë¬¸ì„œ ì „ì²´ ì´í•´í•˜ê¸°]\n"
            "- ì´ ë¬¸ì„œë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”\n"
            "- ì´ ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì¥ê³¼ ê·¼ê±°ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n"
            "- ì´ ë¬¸ì„œì˜ í•µì‹¬ ìš©ì–´ 3ê°€ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n\n"
            "[ì„¸ë¶€ ë‚´ìš© íŒŒì•…í•˜ê¸°]\n"
            "- ì´ ë¬¸ì„œê°€ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n"
            "- ë¬¸ì„œì—ì„œ ì œì‹œëœ í•´ê²°ì±…ì´ë‚˜ ì œì•ˆì€ ë¬´ì—‡ì¸ê°€ìš”?\n"
            "- ì´ ì—°êµ¬ì˜ í•œê³„ì ì´ë‚˜ í–¥í›„ ì—°êµ¬ ë°©í–¥ì€ ë¬´ì—‡ì¸ê°€ìš”?\n\n"
            "ììœ ë¡­ê²Œ ë¬¸ì„œì˜ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
        )
        SessionManager.add_message("assistant", success_message)

    except Exception as e:
        logging.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        st.session_state.pdf_processing_error = str(e)
        error_message = f"âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        SessionManager.add_message("assistant", error_message)
        st.session_state.pdf_processed = False
        st.session_state.qa_chain = None
    finally:
        st.session_state.pdf_is_processing = False
        st.session_state.processing_step = None
        SessionManager.request_rerun()