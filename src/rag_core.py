"""
RAG íŒŒì´í”„ë¼ì¸ì˜ í•µì‹¬ ë¡œì§(ë°ì´í„° ì²˜ë¦¬, ì„ë² ë”©, ê²€ìƒ‰, ìƒì„±)ì„ ë‹´ë‹¹í•˜ëŠ” íŒŒì¼.
"""
import os
import logging
import hashlib
import json
from typing import List, Optional, Dict, Tuple
import tempfile

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.retrievers import EnsembleRetriever

from config import (
    RETRIEVER_CONFIG,
    TEXT_SPLITTER_CONFIG,
    VECTOR_STORE_CACHE_DIR,
)
from session import SessionManager
from utils import log_operation
from graph_builder import build_graph


@log_operation("PDF ë¬¸ì„œ ë¡œë“œ")
def load_pdf_docs(pdf_file_bytes: bytes) -> List:
    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as temp_file:
        temp_file.write(pdf_file_bytes)
        temp_file_path = temp_file.name
    
    try:
        loader = PyMuPDFLoader(file_path=temp_file_path)
        docs = loader.load()
    finally:
        os.remove(temp_file_path)
    return docs

@log_operation("ë¬¸ì„œ ë¶„í• ")
def split_documents(docs: List) -> List:
    chunker = RecursiveCharacterTextSplitter(
        chunk_size=TEXT_SPLITTER_CONFIG["chunk_size"],
        chunk_overlap=TEXT_SPLITTER_CONFIG["chunk_overlap"],
    )
    return chunker.split_documents(docs)

class VectorStoreCache:
    def __init__(self, file_bytes: bytes, embedding_model_name: str):
        self.cache_dir, self.doc_splits_path, self.faiss_index_path = (
            self._get_cache_paths(file_bytes, embedding_model_name)
        )

    def _get_cache_paths(
        self, file_bytes: bytes, embedding_model_name: str
    ) -> Tuple[str, str, str]:
        file_hash = hashlib.sha256(file_bytes).hexdigest()
        model_name_slug = embedding_model_name.replace("/", "_")
        cache_dir = os.path.join(
            VECTOR_STORE_CACHE_DIR, f"{file_hash}_{model_name_slug}"
        )
        doc_splits_path = os.path.join(cache_dir, "doc_splits.json")
        faiss_index_path = os.path.join(cache_dir, "faiss_index")
        return cache_dir, doc_splits_path, faiss_index_path

    def _serialize_docs(self, docs: List[Document]) -> List[Dict]:
        return [
            {"page_content": doc.page_content, "metadata": doc.metadata} for doc in docs
        ]

    def _deserialize_docs(self, docs_as_dicts: List[Dict]) -> List[Document]:
        return [
            Document(page_content=d["page_content"], metadata=d["metadata"])
            for d in docs_as_dicts
        ]

    @log_operation("ë²¡í„° ì €ì¥ì†Œ ìºì‹œ ë¡œë“œ")
    def load(
        self, embedder: "HuggingFaceEmbeddings"
    ) -> Tuple[Optional[List[Document]], Optional["FAISS"]]:
        if os.path.exists(self.doc_splits_path) and os.path.exists(
            self.faiss_index_path
        ):
            try:
                # 1. ë¬¸ì„œ ì¡°ê° ë¡œë“œ
                with open(self.doc_splits_path, "r", encoding="utf-8") as f:
                    doc_splits_as_dicts = json.load(f)
                doc_splits = self._deserialize_docs(doc_splits_as_dicts)

                # 2. FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                vector_store = FAISS.load_local(
                    self.faiss_index_path,
                    embedder,
                    allow_dangerous_deserialization=True,
                )
                logging.info(f"ë²¡í„° ì €ì¥ì†Œ ìºì‹œë¥¼ '{self.cache_dir}'ì—ì„œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
                return doc_splits, vector_store
            except Exception as e:
                logging.warning(f"ìºì‹œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ìºì‹œë¥¼ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
        return None, None

    @log_operation("ë²¡í„° ì €ì¥ì†Œ ìºì‹œ ì €ì¥")
    def save(self, doc_splits: List[Document], vector_store: "FAISS"):
        try:
            os.makedirs(self.cache_dir, exist_ok=True)
            # 1. ë¬¸ì„œ ì¡°ê° ì €ì¥
            with open(self.doc_splits_path, "w", encoding="utf-8") as f:
                json.dump(
                    self._serialize_docs(doc_splits), f, ensure_ascii=False, indent=4
                )
            # 2. FAISS ì¸ë±ìŠ¤ ì €ì¥
            vector_store.save_local(self.faiss_index_path)
            logging.info(f"ë²¡í„° ì €ì¥ì†Œ ìºì‹œë¥¼ '{self.cache_dir}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logging.error(f"ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

@log_operation("FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±")
def create_vector_store(docs: List, embedder: "HuggingFaceEmbeddings") -> "FAISS":
    return FAISS.from_documents(docs, embedder)

@log_operation("BM25 ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±")
def create_bm25_retriever(docs: List, k: int) -> "BM25Retriever":
    retriever = BM25Retriever.from_documents(docs)
    retriever.k = k
    return retriever

@log_operation("Ensemble ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±")
def create_ensemble_retriever(faiss_retriever, bm25_retriever, weights: List[float]):
    return EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever], weights=weights
    )

@log_operation("RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")
def build_rag_pipeline(
    uploaded_file_name: str, file_bytes: bytes, llm, embedder
) -> Tuple[str, bool]:
    cache = VectorStoreCache(file_bytes, embedder.model_name)
    doc_splits, vector_store = cache.load(embedder)
    cache_used = False

    if doc_splits and vector_store:
        cache_used = True
    else:
        docs = load_pdf_docs(file_bytes)
        doc_splits = split_documents(docs)
        vector_store = create_vector_store(doc_splits, embedder)
        cache.save(doc_splits, vector_store)

    faiss_retriever = vector_store.as_retriever(
        search_type=RETRIEVER_CONFIG["search_type"],
        search_kwargs=RETRIEVER_CONFIG["search_kwargs"],
    )
    bm25_retriever = create_bm25_retriever(
        docs=doc_splits, k=RETRIEVER_CONFIG["search_kwargs"]["k"]
    )
    final_retriever = create_ensemble_retriever(
        faiss_retriever, bm25_retriever, RETRIEVER_CONFIG["ensemble_weights"]
    )

    # --- ğŸ’¡ build_graph í˜¸ì¶œ ì‹œ llm ì¸ì ì œê±° ğŸ’¡ ---
    rag_app = build_graph(retriever=final_retriever)

    SessionManager.set("processed_document_splits", doc_splits)
    SessionManager.set("qa_chain", rag_app)
    SessionManager.set("pdf_processed", True)

    logging.info(
        f"'{uploaded_file_name}' ë¬¸ì„œ ì²˜ë¦¬ ë° LangGraph ê¸°ë°˜ QA ì²´ì¸ ìƒì„± ì™„ë£Œ. (ìºì‹œ ì‚¬ìš©: {cache_used})"
    )

    if cache_used:
        success_message = f"âœ… '{uploaded_file_name}' ë¬¸ì„œì˜ ì €ì¥ëœ ìºì‹œë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤."
    else:
        success_message = (
            f"âœ… '{uploaded_file_name}' ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
            "ì´ì œ ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
        )
    return success_message, cache_used


# --- ğŸ’¡ LLM ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ íš¨ìœ¨ì ì¸ í•¨ìˆ˜ ë¶€í™œ ğŸ’¡ ---
@log_operation("íŒŒì´í”„ë¼ì¸ì˜ LLM ì—…ë°ì´íŠ¸")
def update_llm_in_pipeline(llm):
    """ì„¸ì…˜ì˜ LLMì„ êµì²´í•©ë‹ˆë‹¤. ê·¸ë˜í”„ê°€ ì„¸ì…˜ì—ì„œ LLMì„ ê°€ì ¸ì˜¤ë¯€ë¡œ ì¬ë¹Œë“œí•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤."""
    if not SessionManager.get("pdf_processed"):
        raise ValueError("RAG íŒŒì´í”„ë¼ì¸ì´ êµ¬ì¶•ë˜ì§€ ì•Šì•„ LLMì„ ì—…ë°ì´íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    SessionManager.set("llm", llm)
    logging.info(f"ì„¸ì…˜ì˜ LLMì´ ìƒˆë¡œìš´ ëª¨ë¸ '{llm.model}'(ìœ¼)ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")