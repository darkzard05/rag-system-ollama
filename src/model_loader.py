"""
LLM ë° ì„ë² ë”© ëª¨ë¸ ë¡œë”©ì„ ë‹´ë‹¹í•˜ëŠ” íŒŒì¼.
"""

import os
import logging
import streamlit as st
import ollama
from typing import List, TYPE_CHECKING
from concurrent.futures import ThreadPoolExecutor

if TYPE_CHECKING:
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_ollama import OllamaLLM

from config import (
    CACHE_DIR,
    OLLAMA_MODEL_NAME,
    OLLAMA_NUM_PREDICT,
    EMBEDDING_BATCH_SIZE,
)
from utils import log_operation


def _fetch_ollama_models() -> List[str]:
    try:
        ollama_response = ollama.list()
        models = sorted([model["model"] for model in ollama_response.get("models", [])])
        if models:
            logging.info(f"Ollamaì—ì„œ ë‹¤ìŒ ëª¨ë¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {models}")
        return models
    except Exception as e:
        logging.warning(
            f"Ollama ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}"
        )
        return []



@st.cache_data(ttl=3600)
def get_available_models() -> List[str]:
    ollama_models = _fetch_ollama_models()

    if not ollama_models:
        logging.error("ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return [OLLAMA_MODEL_NAME]

    return ollama_models


def _get_dynamic_batch_size(device: str) -> int:
    """GPU VRAMì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    if device != "cuda":
        logging.info("CPU í™˜ê²½ì´ë¯€ë¡œ ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°(64)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return 64

    import torch
    try:
        total_vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        logging.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU VRAM: {total_vram_gb:.2f}GB")

        if total_vram_gb > 16:
            batch_size = 256
        elif total_vram_gb > 8:
            batch_size = 128
        elif total_vram_gb > 4:
            batch_size = 64
        else:
            batch_size = 32
        
        logging.info(f"VRAM ê¸°ë°˜ ë™ì  ë°°ì¹˜ í¬ê¸°: {batch_size}")
        return batch_size
    except Exception as e:
        logging.warning(f"VRAM í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°(64)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return 64


@st.cache_resource(show_spinner=False)
@log_operation("ì„ë² ë”© ëª¨ë¸ ë¡œë”©")
def load_embedding_model(embedding_model_name: str) -> "HuggingFaceEmbeddings":
    import torch
    from langchain_huggingface import HuggingFaceEmbeddings

    # if hasattr(torch, "classes"):
    #     torch.classes.__path__ = []

    device = "cuda" if torch.cuda.is_available() else "cpu"
    logging.info(f"ì„ë² ë”© ëª¨ë¸ìš© ì¥ì¹˜: {device}")

    batch_size = 128  # ê¸°ë³¸ê°’
    if isinstance(EMBEDDING_BATCH_SIZE, int):
        batch_size = EMBEDDING_BATCH_SIZE
        logging.info(f"config.ymlì— ì„¤ì •ëœ ë°°ì¹˜ í¬ê¸°({batch_size})ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    elif EMBEDDING_BATCH_SIZE == "auto":
        batch_size = _get_dynamic_batch_size(device)
    else:
        logging.warning(f"ì˜ëª»ëœ ë°°ì¹˜ í¬ê¸° ì„¤ì •('{EMBEDDING_BATCH_SIZE}'). ê¸°ë³¸ê°’(128)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    return HuggingFaceEmbeddings(
        model_name=embedding_model_name,
        model_kwargs={"device": device},
        encode_kwargs={"device": device, "batch_size": batch_size},
        cache_folder=CACHE_DIR,
    )


@st.cache_resource(show_spinner=False)
@log_operation("Ollama LLM ë¡œë”©")
def load_ollama_llm(_model_name: str) -> "OllamaLLM":
    from langchain_ollama import OllamaLLM

    # --- ğŸ’¡ JSON ë° temperature ì „ì—­ ì„¤ì •ì„ ì œê±°í•˜ê³ , ìˆœìˆ˜í•œ LLM ê°ì²´ë¥¼ ë°˜í™˜ ğŸ’¡ ---
    return OllamaLLM(model=_model_name, num_predict=OLLAMA_NUM_PREDICT)





def load_llm(model_name: str):
    return load_ollama_llm(_model_name=model_name)


def is_embedding_model_cached(model_name: str) -> bool:
    """
    ì£¼ì–´ì§„ Hugging Face ì„ë² ë”© ëª¨ë¸ì´ ë¡œì»¬ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

    Args:
        model_name (str): í™•ì¸í•  Hugging Face ëª¨ë¸ì˜ ì´ë¦„ (ì˜ˆ: "jhgan/ko-sroberta-multitask").

    Returns:
        bool: ëª¨ë¸ì´ ìºì‹œë˜ì–´ ìˆìœ¼ë©´ True, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False.
    """
    model_path_name = f"models--{model_name.replace('/', '--')}"
    cache_path = os.path.join(CACHE_DIR, model_path_name)
    return os.path.exists(cache_path)