"""
LLM ë° ì„ë² ë”© ëª¨ë¸ ë¡œë”©ì„ ë‹´ë‹¹í•˜ëŠ” íŒŒì¼.
"""

import os
import logging
import streamlit as st
import ollama
from typing import List, TYPE_CHECKING
from concurrent.futures import ThreadPoolExecutor

if TYPE_CHECKING:
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_ollama import OllamaLLM
    from langchain_google_genai import ChatGoogleGenerativeAI

from config import (
    CACHE_DIR,
    OLLAMA_MODEL_NAME,
    GEMINI_MODEL_NAME,
    GEMINI_API_KEY,
    OLLAMA_NUM_PREDICT,
    PREFERRED_GEMINI_MODELS,
)
from utils import log_operation


def _fetch_ollama_models() -> List[str]:
    try:
        ollama_response = ollama.list()
        models = sorted([model["model"] for model in ollama_response.get("models", [])])
        if models:
            logging.info(f"Ollamaì—ì„œ ë‹¤ìŒ ëª¨ë¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {models}")
        return models
    except Exception as e:
        logging.warning(
            f"Ollama ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}"
        )
        return []

def _fetch_gemini_models() -> List[str]:
    # import google.generativeai as genai
    # if not GEMINI_API_KEY:
    #     return []
    # try:
    #     genai.configure(api_key=GEMINI_API_KEY)
    #     available_models_from_api = [
    #         m.name.replace("models/", "")
    #         for m in genai.list_models()
    #         if "generateContent" in m.supported_generation_methods
    #     ]
        
    #     filtered_gemini_models = [
    #         model
    #         for model in PREFERRED_GEMINI_MODELS
    #         if model in available_models_from_api
    #     ]

    #     if filtered_gemini_models:
    #         logging.info(f"ì„ ë³„ëœ Gemini ëª¨ë¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {filtered_gemini_models}")
    #         return filtered_gemini_models
        
    #     fallback_models = [
    #         m
    #         for m in available_models_from_api
    #         if any(k in m for k in ["1.5", "pro"])
    #     ][:5]
    #     logging.info(
    #         f"ì„ í˜¸í•˜ëŠ” Gemini ëª¨ë¸ì„ ì°¾ì§€ ëª»í•´, ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ ì¼ë¶€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {fallback_models}"
    #     )
    #     return fallback_models
    # except Exception as e:
    #     logging.warning(f"Gemini ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    return []

@st.cache_data(ttl=3600)
def get_available_models() -> List[str]:
    ollama_models = _fetch_ollama_models()

    if not ollama_models:
        logging.error("ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return [OLLAMA_MODEL_NAME]

    return ollama_models


@st.cache_resource(show_spinner=False)
@log_operation("ì„ë² ë”© ëª¨ë¸ ë¡œë”©")
def load_embedding_model(embedding_model_name: str) -> "HuggingFaceEmbeddings":
    import torch
    from langchain_huggingface import HuggingFaceEmbeddings

    if hasattr(torch, "classes"):
        torch.classes.__path__ = []

    device = "cuda" if torch.cuda.is_available() else "cpu"
    logging.info(f"ì„ë² ë”© ëª¨ë¸ìš© ì¥ì¹˜: {device}")
    return HuggingFaceEmbeddings(
        model_name=embedding_model_name,
        model_kwargs={"device": device},
        encode_kwargs={"device": device, "batch_size": 128},
        cache_folder=CACHE_DIR,
    )


@st.cache_resource(show_spinner=False)
@log_operation("Ollama LLM ë¡œë”©")
def load_ollama_llm(_model_name: str) -> "OllamaLLM":
    from langchain_ollama import OllamaLLM

    # --- ğŸ’¡ JSON ë° temperature ì „ì—­ ì„¤ì •ì„ ì œê±°í•˜ê³ , ìˆœìˆ˜í•œ LLM ê°ì²´ë¥¼ ë°˜í™˜ ğŸ’¡ ---
    return OllamaLLM(model=_model_name, num_predict=OLLAMA_NUM_PREDICT)


# @st.cache_resource(show_spinner=False)
# @log_operation("Gemini LLM ë¡œë”©")
# def load_gemini_llm(_model_name: str) -> "ChatGoogleGenerativeAI":
#     from langchain_google_genai import ChatGoogleGenerativeAI

#     if not GEMINI_API_KEY:
#         raise ValueError("config.py íŒŒì¼ì— Gemini API í‚¤ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
#     return ChatGoogleGenerativeAI(model=_model_name, google_api_key=GEMINI_API_KEY)


def load_llm(model_name: str):
    # if "gemini" in model_name.lower():
    #     return load_gemini_llm(_model_name=model_name)
    # else:
    return load_ollama_llm(_model_name=model_name)


def is_embedding_model_cached(model_name: str) -> bool:
    """
    ì£¼ì–´ì§„ Hugging Face ì„ë² ë”© ëª¨ë¸ì´ ë¡œì»¬ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

    Args:
        model_name (str): í™•ì¸í•  Hugging Face ëª¨ë¸ì˜ ì´ë¦„ (ì˜ˆ: "jhgan/ko-sroberta-multitask").

    Returns:
        bool: ëª¨ë¸ì´ ìºì‹œë˜ì–´ ìˆìœ¼ë©´ True, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False.
    """
    model_path_name = f"models--{model_name.replace('/', '--')}"
    cache_path = os.path.join(CACHE_DIR, model_path_name)
    return os.path.exists(cache_path)