"""
LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ìˆœí™”ëœ RAG ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
ì˜ë„ ë¶„ë¥˜, ìºì‹œ í™•ì¸, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, ìƒì„±ì˜ í•µì‹¬ ë‹¨ê³„ë¥¼ ì§ì„ í™”í•©ë‹ˆë‹¤.
"""

import asyncio
import logging
from typing import Any

from langchain_core.callbacks.manager import adispatch_custom_event
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, START, StateGraph

from api.schemas import AggregatedSearchResult, GraphState
from cache.response_cache import get_response_cache
from common.config import (
    ANALYSIS_PROTOCOL,
    RERANKER_CONFIG,
)
from common.utils import fast_hash
from core.session import SessionManager
from services.monitoring.llm_tracker import ResponsePerformanceTracker

logger = logging.getLogger(__name__)


async def preprocess(state: GraphState) -> dict[str, Any]:
    """ì˜ë„ ë¶„ë¥˜ ë° ìºì‹œ í™•ì¸ì„ ë™ì‹œì— ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    query = state["input"].strip()

    # 1. ì˜ë„ ë¶„ë¥˜ (ë‹¨ìˆœ ê·œì¹™)
    greetings = ["ì•ˆë…•", "hi", "hello", "ë„ì›€ë§", "ê³ ë§ˆì›Œ"]
    if any(g in query.lower() for g in greetings) or len(query) < 2:
        return {"intent": "general", "is_cached": False}

    # 2. ìºì‹œ í™•ì¸
    cache = get_response_cache()
    cached_res = await cache.get(query, use_semantic=True)
    if cached_res:
        SessionManager.add_status_log("ìºì‹œëœ ë‹µë³€ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        return {
            "response": cached_res.response,
            "thought": cached_res.metadata.get("thought", ""),
            "is_cached": True,
        }

    return {"intent": "rag", "is_cached": False}


async def retrieve_and_rerank(
    state: GraphState, config: RunnableConfig
) -> dict[str, Any]:
    """ë¬¸ì„œ ê²€ìƒ‰ ë° ì¬ìˆœìœ„í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    if state.get("is_cached") or state.get("intent") == "general":
        return {}

    from common.config import RAG_PARAMETERS
    from core.search_aggregator import AggregationStrategy, SearchResultAggregator

    query = state["input"]
    cfg = config.get("configurable", {})
    RAG_PARAMETERS.get("retrieval_k", 25)

    # [í•µì‹¬] UI í•¸ë“¤ëŸ¬ë¥¼ ìœ„í•œ ìƒíƒœ ì´ë²¤íŠ¸ ë°œìƒ
    await adispatch_custom_event(
        "status_update", {"message": "ğŸ” ê´€ë ¨ ì§€ì‹ íƒìƒ‰ ì¤‘..."}, config=config
    )
    SessionManager.add_status_log("ğŸ” ë¬¸ì„œ ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ì§€ì‹ íƒìƒ‰ ì‹œì‘")

    # ë¦¬íŠ¸ë¦¬ë²„ íšë“
    bm25 = cfg.get("bm25_retriever")
    faiss = cfg.get("faiss_retriever")

    # ë³‘ë ¬ ê²€ìƒ‰
    tasks = []
    if bm25:
        tasks.append(asyncio.create_task(bm25.ainvoke(query)))
    if faiss:
        tasks.append(asyncio.create_task(faiss.ainvoke(query)))

    results = await asyncio.gather(*tasks) if tasks else [[], []]

    # ê²°ê³¼ ë³‘í•© ë° RRF ì§‘ê³„
    all_docs = []
    for i, res in enumerate(results):
        source = "bm25" if i == 0 else "faiss"
        for doc in res:
            all_docs.append(
                AggregatedSearchResult(
                    doc_id=doc.metadata.get("doc_id", fast_hash(doc.page_content)),
                    content=doc.page_content,
                    score=doc.metadata.get("score", 0.5),
                    node_id=source,
                    metadata=doc.metadata,
                )
            )

    aggregator = SearchResultAggregator()
    weights = {"bm25": 0.4, "faiss": 0.6}
    aggregated, _ = aggregator.aggregate_results(
        {"all": all_docs},
        strategy=AggregationStrategy.WEIGHTED_RRF,
        top_k=25,
        weights=weights,
    )

    final_docs = [
        Document(page_content=r.content, metadata=r.metadata) for r in aggregated
    ]
    SessionManager.add_status_log(
        f"ğŸ“š í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ ({len(final_docs)}ê°œ í›„ë³´ í™•ë³´)"
    )

    # FlashRank ë¦¬ë­í‚¹ (ì„ íƒì )
    if RERANKER_CONFIG.get("enabled", True) and len(final_docs) > 1:
        await adispatch_custom_event(
            "status_update",
            {"message": "âš–ï¸ ì§€ì‹ ìš°ì„ ìˆœìœ„ ì •ì œ ì¤‘ (FlashRank)"},
            config=config,
        )
        SessionManager.add_status_log("âš–ï¸ ì§€ì‹ì˜ ìš°ì„ ìˆœìœ„ ì¬ì¡°ì • ë° ì •ì œ ì¤‘ (FlashRank)")
        from flashrank import RerankRequest

        from core.model_loader import ModelManager

        ranker = await ModelManager.get_flashranker()
        passages = [
            {"id": i, "text": d.page_content, "meta": d.metadata}
            for i, d in enumerate(final_docs)
        ]

        await ModelManager.acquire_inference_lock()
        try:
            results = await asyncio.to_thread(
                ranker.rerank, RerankRequest(query=query, passages=passages)
            )
            final_docs = [
                Document(page_content=r["text"], metadata=r["meta"])
                for r in results[:10]
            ]
            SessionManager.add_status_log(
                f"âœ… ìµœì ì˜ ì§€ì‹ 10ê°œ ì„ ë³„ ì™„ë£Œ (ì‹ ë¢°ë„: {results[0]['score']:.2f})"
            )
        finally:
            ModelManager.release_inference_lock()

    return {"relevant_docs": final_docs}


async def generate(state: GraphState, config: RunnableConfig) -> dict[str, Any]:
    """ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if state.get("is_cached"):
        return {}

    cfg = config.get("configurable", {})
    llm = cfg.get("llm")
    if not llm:
        return {"response": "âŒ LLM ë¯¸ë¡œë“œ"}

    await adispatch_custom_event(
        "status_update", {"message": "âœï¸ ë‹µë³€ ì‘ì„± ì¤‘..."}, config=config
    )
    SessionManager.add_status_log("ğŸ§  ë‹µë³€ ë…¼ë¦¬ ì„¤ê³„ ë° ìƒì„± ì‹œì‘")

    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    docs = state.get("relevant_docs", [])
    context = ""
    for i, d in enumerate(docs):
        context += (
            f"### [ìë£Œ {i + 1}] (P{d.metadata.get('page', '?')})\n{d.page_content}\n\n"
        )

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    sys_msg = SystemMessage(content="ì „ë¬¸ ë¬¸ì„œ ë¶„ì„ê°€ë¡œì„œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.")
    human_msg = HumanMessage(
        content=f"{ANALYSIS_PROTOCOL}\n\n[Context]\n{context}\n\n[Question]\n{state['input']}"
    )

    tracker = ResponsePerformanceTracker(state["input"], llm)
    tracker.set_context(context, doc_count=len(docs))

    from core.model_loader import ModelManager

    await ModelManager.acquire_inference_lock()
    try:
        async for chunk in llm.astream([sys_msg, human_msg], config=config):
            msg = getattr(chunk, "message", chunk)
            content, thought = msg.content, msg.additional_kwargs.get("thinking", "")
            tracker.record_chunk(content, thought)
            if content or thought:
                await adispatch_custom_event(
                    "response_chunk",
                    {"chunk": content, "thought": thought},
                    config=config,
                )
    finally:
        ModelManager.release_inference_lock()

    # ì„±ëŠ¥ ì§€í‘œ í™•ì • ë° ë°˜í™˜ ë°ì´í„° êµ¬ì„±
    stats = tracker.finalize_and_log()
    return {
        "response": tracker.full_response,
        "thought": tracker.full_thought,
        "performance": stats.model_dump() if hasattr(stats, "model_dump") else stats,
    }


def build_graph() -> Any:
    """ê·¸ë˜í”„ë¥¼ ë¹Œë“œí•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤."""
    workflow = StateGraph(GraphState)

    workflow.add_node("preprocess", preprocess)
    workflow.add_node("retrieve", retrieve_and_rerank)
    workflow.add_node("generate", generate)

    workflow.add_edge(START, "preprocess")

    # ì¡°ê±´ë¶€ ì—ì§€: ìºì‹œê°€ ìˆìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ, ì¼ìƒ ëŒ€í™”ë©´ ë°”ë¡œ ìƒì„±, ì•„ë‹ˆë©´ ê²€ìƒ‰
    workflow.add_conditional_edges(
        "preprocess",
        lambda s: (
            "END"
            if s.get("is_cached")
            else ("generate" if s.get("intent") == "general" else "retrieve")
        ),
        {"END": END, "generate": "generate", "retrieve": "retrieve"},
    )

    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", END)

    return workflow.compile()
