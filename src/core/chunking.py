"""
ë¬¸ì„œ ë¶„í• (Chunking)ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆ.
"""

import logging
import os

import numpy as np
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

from common.config import SEMANTIC_CHUNKER_CONFIG, TEXT_SPLITTER_CONFIG
from core.semantic_chunker import EmbeddingBasedSemanticChunker
from core.session import SessionManager
from services.monitoring.performance_monitor import (
    OperationType,
    get_performance_monitor,
)

logger = logging.getLogger(__name__)
monitor = get_performance_monitor()


def _get_optimal_batch_size(embedder: Embeddings) -> int:
    """í•˜ë“œì›¨ì–´ ì‚¬ì–‘ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê²°ì •"""
    import torch

    if getattr(embedder, "model_kwargs", {}).get("device") == "cuda":
        try:
            total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            return 128 if total_mem > 10 else (64 if total_mem > 4 else 32)
        except Exception:
            return 32
    return min(max(4, os.cpu_count() or 4), 16)


def _init_semantic_chunker(embedder: Embeddings) -> EmbeddingBasedSemanticChunker:
    """ì„¤ì •ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ë¡ ì  ë¶„í• ê¸° ì´ˆê¸°í™”"""
    cfg = SEMANTIC_CHUNKER_CONFIG
    return EmbeddingBasedSemanticChunker(
        embedder=embedder,
        breakpoint_threshold_type=cfg.get("breakpoint_threshold_type", "percentile"),
        breakpoint_threshold_value=float(cfg.get("breakpoint_threshold_value", 95.0)),
        sentence_split_regex=cfg.get("sentence_split_regex", r"[.!?]\s+"),
        min_chunk_size=int(cfg.get("min_chunk_size", 100)),
        max_chunk_size=int(cfg.get("max_chunk_size", 800)),
        similarity_threshold=float(cfg.get("similarity_threshold", 0.5)),
        batch_size=_get_optimal_batch_size(embedder),
    )


def _postprocess_metadata(split_docs: list[Document]) -> None:
    """ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° ì •ë¦¬ ë° ë‚´ìš© ìœ í˜•(ì½˜í…ì¸ /ì°¸ê³ ë¬¸í—Œ ë“±) ì‹ë³„"""
    noise_keywords = ["index", "references", "bibliography", "doi:", "isbn"]
    found_ref_start = False

    for i, doc in enumerate(split_docs):
        doc.metadata = doc.metadata.copy()
        doc.metadata["chunk_index"] = i
        content_lower = doc.page_content.lower()

        # ì°¸ê³ ë¬¸í—Œ ì„¹ì…˜ ê°ì§€
        if doc.metadata.get("is_reference_start") or any(
            kw in content_lower[:50] for kw in ["## references", "references\n---"]
        ):
            found_ref_start = True

        # ë…¸ì´ì¦ˆ íŒë³„
        is_noise = any(kw in content_lower[:100] for kw in noise_keywords)
        if not is_noise and (
            content_lower.count("doi:") > 2 or content_lower.count(",") > 25
        ):
            is_noise = True

        doc.metadata.update(
            {
                "is_content": not (is_noise or found_ref_start),
                "is_reference": found_ref_start,
                "is_anchor": doc.metadata.get("is_anchor", False)
                if i == 0
                else False,  # ì²« í˜ì´ì§€ë§Œ ì•µì»¤ ìœ ì§€
                "is_header": True
                if (doc.metadata.get("page") == 1 and i < 3)
                else doc.metadata.get("is_header", False),
            }
        )


async def split_documents(
    docs: list[Document],
    embedder: Embeddings | None = None,
    session_id: str | None = None,
) -> tuple[list[Document], list[np.ndarray] | None]:
    """ì„¤ì •ì— ë”°ë¼ ë¬¸ì„œë¥¼ ë¶„í• í•˜ê³  ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if not docs:
        return [], None

    is_already_chunked = docs[0].metadata.get("is_already_chunked", False)
    split_docs: list[Document] = []
    vectors: list[np.ndarray] | None = None

    # [ìµœì í™”] í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì´ë¯¸ ë¶„í• ë˜ì—ˆë”ë¼ë„ ë„ˆë¬´ ê¸´ ê²½ìš°(ì˜¤ë²„í”Œë¡œìš°) ì¬ë¶„í•  ìˆ˜í–‰
    max_chunk_size = TEXT_SPLITTER_CONFIG.get("chunk_size", 500)
    needs_sub_chunking = is_already_chunked and any(
        len(d.page_content) > max_chunk_size * 1.5 for d in docs
    )

    if is_already_chunked and not needs_sub_chunking:
        SessionManager.add_status_log(
            f"ğŸ“‘ ê¸°ì¡´ ë¶„í•  êµ¬ì¡° í™œìš© ({len(docs)}ê°œ ì„¹ì…˜)", session_id=session_id
        )
        split_docs = docs
        if embedder:
            SessionManager.add_status_log("ğŸ§  ì§€ì‹ ë²¡í„°í™” ì¤‘...", session_id=session_id)
            vectors = [
                np.array(v)
                for v in embedder.embed_documents([d.page_content for d in split_docs])
            ]
    else:
        if needs_sub_chunking:
            SessionManager.add_status_log(
                "âœ‚ï¸ ëŒ€í˜• ì„¹ì…˜ ê°ì§€: ì •ë°€ ê²€ìƒ‰ì„ ìœ„í•œ í•˜ìœ„ ë¶„í•  ì‹œì‘",
                session_id=session_id,
            )
        else:
            SessionManager.add_status_log(
                "âœ‚ï¸ ë¬¸ì„œ ë¶„í•  ë° ë¬¸ë§¥ ì¶”ì¶œ ì¤‘...", session_id=session_id
            )

        use_semantic = SEMANTIC_CHUNKER_CONFIG.get("enabled", False)

        if use_semantic and embedder:
            with monitor.track_operation(
                OperationType.SEMANTIC_CHUNKING, {"doc_count": len(docs)}
            ):
                semantic_chunker = _init_semantic_chunker(embedder)
                split_docs, vectors = await semantic_chunker.split_documents(docs)
                msg = f"âœ‚ï¸ ì˜ë¯¸ë¡ ì  ë¶„í•  ì™„ë£Œ ({len(split_docs)}ê°œ ì¡°ê°)"
        else:
            recursive_chunker = RecursiveCharacterTextSplitter(
                chunk_size=TEXT_SPLITTER_CONFIG["chunk_size"],
                chunk_overlap=TEXT_SPLITTER_CONFIG["chunk_overlap"],
            )
            split_docs = recursive_chunker.split_documents(docs)
            if embedder:
                vectors = [
                    np.array(v)
                    for v in embedder.embed_documents(
                        [d.page_content for d in split_docs]
                    )
                ]
            msg = f"âœ‚ï¸ í‘œì¤€ ë¶„í•  ì™„ë£Œ ({len(split_docs)}ê°œ ì¡°ê°)"

        SessionManager.add_status_log(msg, session_id=session_id)
        logger.info(
            f"[RAG] [CHUNKING] ë¶„í•  ì™„ë£Œ | ì›ë³¸: {len(docs)} | ì²­í¬: {len(split_docs)}"
        )

    _postprocess_metadata(split_docs)
    return split_docs, vectors
