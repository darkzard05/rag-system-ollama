"""
RAG íŒŒì´í”„ë¼ì¸ì˜ í•µì‹¬ ë¡œì§(ë°ì´í„° ì²˜ë¦¬, ì„ë² ë”©, ê²€ìƒ‰, ìƒì„±)ì„ ë‹´ë‹¹í•˜ëŠ” íŒŒì¼.
"""

from __future__ import annotations

import functools
import hashlib
import json
import logging
import os
import pickle
from collections.abc import Callable
from typing import TYPE_CHECKING, Any

import numpy as np
from common.typing_utils import (
    DocumentDictList,
    DocumentList,
    T,
)

if TYPE_CHECKING:
    from langchain.retrievers import EnsembleRetriever
    from langchain_community.retrievers import BM25Retriever
    from langchain_community.vectorstores import FAISS
    from langchain_core.documents import Document
    from langchain_huggingface import HuggingFaceEmbeddings

from common.config import (
    CACHE_CHECK_PERMISSIONS,
    CACHE_HMAC_SECRET,
    CACHE_SECURITY_LEVEL,
    CACHE_TRUSTED_PATHS,
    RETRIEVER_CONFIG,
    SEMANTIC_CHUNKER_CONFIG,
    TEXT_SPLITTER_CONFIG,
    VECTOR_STORE_CACHE_DIR,
)
from common.exceptions import (
    EmptyPDFError,
    InsufficientChunksError,
    PDFProcessingError,
    VectorStoreError,
)
from common.utils import log_operation, preprocess_text
from core.graph_builder import build_graph
from core.semantic_chunker import EmbeddingBasedSemanticChunker
from core.session import SessionManager
from security.cache_security import (
    CacheIntegrityError,
    CachePermissionError,
    CacheSecurityManager,
    CacheTrustError,
)
from services.monitoring.performance_monitor import (
    OperationType,
    get_performance_monitor,
)
from services.optimization.index_optimizer import get_index_optimizer

logger = logging.getLogger(__name__)
monitor = get_performance_monitor()


class RAGSystem:
    """
    RAG ì‹œìŠ¤í…œì˜ í†µí•© ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ í´ë˜ìŠ¤.
    ì„¸ì…˜ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬ì™€ LangGraph ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì„ ì—°ê²°í•©ë‹ˆë‹¤.
    """

    def __init__(self, session_id: str = "default"):
        self.session_id = session_id
        SessionManager.init_session(session_id=session_id)

    def _ensure_session_context(self) -> None:
        """í˜„ì¬ ìŠ¤ë ˆë“œì˜ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤."""
        SessionManager.set_session_id(self.session_id)

    async def load_document(
        self, file_path: str, file_name: str, embedder: HuggingFaceEmbeddings
    ) -> tuple[str, bool]:
        """
        ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            file_path: PDF íŒŒì¼ì˜ ë¡œì»¬ ê²½ë¡œ
            file_name: ì‚¬ìš©ìì—ê²Œ í‘œì‹œë  íŒŒì¼ ì´ë¦„
            embedder: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

        Returns:
            Tuple[ì„±ê³µ ë©”ì‹œì§€, ìºì‹œ ì‚¬ìš© ì—¬ë¶€]
        """
        self._ensure_session_context()
        return build_rag_pipeline(
            uploaded_file_name=file_name, file_path=file_path, embedder=embedder
        )

    async def aquery(self, query: str, llm: T | None = None) -> dict[str, Any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            llm: ì‚¬ìš©í•  LLM ì¸ìŠ¤í„´ìŠ¤ (ìƒëµ ì‹œ ì„¸ì…˜ì— ì €ì¥ëœ ëª¨ë¸ ì‚¬ìš©)

        Returns:
            GraphOutput êµ¬ì¡°ì˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self._ensure_session_context()

        if llm:
            SessionManager.set("llm", llm)

        rag_engine = SessionManager.get("rag_engine")
        if not rag_engine:
            raise VectorStoreError(
                details={
                    "reason": "RAG ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”."
                }
            )

        current_llm = SessionManager.get("llm")
        config = {"configurable": {"llm": current_llm}}

        # LangGraph í˜¸ì¶œ
        return await rag_engine.ainvoke({"input": query}, config=config)

    def get_status(self) -> list[str]:
        """í˜„ì¬ ì„¸ì…˜ì˜ ì‘ì—… ë¡œê·¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        self._ensure_session_context()
        return SessionManager.get("status_logs", [])

    def clear_session(self) -> None:
        """ì„¸ì…˜ ë°ì´í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self._ensure_session_context()
        SessionManager.reset_all_state()

    def process_documents(self, documents: list[str]) -> None:
        """ì…ë ¥ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤. (í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„± ìœ ì§€)"""
        if not documents:
            raise EmptyPDFError(
                filename="(in-memory)",
                details={"reason": "documents list is empty"},
            )


def _compute_file_hash(file_path: str) -> str:
    """íŒŒì¼ì˜ SHA256 í•´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    sha256_hash = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except Exception as e:
        logger.error(f"íŒŒì¼ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return ""


import concurrent.futures  # noqa: E402


def _extract_page_worker(
    file_path: str, page_num: int, total_pages: int, file_name: str
) -> Document | None:
    """ê°œë³„ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ (ìŠ¤ë ˆë“œ ì„¸ì´í”„)"""
    try:
        import fitz  # PyMuPDF
        from langchain_core.documents import Document

        # ê° ìŠ¤ë ˆë“œì—ì„œ íŒŒì¼ì„ ìƒˆë¡œ ì—´ì–´ ë…ë¦½ì ì¸ ë¬¸ì„œ ê°ì²´ ì‚¬ìš©
        with fitz.open(file_path) as doc:
            page = doc[page_num]
            text = page.get_text()
            if text:
                clean_text = preprocess_text(text)
                if clean_text and len(clean_text) > 10:
                    metadata = {
                        "source": file_name,
                        "page": int(page_num + 1),
                        "total_pages": int(total_pages),
                    }
                    return Document(page_content=clean_text, metadata=metadata)
    except Exception as e:
        logger.warning(f"í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return None


def _extract_pages_batch_worker(
    file_bytes: bytes, page_range: list[int], total_pages: int, file_name: str
) -> list[tuple[int, Document]]:
    """í˜ì´ì§€ ë²”ìœ„ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ (ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¼ ì‚¬ìš©ìœ¼ë¡œ íŒŒì¼ ì ê¸ˆ ë°©ì§€)"""
    results = []
    try:
        import fitz  # PyMuPDF
        from langchain_core.documents import Document

        # [ê°œì„ ] íŒŒì¼ ê²½ë¡œ ëŒ€ì‹  ë©”ëª¨ë¦¬ ë²„í¼ë¡œë¶€í„° ë¬¸ì„œë¥¼ ì—´ì–´ ê³µìœ  ìœ„ë°˜ ë°©ì§€
        with fitz.open(stream=file_bytes, filetype="pdf") as doc:
            for page_num in page_range:
                try:
                    page = doc[page_num]
                    text = page.get_text()
                    if text:
                        clean_text = preprocess_text(text)
                        if clean_text and len(clean_text) > 10:
                            metadata = {
                                "source": file_name,
                                "page": int(page_num + 1),
                                "total_pages": int(total_pages),
                            }
                            results.append(
                                (
                                    page_num,
                                    Document(
                                        page_content=clean_text, metadata=metadata
                                    ),
                                )
                            )
                except Exception as e:
                    logger.warning(f"í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ë¬¸ì„œ ì˜¤í”ˆ ì‹¤íŒ¨: {e}")
    return results


def _load_pdf_docs(
    file_path: str, file_name: str, on_progress: Callable[[], None] | None = None
) -> list[Document]:
    """
    PDF íŒŒì¼ì„ ë°°ì¹˜ ê¸°ë°˜ ë³‘ë ¬ ë¡œë“œë¡œ ë³€í™˜í•˜ì—¬ ìµœì í™”í•©ë‹ˆë‹¤.
    """
    with monitor.track_operation(OperationType.PDF_LOADING, {"file": file_name}) as op:
        try:
            import fitz  # PyMuPDF

            SessionManager.add_status_log("ë¬¸ì„œ ë¶„ì„ ì¤€ë¹„ ì¤‘")
            if on_progress:
                on_progress()

            try:
                doc_file = fitz.open(file_path)
                total_pages = len(doc_file)
                doc_file.close()
            except Exception as e:
                raise PDFProcessingError(
                    filename=file_name,
                    details={"reason": f"íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŒ: {str(e)}"},
                ) from e

            if total_pages == 0:
                raise EmptyPDFError(
                    filename=file_name, details={"reason": "PDF í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."}
                )

            # [ê°œì„ ] íŒŒì¼ì„ í•œ ë²ˆë§Œ ì½ì–´ ë©”ëª¨ë¦¬ì— ë²„í¼ë§ (íŒŒì¼ ì ê¸ˆ ì›ì²œ ë°©ì§€ ë° I/O ìµœì í™”)
            with open(file_path, "rb") as f:
                file_bytes = f.read()

            # [ìµœì í™”] í˜ì´ì§€ ìˆ˜ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë³‘ë ¬í™” ê²°ì • (ì„ê³„ê°’ 5pë¡œ í•˜í–¥)
            if total_pages <= 5:
                docs = []
                # [ê°œì„ ] ë‹¨ì¼ ìŠ¤ë ˆë“œì—ì„œë„ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¼ ì‚¬ìš©
                with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                    for i in range(total_pages):
                        page = doc[i]
                        text = page.get_text()
                        if text:
                            clean_text = preprocess_text(text)
                            if clean_text and len(clean_text) > 10:
                                docs.append(
                                    Document(
                                        page_content=clean_text,
                                        metadata={
                                            "source": file_name,
                                            "page": i + 1,
                                            "total_pages": total_pages,
                                        },
                                    )
                                )
                        if (i + 1) % 5 == 0 or i == total_pages - 1:
                            SessionManager.replace_last_status_log(
                                f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ({i + 1}/{total_pages}p)"
                            )
                            if on_progress:
                                on_progress()
            else:
                SessionManager.replace_last_status_log(
                    f"ë³‘ë ¬ ë°°ì¹˜ ì¶”ì¶œ ì‹œì‘ (ì´ {total_pages}p)"
                )
                if on_progress:
                    on_progress()

                # [ìµœì í™”] ì›Œì»¤ ìˆ˜ë¥¼ CPU ì½”ì–´ ìˆ˜ì— ë§ì¶° ë” ì ê·¹ì ìœ¼ë¡œ í• ë‹¹
                max_workers = min(os.cpu_count() or 4, 12)
                # í˜ì´ì§€ë¥¼ ë” ì‘ê²Œ ìª¼ê°œì–´ ë¶€í•˜ ë¶„ì‚°
                batch_size = max(1, total_pages // (max_workers * 2))
                batches = [
                    list(range(i, min(i + batch_size, total_pages)))
                    for i in range(0, total_pages, batch_size)
                ]

                all_results = [None] * total_pages
                completed_pages = 0

                with concurrent.futures.ThreadPoolExecutor(
                    max_workers=max_workers
                ) as executor:
                    futures = [
                        executor.submit(
                            _extract_pages_batch_worker,
                            file_bytes,  # [ìˆ˜ì •] íŒŒì¼ ê²½ë¡œ ëŒ€ì‹  ë©”ëª¨ë¦¬ ë°ì´í„° ì „ë‹¬
                            batch,
                            total_pages,
                            file_name,
                        )
                        for batch in batches
                    ]

                    for future in concurrent.futures.as_completed(futures):
                        batch_results = future.result()
                        for page_idx, doc in batch_results:
                            all_results[page_idx] = doc

                        completed_pages += len(batch_results)
                        # [ìˆ˜ì •] SessionManager ì—…ë°ì´íŠ¸ëŠ” ìŠ¤ë ˆë“œ ì„¸ì´í”„í•˜ë¯€ë¡œ ìœ ì§€í•˜ë˜,
                        # UI ê°±ì‹  ì½œë°±(on_progress)ì€ ë©”ì¸ ë£¨í”„ì¸ ì´ê³³ì—ì„œ í˜¸ì¶œí•˜ì—¬ ì•ˆì „ì„± í™•ë³´
                        SessionManager.replace_last_status_log(
                            f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ({min(completed_pages, total_pages)}/{total_pages}p)"
                        )
                        if on_progress:
                            try:
                                on_progress()
                            except Exception as e:
                                logger.debug(
                                    f"UI ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€ (Context missing): {e}"
                                )

                docs = [r for r in all_results if r is not None]

            if not docs:
                raise EmptyPDFError(
                    filename=file_name,
                    details={
                        "reason": "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ê¸°ë°˜ PDFì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    },
                )

            SessionManager.replace_last_status_log(f"ì¶”ì¶œ ì™„ë£Œ (êµ¬ê°„ {len(docs)}ê°œ)")
            op.tokens = sum(len(doc.page_content.split()) for doc in docs)
            return docs
        except (PDFProcessingError, EmptyPDFError):
            raise
        except Exception as e:
            logger.error(f"PDF ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            raise PDFProcessingError(
                filename=file_name, details={"reason": str(e)}
            ) from e


def _split_documents(
    docs: list[Document],
    embedder: HuggingFaceEmbeddings | None = None,
) -> tuple[list[Document], list[np.ndarray] | None]:
    """
    ì„¤ì •ì— ë”°ë¼ ì˜ë¯¸ë¡ ì  ë¶„í• ê¸° ë˜ëŠ” RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
    """
    SessionManager.add_status_log("ë¬¸ì¥ ë¶„í•  ì¤‘...")
    with monitor.track_operation(
        OperationType.SEMANTIC_CHUNKING, {"doc_count": len(docs)}
    ) as op:
        from langchain.text_splitter import RecursiveCharacterTextSplitter

        use_semantic = SEMANTIC_CHUNKER_CONFIG.get("enabled", False)
        split_docs = []
        vectors = None

        if use_semantic and embedder:
            # [ìµœì í™”] ë¬´ê±°ìš´ ìë™ ë°°ì¹˜ ìµœì í™” ëŒ€ì‹  ê³ ì •ê°’ ì‚¬ìš©
            batch_size = (
                32
                if getattr(embedder, "model_kwargs", {}).get("device") == "cuda"
                else 4
            )

            semantic_chunker = EmbeddingBasedSemanticChunker(
                embedder=embedder,
                breakpoint_threshold_type=SEMANTIC_CHUNKER_CONFIG.get(
                    "breakpoint_threshold_type", "percentile"
                ),
                breakpoint_threshold_value=float(
                    SEMANTIC_CHUNKER_CONFIG.get("breakpoint_threshold_value", 95.0)
                ),
                sentence_split_regex=SEMANTIC_CHUNKER_CONFIG.get(
                    "sentence_split_regex", r"[.!?]\s+"
                ),
                min_chunk_size=int(SEMANTIC_CHUNKER_CONFIG.get("min_chunk_size", 100)),
                max_chunk_size=int(SEMANTIC_CHUNKER_CONFIG.get("max_chunk_size", 800)),
                similarity_threshold=float(
                    SEMANTIC_CHUNKER_CONFIG.get("similarity_threshold", 0.5)
                ),
                batch_size=batch_size,
            )

            split_docs, vectors = semantic_chunker.split_documents(docs)
            logger.info(
                f"ì˜ë¯¸ë¡ ì  ë¶„í•  ì™„ë£Œ: {len(docs)} ë¬¸ì„œ -> {len(split_docs)} ì²­í¬ (ë²¡í„° ì¬ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ)"
            )
        else:
            chunker = RecursiveCharacterTextSplitter(
                chunk_size=TEXT_SPLITTER_CONFIG["chunk_size"],
                chunk_overlap=TEXT_SPLITTER_CONFIG["chunk_overlap"],
            )
            split_docs = chunker.split_documents(docs)
            logger.info(f"ê¸°ë³¸ ë¶„í•  ì™„ë£Œ: {len(docs)} ë¬¸ì„œ -> {len(split_docs)} ì²­í¬")

        # ì²­í¬ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for i, doc in enumerate(split_docs):
            doc.metadata = doc.metadata.copy()
            doc.metadata["chunk_index"] = i

        op.tokens = sum(len(doc.page_content.split()) for doc in split_docs)
        return split_docs, vectors


def _serialize_docs(docs: DocumentList) -> DocumentDictList:
    return [
        doc.model_dump() if hasattr(doc, "model_dump") else doc.dict() for doc in docs
    ]


def _deserialize_docs(docs_as_dicts: DocumentDictList) -> DocumentList:
    from langchain_core.documents import Document

    return [Document(**d) for d in docs_as_dicts]


@functools.lru_cache(maxsize=1)
def _compute_config_hash() -> str:
    """ì„¤ì • ë³€ê²½ ê°ì§€ìš© í•´ì‹œ ìƒì„± (ë³´ì•ˆìš© ì•„ë‹˜)"""
    config_dict = {
        "version": "2.1",  # ğŸš€ ì „ì²˜ë¦¬ ë¡œì§ ë³€ê²½ìœ¼ë¡œ ì¸í•œ ìºì‹œ ë¬´íš¨í™” ê°•ì œ
        "semantic_chunker": SEMANTIC_CHUNKER_CONFIG,
        "text_splitter": TEXT_SPLITTER_CONFIG,
        "retriever": RETRIEVER_CONFIG,
    }
    config_str = json.dumps(config_dict, sort_keys=True, default=str)
    # usedforsecurity=False: ì´ í•´ì‹œëŠ” ë³´ì•ˆ ëª©ì ì´ ì•„ë‹Œ ì„¤ì • ë³€ê²½ ê°ì§€ìš©ì„ì„ ëª…ì‹œ
    return hashlib.sha256(config_str.encode()).hexdigest()[:12]


class VectorStoreCache:
    """ë²¡í„° ì €ì¥ì†Œ ë° ë¦¬íŠ¸ë¦¬ë²„ ìºì‹œ ê´€ë¦¬ì"""

    def __init__(self, file_path: str, embedding_model_name: str, file_hash: str | None = None):
        self._file_hash = file_hash or _compute_file_hash(file_path)
        (
            self.cache_dir,
            self.doc_splits_path,
            self.faiss_index_path,
            self.bm25_retriever_path,
        ) = self._get_cache_paths(self._file_hash, embedding_model_name)

        # ìºì‹œ ë³´ì•ˆ ê´€ë¦¬ì ì´ˆê¸°í™”
        self.security_manager = CacheSecurityManager(
            security_level=CACHE_SECURITY_LEVEL,
            hmac_secret=CACHE_HMAC_SECRET,
            trusted_paths=CACHE_TRUSTED_PATHS,
            check_permissions=CACHE_CHECK_PERMISSIONS,
        )

    def _get_cache_paths(
        self, file_hash: str, embedding_model_name: str
    ) -> tuple[str, str, str, str]:
        # íŒŒì¼ ê²½ë¡œì— ì•ˆì „í•˜ì§€ ì•Šì€ ë¬¸ì ì œê±°
        model_name_slug = embedding_model_name.replace("/", "_").replace("\\", "_")
        config_hash = _compute_config_hash()

        cache_dir = os.path.join(
            VECTOR_STORE_CACHE_DIR, f"{file_hash}_{model_name_slug}_{config_hash}"
        )

        return (
            cache_dir,
            os.path.join(cache_dir, "doc_splits.json"),  # [.pkl -> .json]
            os.path.join(cache_dir, "faiss_index"),
            os.path.join(cache_dir, "bm25_docs.json"),   # [.pkl -> .json]
        )

    def _purge_cache(self, reason: str):
        """ë³´ì•ˆ ìœ„í˜‘ì´ë‚˜ ì†ìƒì´ ê°ì§€ëœ ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ì™„ì „íˆ ì‚­ì œí•©ë‹ˆë‹¤."""
        if os.path.exists(self.cache_dir):
            try:
                import shutil

                shutil.rmtree(self.cache_dir)
                logger.critical(
                    f"[Security] ìºì‹œ ê°•ì œ ì‚­ì œë¨ ({reason}): {self.cache_dir}"
                )
            except Exception as e:
                logger.error(f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def load(
        self,
        embedder: HuggingFaceEmbeddings,
    ) -> tuple[list[Document] | None, FAISS | None, BM25Retriever | None]:
        """
        ìºì‹œëœ RAG ì»´í¬ë„ŒíŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        ë³´ì•ˆ ì •ì±…:
        - ì‹ ë¢° ê²½ë¡œ ìœ„ë°˜(CacheTrustError) ë˜ëŠ” ë¬´ê²°ì„± ìœ„ë°˜(CacheIntegrityError) ì‹œ
          ë³´ì•ˆ ìˆ˜ì¤€ì— ìƒê´€ì—†ì´ ìºì‹œë¥¼ ì¦‰ì‹œ ì‚­ì œí•˜ê³  ë¡œë“œë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
        """
        if not all(
            os.path.exists(p)
            for p in [
                self.doc_splits_path,
                self.faiss_index_path,
                self.bm25_retriever_path,
            ]
        ):
            return None, None, None

        try:
            from langchain_community.vectorstores import FAISS

            # --- 1. ë³´ì•ˆ ë° ì‹ ë¢° ê²€ì¦ ---
            # ëª¨ë“  ì£¼ìš” ìºì‹œ ì»´í¬ë„ŒíŠ¸ì— ëŒ€í•´ ë£¨í”„ë¥¼ ëŒë©° ê²€ì¦ ìˆ˜í–‰
            paths_to_verify = [
                (self.doc_splits_path, "ë¬¸ì„œ ë°ì´í„°"),
                (self.faiss_index_path, "FAISS ì¸ë±ìŠ¤"),
                (self.bm25_retriever_path, "BM25 ë¦¬íŠ¸ë¦¬ë²„"),
            ]

            for path, desc in paths_to_verify:
                try:
                    # ì‹ ë¢° ê²½ë¡œ ê²€ì¦ (ëª¨ë“  íŒŒì¼/í´ë”)
                    self.security_manager.verify_cache_trust(path)

                    # ë¬´ê²°ì„± ê²€ì¦ (íŒŒì¼ì¸ ê²½ìš°ì—ë§Œ SHA256 ì²´í¬)
                    if os.path.isfile(path):
                        # [ë³´ì•ˆ ê°•í™”] ê³ ë³´ì•ˆ ë ˆë²¨ì—ì„œëŠ” HMAC ê²€ì¦ì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ ë¹„ë°€í‚¤ê°€ ì—†ìœ¼ë©´ ë¡œë“œë¥¼ ê±°ë¶€í•¨
                        if CACHE_SECURITY_LEVEL == "high" and not CACHE_HMAC_SECRET:
                            raise CacheIntegrityError(
                                f"ë³´ì•ˆ ë ˆë²¨ 'high'ì—ì„œëŠ” HMAC ë¹„ë°€í‚¤ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤: {path}"
                            )
                        self.security_manager.verify_cache_integrity(path)
                    elif os.path.isdir(path):
                        # FAISS ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ë‚´ì˜ í•µì‹¬ íŒŒì¼ ê²€ì¦
                        index_file = os.path.join(path, "index.faiss")
                        if os.path.exists(index_file):
                            self.security_manager.verify_cache_integrity(index_file)

                except (CacheTrustError, CacheIntegrityError) as e:
                    self._purge_cache(
                        reason=f"Security Violation in {desc}: {type(e).__name__}"
                    )
                    return None, None, None
                except CachePermissionError as e:
                    if CACHE_SECURITY_LEVEL == "high":
                        self._purge_cache(reason=f"Permission Violation in {desc}")
                        return None, None, None
                    logger.warning(f"ìºì‹œ ê¶Œí•œ ê²½ê³  ({desc}): {e}")

            # --- 2. ë°ì´í„° ë¡œë“œ (ëª¨ë“  ê²€ì¦ í†µê³¼ í›„) ---
            # 1. ë¬¸ì„œ ë¡œë“œ (Pickle ëŒ€ì‹  JSON ì‚¬ìš©)
            with open(self.doc_splits_path, "r", encoding="utf-8") as f:
                doc_dicts = json.load(f)
            doc_splits = _deserialize_docs(doc_dicts)

            # 2. FAISS ë¡œë“œ
            # ì´ë¯¸ ìœ„ì—ì„œ ë¬´ê²°ì„±/ì‹ ë¢° ê²€ì¦ì„ ë§ˆì³¤ìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ë¡œë“œ
            vector_store = FAISS.load_local(
                self.faiss_index_path,
                embedder,
                allow_dangerous_deserialization=True,
            )

            # 3. BM25 ë¡œë“œ (Rebuild from JSON docs)
            with open(self.bm25_retriever_path, "r", encoding="utf-8") as f:
                bm25_doc_dicts = json.load(f)
            bm25_docs = _deserialize_docs(bm25_doc_dicts)
            
            from langchain_community.retrievers import BM25Retriever
            bm25_retriever = BM25Retriever.from_documents(bm25_docs)
            bm25_retriever.k = RETRIEVER_CONFIG["search_kwargs"]["k"]

            logger.info(f"RAG ìºì‹œ ì•ˆì „ ë¡œë“œ ì™„ë£Œ (JSON): '{self.cache_dir}'")
            return doc_splits, vector_store, bm25_retriever

        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}. ìºì‹œë¥¼ íê¸°í•©ë‹ˆë‹¤.")
            self._purge_cache(reason="Load Error / Corruption")
            return None, None, None

    def save(
        self,
        doc_splits: DocumentList,
        vector_store: FAISS,
        bm25_retriever: BM25Retriever,
    ) -> None:
        """
        RAG ì»´í¬ë„ŒíŠ¸ë¥¼ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤.
        ì €ì¥ í›„ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ ê²€ì¦í•˜ì—¬ ë¬´ê²°ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
        """
        try:
            os.makedirs(self.cache_dir, exist_ok=True)

            # 1. ë¬¸ì„œ ì €ì¥ (Pickle ëŒ€ì‹  JSON ì‚¬ìš©)
            try:
                serialized_splits = _serialize_docs(doc_splits)
                with open(self.doc_splits_path, "w", encoding="utf-8") as f:
                    json.dump(serialized_splits, f, ensure_ascii=False)

                # ë¬¸ì„œ ìºì‹œ ë©”íƒ€ë°ì´í„° ìƒì„±
                doc_meta = self.security_manager.create_metadata_for_file(
                    self.doc_splits_path, description="Document splits cache (JSON)"
                )
                self.security_manager.save_cache_metadata(
                    self.doc_splits_path + ".meta", doc_meta
                )
            except Exception as e:
                logger.error(f"ë¬¸ì„œ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
                raise OSError(f"Failed to save doc_splits: {e}") from e

            # 2. FAISS ì €ì¥
            try:
                vector_store.save_local(self.faiss_index_path)

                # [ìˆ˜ì •] FAISS ë‚´ë¶€ í•µì‹¬ íŒŒì¼ë“¤(index.faiss, index.pkl)ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ìƒì„±
                for filename in ["index.faiss", "index.pkl"]:
                    file_p = os.path.join(self.faiss_index_path, filename)
                    if os.path.exists(file_p):
                        meta = self.security_manager.create_metadata_for_file(
                            file_p, description=f"FAISS index part: {filename}"
                        )
                        self.security_manager.save_cache_metadata(
                            file_p + ".meta", meta
                        )
            except Exception as e:
                logger.error(f"FAISS ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                raise OSError(f"Failed to save FAISS index: {e}") from e

            # 3. BM25 ì €ì¥ (JSON ì‚¬ìš© - ë‚´ë¶€ ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì—¬ ì €ì¥)
            try:
                # BM25Retrieverì—ì„œ ë¬¸ì„œ ì¶”ì¶œ
                bm25_docs = getattr(bm25_retriever, "docs", doc_splits)
                serialized_bm25 = _serialize_docs(bm25_docs)
                
                with open(self.bm25_retriever_path, "w", encoding="utf-8") as f:
                    json.dump(serialized_bm25, f, ensure_ascii=False)

                metadata = self.security_manager.create_metadata_for_file(
                    self.bm25_retriever_path, description="BM25 docs cache (JSON)"
                )

                if CACHE_SECURITY_LEVEL == "high" and CACHE_HMAC_SECRET:
                    with open(self.bm25_retriever_path, "rb") as f:
                        file_data = f.read()
                    metadata.integrity_hmac = (
                        self.security_manager.compute_integrity_hmac(file_data)
                    )

                self.security_manager.save_cache_metadata(
                    self.bm25_retriever_path + ".meta", metadata
                )
            except Exception as e:
                logger.error(f"BM25 ë¦¬íŠ¸ë¦¬ë²„ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
                raise OSError(f"Failed to save BM25 docs: {e}") from e

            # --- ê²€ì¦ ë‹¨ê³„ (Verification) ---
            required_paths = [
                self.doc_splits_path,
                os.path.join(self.faiss_index_path, "index.faiss"),
                os.path.join(self.faiss_index_path, "index.pkl"),
                self.bm25_retriever_path,
            ]

            missing = [p for p in required_paths if not os.path.exists(p)]
            if missing:
                error_msg = f"ìºì‹œ ì €ì¥ ê²€ì¦ ì‹¤íŒ¨. ëˆ„ë½ëœ íŒŒì¼: {missing}"
                logger.error(error_msg)
                self._purge_cache(reason="Verification Failed After Save")
                raise OSError(error_msg)

            logger.info(f"RAG ìºì‹œ ì €ì¥ ë° ê²€ì¦ ì™„ë£Œ: '{self.cache_dir}'")
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            self._purge_cache(reason=f"Exception during save: {type(e).__name__}")
            raise


@log_operation("FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±")
def _create_vector_store(
    docs: list[Document],
    embedder: HuggingFaceEmbeddings,
    vectors: list[np.ndarray] | None = None,
) -> FAISS:
    """
    FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    [ìµœì í™”] ë°ì´í„° ê·œëª¨ì— ë”°ë¼ Flat ë˜ëŠ” HNSW ì¸ë±ìŠ¤ë¥¼ ìë™ ì„ íƒí•˜ê³  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    """
    from langchain_community.vectorstores import FAISS
    from langchain_community.vectorstores.utils import DistanceStrategy

    # 1. ì„ë² ë”© ë°ì´í„° ì¤€ë¹„ ë° ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©)
    if vectors is None:
        texts = [d.page_content for d in docs]
        vectors_list = embedder.embed_documents(texts)
        vectors = np.array(vectors_list).astype("float32")
    else:
        # [ìˆ˜ì •] list[np.ndarray] í˜•íƒœì¸ ê²½ìš° 2D numpy arrayë¡œ ë³€í™˜
        if isinstance(vectors, list):
            vectors = np.array(vectors).astype("float32")

    # [ìµœì í™”] FAISSì˜ C++ ìµœì í™” ì •ê·œí™” í•¨ìˆ˜ ì‚¬ìš© (SIMD ê°€ì†)
    import faiss
    faiss.normalize_L2(vectors)
    normalized_vectors = vectors

    text_embeddings = list(
        zip([d.page_content for d in docs], normalized_vectors, strict=False)
    )
    metadatas = [d.metadata for d in docs]

    # 2. ë°ì´í„° ê·œëª¨ì— ë”°ë¥¸ ì¸ë±ìŠ¤ ì „ëµ ê²°ì •
    chunk_count = len(docs)

    # LangChain FAISSì—ì„œëŠ” distance_strategyë¥¼ í†µí•´ ë‚´ë¶€ ë©”íŠ¸ë¦­ ê²°ì •
    # METRIC_INNER_PRODUCT + Normalized Vectors = Cosine Similarity
    distance_strategy = DistanceStrategy.MAX_INNER_PRODUCT

    if chunk_count >= 1000:
        # HNSW ì ìš© (ëŒ€ê·œëª¨ ê³ ì† ê²€ìƒ‰)
        logger.info(f"[FAISS] HNSW ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘ (Chunks: {chunk_count})")

        import faiss
        from langchain_community.docstore.in_memory import InMemoryDocstore
        import uuid

        d = normalized_vectors.shape[1]
        # HNSW íŒŒë¼ë¯¸í„°: m=32, ef_construction=128
        index = faiss.IndexHNSWFlat(d, 32, faiss.METRIC_INNER_PRODUCT)
        index.hnsw.efConstruction = 128
        
        # ë²¡í„° ì¶”ê°€
        index.add(normalized_vectors)
        
        # Docstore ë° ID ë§µ ìƒì„±
        docstore = InMemoryDocstore({})
        index_to_docstore_id = {}
        
        for i, (content, metadata) in enumerate(zip([d.page_content for d in docs], metadatas, strict=False)):
            doc_id = str(uuid.uuid4())
            doc = Document(page_content=content, metadata=metadata)
            docstore.add({doc_id: doc})
            index_to_docstore_id[i] = doc_id
            
        vector_store = FAISS(
            embedding_function=embedder,
            index=index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id,
            distance_strategy=distance_strategy
        )
        logger.info("[FAISS] HNSW ìµœì í™” ì¸ë±ìŠ¤ ì§ì ‘ êµ¬ì¶• ì™„ë£Œ")
    else:
        # ì†Œê·œëª¨ëŠ” ì •í™•í•œ ê²€ìƒ‰(Flat) ìˆ˜í–‰
        logger.debug(f"[FAISS] Flat ì¸ë±ìŠ¤ ì‚¬ìš© (Chunks: {chunk_count})")
        vector_store = FAISS.from_embeddings(
            text_embeddings=text_embeddings,
            embedding=embedder,
            metadatas=metadatas,
            distance_strategy=distance_strategy
        )

    return vector_store


def _create_bm25_retriever(docs: list[Document]) -> BM25Retriever:
    from langchain_community.retrievers import BM25Retriever

    retriever = BM25Retriever.from_documents(docs)
    retriever.k = RETRIEVER_CONFIG["search_kwargs"]["k"]
    return retriever


def _create_ensemble_retriever(
    vector_store: FAISS,
    bm25_retriever: BM25Retriever,
) -> EnsembleRetriever:
    from langchain.retrievers import EnsembleRetriever

    faiss_retriever = vector_store.as_retriever(
        search_type=RETRIEVER_CONFIG["search_type"],
        search_kwargs=RETRIEVER_CONFIG["search_kwargs"],
    )
    return EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=RETRIEVER_CONFIG["ensemble_weights"],
    )


@log_operation("ê²€ìƒ‰ ì»´í¬ë„ŒíŠ¸ ë¡œë“œ/ìƒì„±")
def _load_and_build_retrieval_components(
    file_path: str,
    file_name: str,
    _embedder: HuggingFaceEmbeddings,
    embedding_model_name: str,
    _on_progress=None,
    _file_hash: str | None = None
) -> tuple[DocumentList, FAISS, BM25Retriever, bool]:
    cache = VectorStoreCache(file_path, embedding_model_name, file_hash=_file_hash)
    doc_splits, vector_store, bm25_retriever = cache.load(_embedder)

    cache_used = all(x is not None for x in [doc_splits, vector_store, bm25_retriever])

    if not cache_used:
        import numpy as np
        import torch

        docs = _load_pdf_docs(file_path, file_name, on_progress=_on_progress)
        # ë¹ˆ ë¬¸ì„œ ì²˜ë¦¬
        if not docs:
            raise EmptyPDFError(
                filename=file_name,
                details={"reason": "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."},
            )

        if _on_progress:
            _on_progress()

        # [ìµœì í™” 1] ë¬¸ì„œ í¬ê¸°ì— ë”°ë¥¸ ë°”ì´íŒ¨ìŠ¤ ì „ëµ (2000ì ë¯¸ë§Œì€ ê³ ì† ì²˜ë¦¬)
        total_text_len = sum(len(d.page_content) for d in docs)
        is_small_doc = total_text_len < 2000

        # [ìµœì í™” 2] 1ì°¨ ë¶„í•  ë° ì„ë² ë”© ìƒì„± (ë‹¨ì¼ íŒ¨ìŠ¤)
        doc_splits, precomputed_vectors = _split_documents(docs, _embedder)
        if _on_progress:
            _on_progress()

        if not doc_splits:
            raise InsufficientChunksError(chunk_count=0, min_required=1)

        # [ìµœì í™” 3] ë²¡í„° ì¬ì‚¬ìš©ì„ í†µí•œ ì¸ë±ìŠ¤ ìµœì í™”
        optimized_vectors = precomputed_vectors
        if not is_small_doc:
            try:
                SessionManager.add_status_log("ì¸ë±ìŠ¤ ìµœì í™” ì¤‘")
                if _on_progress:
                    _on_progress()

                # ë²¡í„°ê°€ ì—†ìœ¼ë©´(ê¸°ë³¸ ë¶„í• ê¸° ì‚¬ìš© ì‹œ) í•œ ë²ˆë§Œ ìƒì„±
                if optimized_vectors is None:
                    texts = [d.page_content for d in doc_splits]
                    vectors = _embedder.embed_documents(texts)
                    optimized_vectors = [np.array(v) for v in vectors]

                optimizer = get_index_optimizer()
                doc_splits, optimized_vectors, q_meta, stats = optimizer.optimize_index(
                    doc_splits, optimized_vectors
                )

                # [ìˆ˜ì •] ì–‘ìí™”ëœ ë²¡í„°ë¥¼ ì›ë˜ì˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ ë³´ì¥
                if q_meta and q_meta.get("method") != "none":
                    optimized_vectors = optimizer.quantizer.dequantize_vectors(
                        optimized_vectors, q_meta
                    )

                logger.info(f"ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ: ì¤‘ë³µ {stats.pruned_documents}ê°œ ì œê±°")
                SessionManager.replace_last_status_log(
                    f"ì¤‘ë³µ ë‚´ìš© {stats.pruned_documents}ê°œ ì •ë¦¬"
                )
                if _on_progress:
                    _on_progress()
            except Exception as e:
                logger.warning(f"ì¸ë±ìŠ¤ ìµœì í™” ë‹¨ê³„ ê±´ë„ˆëœ€ (ê²½ë¯¸í•œ ì˜¤ë¥˜): {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ doc_splitsì™€ optimized_vectorsëŠ” ìœ ì§€ë¨

        # [ìµœì í™” 4] ê³„ì‚°ëœ ë²¡í„°ë¥¼ FAISSì— ì§ì ‘ ì£¼ì… (GPU ì¶”ê°€ í˜¸ì¶œ 0íšŒ)
        vector_store = _create_vector_store(
            doc_splits, _embedder, vectors=optimized_vectors
        )
        bm25_retriever = _create_bm25_retriever(doc_splits or [])

        # ìºì‹œ ì €ì¥
        cache.save(doc_splits, vector_store, bm25_retriever)

        # [ìµœì í™” 5] GPU ìì› ì¦‰ì‹œ ë°˜í™˜ (Ollamaì™€ì˜ VRAM ê²½í•© ë°©ì§€)
        if torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
                logger.debug("[System] [Memory] CUDA ìºì‹œ ë¹„ìš°ê¸° ì™„ë£Œ")
            except Exception:
                pass

        SessionManager.add_status_log("ì‹ ê·œ ì¸ë±ì‹± ì™„ë£Œ")

    return doc_splits, vector_store, bm25_retriever, cache_used


@log_operation("RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")
def build_rag_pipeline(
    uploaded_file_name: str,
    file_path: str,
    embedder: HuggingFaceEmbeddings,
    on_progress=None,
) -> tuple[str, bool]:
    """
    RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ê³  ì„¸ì…˜ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    # [ìµœì í™”] íŒŒì¼ í•´ì‹œëŠ” ì—¬ê¸°ì„œ í•œ ë²ˆë§Œ ê³„ì‚°í•˜ì—¬ í•˜ìœ„ í•¨ìˆ˜ë¡œ ì „ë‹¬
    file_hash = _compute_file_hash(file_path)

    # [ìµœì í™”] embedder ê°ì²´ëŠ” í•´ì‹±ì—ì„œ ì œì™¸í•˜ê³ , ëª¨ë¸ëª…ê³¼ íŒŒì¼ í•´ì‹œë¥¼ ëª…ì‹œì  í‚¤ë¡œ ì „ë‹¬
    doc_splits, vector_store, bm25_retriever, cache_used = (
        _load_and_build_retrieval_components(
            file_path,
            uploaded_file_name,
            _embedder=embedder,
            embedding_model_name=embedder.model_name,
            _on_progress=on_progress,
            _file_hash=file_hash
        )
    )

    if cache_used:
        SessionManager.add_status_log("ìºì‹œ ë°ì´í„° ë¡œë“œ")
        if on_progress:
            on_progress()

    # [ìµœì í™”] ë³‘ë ¬ ê²€ìƒ‰ì„ ìœ„í•´ ê°œë³„ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ë° ì„¸ì…˜ ì €ì¥
    faiss_retriever = vector_store.as_retriever(
        search_type=RETRIEVER_CONFIG["search_type"],
        search_kwargs=RETRIEVER_CONFIG["search_kwargs"],
    )

    SessionManager.set("faiss_retriever", faiss_retriever)
    SessionManager.set("bm25_retriever", bm25_retriever)

    # ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€ (EnsembleRetrieverë„ ìƒì„±)
    from langchain.retrievers import EnsembleRetriever

    final_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=RETRIEVER_CONFIG["ensemble_weights"],
    )
    rag_engine = build_graph(retriever=final_retriever)

    SessionManager.set("vector_store", vector_store)
    SessionManager.set("rag_engine", rag_engine)
    SessionManager.set("pdf_processed", True)
    SessionManager.add_status_log("ì§ˆë¬¸ ê°€ëŠ¥")

    if on_progress:
        on_progress()

    logger.info(
        f"RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ: '{uploaded_file_name}' (ìºì‹œ ì‚¬ìš©: {cache_used})"
    )

    if cache_used:
        return f"'{uploaded_file_name}' ë¬¸ì„œ ìºì‹œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ", True
    return (
        f"'{uploaded_file_name}' ì‹ ê·œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ"
    ), False


@log_operation("íŒŒì´í”„ë¼ì¸ LLM ì—…ë°ì´íŠ¸")
def update_llm_in_pipeline(llm: T | None) -> None:
    """
    ì„¸ì…˜ì˜ LLMì„ êµì²´í•©ë‹ˆë‹¤.

    ê·¸ë˜í”„ê°€ ì„¸ì…˜ì—ì„œ LLMì„ ê°€ì ¸ì˜¤ë¯€ë¡œ ì¬ë¹Œë“œí•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.

    Args:
        llm: ì—…ë°ì´íŠ¸í•  ìƒˆë¡œìš´ LLM ëª¨ë¸.

    Raises:
        ValueError: RAG íŒŒì´í”„ë¼ì¸ì´ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ì„ ë•Œ.
    """
    if not SessionManager.get("pdf_processed"):
        raise ValueError("RAG íŒŒì´í”„ë¼ì¸ì´ êµ¬ì¶•ë˜ì§€ ì•Šì•„ LLMì„ ì—…ë°ì´íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    SessionManager.set("llm", llm)
    logger.info(f"ì„¸ì…˜ LLM ì—…ë°ì´íŠ¸ ì™„ë£Œ: '{getattr(llm, 'model', 'unknown')}'")
