"""
RAG ì‹œìŠ¤í…œì˜ í†µí•© ì—”ì§„ (Core Engine).
ë¬¸ì„œ ë¡œë”©, ì¸ë±ì‹±, ê²€ìƒ‰, ì§ˆì˜ì‘ë‹µì˜ ëª¨ë“  ê³¼ì •ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import contextlib
import copy
import gc
import logging
from typing import Any

import torch
from langchain_core.embeddings import Embeddings

from cache.vector_cache import VectorStoreCache
from common.config import ENABLE_VECTOR_CACHE, RETRIEVER_CONFIG
from common.exceptions import (
    EmptyPDFError,
    InsufficientChunksError,
    VectorStoreError,
)
from common.typing_utils import T
from core.chunking import split_documents
from core.document_processor import compute_file_hash, load_pdf_docs
from core.graph_builder import build_graph
from core.resource_pool import get_resource_pool
from core.retriever_factory import create_bm25_retriever, create_vector_store
from core.session import SessionManager
from services.optimization.index_optimizer import get_index_optimizer

logger = logging.getLogger(__name__)


class RAGSystem:
    """
    RAG ì‹œìŠ¤í…œì˜ í†µí•© ì¸í„°í˜ì´ìŠ¤.
    ì¸ë±ì‹±ë¶€í„° ì§ˆì˜ì‘ë‹µê¹Œì§€ì˜ ì „ì²´ ë¼ì´í”„ì‚¬ì´í´ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """

    def __init__(self, session_id: str = "default"):
        self.session_id = session_id
        SessionManager.init_session(session_id=session_id)

    def _ensure_session_context(self) -> None:
        """í˜„ì¬ ìŠ¤ë ˆë“œì˜ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤."""
        SessionManager.set_session_id(self.session_id)

    async def build_pipeline(
        self, file_path: str, file_name: str, embedder: Embeddings, on_progress=None
    ) -> tuple[str, bool]:
        """ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        self._ensure_session_context()

        file_hash = compute_file_hash(file_path)
        SessionManager.set("file_hash", file_hash, session_id=self.session_id)

        emb_model_name = getattr(
            embedder, "model", getattr(embedder, "model_name", "unknown")
        )
        cache = VectorStoreCache(file_path, emb_model_name, file_hash=file_hash)

        # 1. ìºì‹œ ì‹œë„
        if ENABLE_VECTOR_CACHE:
            cache_data = cache.load(embedder)
            if all(x is not None for x in cache_data):
                doc_splits, vector_store, bm25_retriever = cache_data
                SessionManager.add_status_log(
                    "âš¡ ê¸°ì¡´ ë¶„ì„ ë°ì´í„° ë°œê²¬ (ìºì‹œ í™œìš©)", session_id=self.session_id
                )
                await self._register_and_finalize(
                    file_hash, vector_store, bm25_retriever, on_progress
                )
                return f"'{file_name}' ìºì‹œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ", True

        # 2. ì‹ ê·œ ë¬¸ì„œ ë¡œë“œ
        docs = load_pdf_docs(
            file_path, file_name, on_progress=on_progress, session_id=self.session_id
        )
        if not docs:
            raise EmptyPDFError(
                filename=file_name, details={"reason": "í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            )

        # 3. ì–¸ì–´ ê°ì§€
        sample_text = docs[0].page_content[:1000]
        lang = (
            "Korean"
            if any("\uac00" <= char <= "\ud7a3" for char in sample_text)
            else "English"
        )
        SessionManager.set("doc_language", lang, session_id=self.session_id)

        # 4. ì²­í‚¹ ë° ë²¡í„°í™”
        doc_splits, vectors = await split_documents(
            docs, embedder, session_id=self.session_id
        )
        if not doc_splits:
            raise InsufficientChunksError(chunk_count=0, min_required=1)

        # 5. ì¸ë±ìŠ¤ ìµœì í™” (ì„ íƒì : ì¤‘ë³µ ì œê±° ë° ë©”íƒ€ë°ì´í„° ì¸ë±ì‹±)
        if sum(len(d.page_content) for d in docs) >= 2000 and vectors is not None:
            with contextlib.suppress(Exception):
                from services.optimization.index_optimizer import (
                    CompressionMethod,
                    IndexOptimizationConfig,
                    VectorQuantizationConfig,
                )

                # [ìµœì í™”] ìˆ˜ë™ ì–‘ìí™”ëŠ” ì˜¤ë²„í—¤ë“œê°€ í¬ê³  FAISS ë‚´ì¥ ê¸°ëŠ¥ë³´ë‹¤ ë¹„íš¨ìœ¨ì ì´ë¯€ë¡œ NONEìœ¼ë¡œ ì„¤ì •
                opt_config = IndexOptimizationConfig(
                    quantization_config=VectorQuantizationConfig(
                        compression_method=CompressionMethod.NONE
                    )
                )
                optimizer = get_index_optimizer(opt_config)
                doc_splits, vectors, _, stats = optimizer.optimize_index(
                    doc_splits, vectors
                )
                SessionManager.add_status_log(
                    f"ğŸ§¹ ë°ì´í„° ìµœì í™” ì™„ë£Œ ({stats.pruned_documents}ê°œ ì¤‘ë³µ ì œê±°)",
                    session_id=self.session_id,
                )

        # 6. ì»´í¬ë„ŒíŠ¸ ìƒì„±
        vector_store = create_vector_store(doc_splits, embedder, vectors=vectors)
        bm25_retriever = create_bm25_retriever(doc_splits)

        # 7. ìºì‹œ ì €ì¥
        if ENABLE_VECTOR_CACHE:
            cache.save(doc_splits, vector_store, bm25_retriever)

        # 8. ìµœì¢… ë“±ë¡
        await self._register_and_finalize(
            file_hash, vector_store, bm25_retriever, on_progress
        )

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            with contextlib.suppress(Exception):
                torch.cuda.empty_cache()
        gc.collect()

        return f"'{file_name}' ì‹ ê·œ ì¸ë±ì‹± ì™„ë£Œ", False

    async def _register_and_finalize(
        self, file_hash, vector_store, bm25_retriever, on_progress
    ):
        """ë¦¬ì†ŒìŠ¤ë¥¼ ë“±ë¡í•˜ê³  íŒŒì´í”„ë¼ì¸ì„ ìµœì¢… ì¡°ë¦½í•©ë‹ˆë‹¤."""
        await get_resource_pool().register(file_hash, vector_store, bm25_retriever)
        SessionManager.set("rag_engine", build_graph(), session_id=self.session_id)
        SessionManager.set("pdf_processed", True, session_id=self.session_id)
        SessionManager.add_status_log(
            "âœ¨ ê²€ìƒ‰ ì—”ì§„ êµ¬ì¶• ì™„ë£Œ", session_id=self.session_id
        )
        if on_progress:
            on_progress()

    async def aquery(self, query: str, llm: T | None = None) -> dict[str, Any]:
        """[ê¸°ë³¸] ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë¹„ë™ê¸°ë¡œ ìƒì„±í•©ë‹ˆë‹¤ (Full Response)."""
        self._ensure_session_context()
        config = await self._prepare_config(llm)
        rag_engine = SessionManager.get("rag_engine", session_id=self.session_id)
        if not rag_engine:
            raise VectorStoreError(
                details={"reason": "íŒŒì´í”„ë¼ì¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            )
        return await rag_engine.ainvoke({"input": query}, config=config)

    async def astream_events(self, query: str, llm: T | None = None):
        """[ìŠ¤íŠ¸ë¦¬ë°] ì§ˆë¬¸ì— ëŒ€í•œ ì´ë²¤íŠ¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤."""
        self._ensure_session_context()
        config = await self._prepare_config(llm)
        rag_engine = SessionManager.get("rag_engine", session_id=self.session_id)
        if not rag_engine:
            raise VectorStoreError(
                details={"reason": "íŒŒì´í”„ë¼ì¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            )
        return rag_engine.astream_events({"input": query}, config=config, version="v2")

    async def load_document(
        self, file_path: str, file_name: str, embedder: Embeddings, on_progress=None
    ) -> tuple[str, bool]:
        """build_pipelineì˜ í•˜ìœ„ í˜¸í™˜ì„± ì—ì¼ë¦¬ì–´ìŠ¤"""
        return await self.build_pipeline(file_path, file_name, embedder, on_progress)

    async def _prepare_config(self, llm: T | None = None) -> dict:
        """ê²€ìƒ‰ê¸° ë° ëª¨ë¸ ì„¤ì •ì„ í¬í•¨í•œ ì‹¤í–‰ Configë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."""
        if llm:
            SessionManager.set("llm", llm, session_id=self.session_id)

        file_hash = SessionManager.get("file_hash", session_id=self.session_id)
        vector_store, bm25_shared = await get_resource_pool().get(file_hash)

        # ë¦¬ì†ŒìŠ¤ ë³µêµ¬ ë¡œì§
        if not vector_store:
            await self.build_pipeline(
                SessionManager.get("pdf_file_path", session_id=self.session_id),
                SessionManager.get(
                    "last_uploaded_file_name", session_id=self.session_id
                ),
                SessionManager.get("embedder", session_id=self.session_id),
            )
            vector_store, bm25_shared = await get_resource_pool().get(file_hash)

        faiss_ret = (
            vector_store.as_retriever(
                search_type=RETRIEVER_CONFIG["search_type"],
                search_kwargs=RETRIEVER_CONFIG["search_kwargs"],
            )
            if vector_store
            else None
        )

        bm25_ret = copy.copy(bm25_shared) if bm25_shared else None
        if bm25_ret:
            bm25_ret.k = RETRIEVER_CONFIG.get("search_kwargs", {}).get("k", 5)

        return {
            "configurable": {
                "llm": SessionManager.get("llm", session_id=self.session_id),
                "session_id": self.session_id,
                "faiss_retriever": faiss_ret,
                "bm25_retriever": bm25_ret,
                "doc_language": SessionManager.get(
                    "doc_language", session_id=self.session_id
                ),
            }
        }

    def get_status(self) -> list[str]:
        self._ensure_session_context()
        return SessionManager.get("status_logs", session_id=self.session_id) or []

    def clear_session(self) -> None:
        self._ensure_session_context()
        SessionManager.reset_all_state(session_id=self.session_id)
