"""
RAG íŒŒì´í”„ë¼ì¸ì˜ í•µì‹¬ ë¡œì§(ë°ì´í„° ì²˜ë¦¬, ì„ë² ë”©, ê²€ìƒ‰, ìƒì„±)ì„ ë‹´ë‹¹í•˜ëŠ” íŒŒì¼.
"""

from __future__ import annotations

import functools
import hashlib
import json
import logging
import os
import threading
from collections.abc import Callable
from typing import TYPE_CHECKING, Any

import numpy as np

from common.typing_utils import (
    DocumentDictList,
    DocumentList,
    T,
)

if TYPE_CHECKING:
    from langchain_classic.retrievers import EnsembleRetriever
    from langchain_community.retrievers import BM25Retriever
    from langchain_community.vectorstores import FAISS
    from langchain_core.documents import Document
    from langchain_huggingface import HuggingFaceEmbeddings

from common.config import (
    CACHE_CHECK_PERMISSIONS,
    CACHE_HMAC_SECRET,
    CACHE_SECURITY_LEVEL,
    CACHE_TRUSTED_PATHS,
    RETRIEVER_CONFIG,
    SEMANTIC_CHUNKER_CONFIG,
    TEXT_SPLITTER_CONFIG,
    VECTOR_STORE_CACHE_DIR,
)
from common.exceptions import (
    EmptyPDFError,
    InsufficientChunksError,
    PDFProcessingError,
    VectorStoreError,
)
from common.utils import log_operation, preprocess_text
from core.graph_builder import build_graph
from core.semantic_chunker import EmbeddingBasedSemanticChunker
from core.session import SessionManager
from security.cache_security import (
    CacheIntegrityError,
    CachePermissionError,
    CacheSecurityManager,
    CacheTrustError,
)
from services.monitoring.performance_monitor import (
    OperationType,
    get_performance_monitor,
)
from services.optimization.index_optimizer import get_index_optimizer

logger = logging.getLogger(__name__)
monitor = get_performance_monitor()


import re

# --- ìµœì í™”ëœ í† í¬ë‚˜ì´ì € ---
_RE_KOREAN_TOKEN = re.compile(r"[ê°€-í£]{2,}|[a-zA-Z]{2,}|[0-9]+")


def bm25_tokenizer(text: str) -> list[str]:
    """
    [ìµœì í™”] í•œêµ­ì–´ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ Hybrid í† í¬ë‚˜ì´ì €.
    ê¸°ë³¸ ì •ê·œì‹ ì¶”ì¶œ + ì–´ë¯¸ ì œê±° + Bi-gram ìƒì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    if not text:
        return []

    # 1. ê¸°ë³¸ í† í° ì¶”ì¶œ
    tokens = _RE_KOREAN_TOKEN.findall(text.lower())
    if not tokens:
        return text.split()

    final_tokens = []
    # ìì£¼ ì“°ì´ëŠ” ì¡°ì‚¬/ì–´ë¯¸ (ê°„ì´ ë¶ˆìš©ì–´ ì²˜ë¦¬)
    particles = ("ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ë¡œ", "ì„œ", "ë“¤")

    for token in tokens:
        final_tokens.append(token)

        # í•œê¸€ì¸ ê²½ìš° ì¶”ê°€ ì²˜ë¦¬
        if "ê°€" <= token[0] <= "í£":
            # 2. ê°„ë‹¨í•œ ì–´ë¯¸/ì¡°ì‚¬ ì œê±° (ëê¸€ì ì²´í¬)
            if len(token) > 2 and token.endswith(particles):
                stem = token[:-1]
                if len(stem) >= 2:
                    final_tokens.append(stem)

            # 3. Bi-gram ìƒì„± (3ê¸€ì ì´ìƒì¸ ê²½ìš°)
            # ë³µí•©ëª…ì‚¬ ê²€ìƒ‰ ì¬í˜„ìœ¨ í–¥ìƒ (ì˜ˆ: ì¸ê³µì§€ëŠ¥ -> ì¸ê³µ, ê³µì§€, ì§€ëŠ¥)
            if len(token) >= 3:
                for i in range(len(token) - 1):
                    final_tokens.append(token[i : i + 2])

    return final_tokens


class RAGSystem:
    """
    RAG ì‹œìŠ¤í…œì˜ í†µí•© ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ í´ë˜ìŠ¤.
    ì„¸ì…˜ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬ì™€ LangGraph ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì„ ì—°ê²°í•©ë‹ˆë‹¤.
    """

    def __init__(self, session_id: str = "default"):
        self.session_id = session_id
        SessionManager.init_session(session_id=session_id)

    def _ensure_session_context(self) -> None:
        """í˜„ì¬ ìŠ¤ë ˆë“œì˜ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤."""
        SessionManager.set_session_id(self.session_id)

    async def load_document(
        self, file_path: str, file_name: str, embedder: HuggingFaceEmbeddings
    ) -> tuple[str, bool]:
        """
        ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            file_path: PDF íŒŒì¼ì˜ ë¡œì»¬ ê²½ë¡œ
            file_name: ì‚¬ìš©ìì—ê²Œ í‘œì‹œë  íŒŒì¼ ì´ë¦„
            embedder: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

        Returns:
            Tuple[ì„±ê³µ ë©”ì‹œì§€, ìºì‹œ ì‚¬ìš© ì—¬ë¶€]
        """
        self._ensure_session_context()
        return build_rag_pipeline(
            uploaded_file_name=file_name, file_path=file_path, embedder=embedder
        )

    async def aquery(self, query: str, llm: T | None = None) -> dict[str, Any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            llm: ì‚¬ìš©í•  LLM ì¸ìŠ¤í„´ìŠ¤ (ìƒëµ ì‹œ ì„¸ì…˜ì— ì €ì¥ëœ ëª¨ë¸ ì‚¬ìš©)

        Returns:
            GraphOutput êµ¬ì¡°ì˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self._ensure_session_context()

        if llm:
            SessionManager.set("llm", llm)

        rag_engine = SessionManager.get("rag_engine")
        if not rag_engine:
            raise VectorStoreError(
                details={
                    "reason": "RAG ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”."
                }
            )

        current_llm = SessionManager.get("llm")
        config = {"configurable": {"llm": current_llm}}

        # LangGraph í˜¸ì¶œ
        return await rag_engine.ainvoke({"input": query}, config=config)

    def get_status(self) -> list[str]:
        """í˜„ì¬ ì„¸ì…˜ì˜ ì‘ì—… ë¡œê·¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        self._ensure_session_context()
        return SessionManager.get("status_logs", [])

    def clear_session(self) -> None:
        """ì„¸ì…˜ ë°ì´í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self._ensure_session_context()
        SessionManager.reset_all_state()

    def process_documents(self, documents: list[str]) -> None:
        """ì…ë ¥ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤. (í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„± ìœ ì§€)"""
        if not documents:
            raise EmptyPDFError(
                filename="(in-memory)",
                details={"reason": "documents list is empty"},
            )


def _compute_file_hash(file_path: str, data: bytes | None = None) -> str:
    """
    íŒŒì¼ ë˜ëŠ” ë°ì´í„°ì˜ SHA256 í•´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    [ìµœì í™”] ë°ì´í„°ê°€ ì´ë¯¸ ë©”ëª¨ë¦¬ì— ìˆë‹¤ë©´ íŒŒì¼ì„ ë‹¤ì‹œ ì½ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    sha256_hash = hashlib.sha256()
    try:
        if data is not None:
            sha256_hash.update(data)
        else:
            with open(file_path, "rb") as f:
                for byte_block in iter(lambda: f.read(8192), b""):
                    sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except Exception as e:
        logger.error(f"í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return ""


import concurrent.futures

import fitz  # PyMuPDF
from langchain_core.documents import Document


def _extract_page_worker(
    file_path: str, page_num: int, total_pages: int, file_name: str
) -> Document | None:
    """ê°œë³„ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ (ìŠ¤ë ˆë“œ ì„¸ì´í”„)"""
    try:
        # ê° ìŠ¤ë ˆë“œì—ì„œ íŒŒì¼ì„ ìƒˆë¡œ ì—´ì–´ ë…ë¦½ì ì¸ ë¬¸ì„œ ê°ì²´ ì‚¬ìš©
        with fitz.open(file_path) as doc:
            page = doc[page_num]
            text = page.get_text()
            if text:
                clean_text = preprocess_text(text)
                if clean_text and len(clean_text) > 10:
                    metadata = {
                        "source": file_name,
                        "page": int(page_num + 1),
                        "total_pages": int(total_pages),
                    }
                    return Document(page_content=clean_text, metadata=metadata)
    except Exception as e:
        logger.warning(f"í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return None


def _extract_pages_batch_worker(
    file_bytes: bytes | None,
    file_path: str,
    page_range: list[int],
    total_pages: int,
    file_name: str,
) -> list[tuple[int, Document]]:
    """í˜ì´ì§€ ë²”ìœ„ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ (í•˜ì´ë¸Œë¦¬ë“œ ë¡œë”© ì§€ì›)"""
    results = []
    doc = None
    try:
        # [ìµœì í™”] í•˜ì´ë¸Œë¦¬ë“œ ë¡œë”© ì „ëµ
        # ì‘ì€ íŒŒì¼ -> ë©”ëª¨ë¦¬ë·° (ì†ë„), í° íŒŒì¼ -> íŒŒì¼ ê²½ë¡œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if file_bytes is not None:
            mv = memoryview(file_bytes)
            doc = fitz.open(stream=mv, filetype="pdf")
        else:
            doc = fitz.open(file_path)

        for page_num in page_range:
            try:
                page = doc[page_num]
                text = page.get_text()
                if text:
                    clean_text = preprocess_text(text)
                    if clean_text and len(clean_text) > 10:
                        metadata = {
                            "source": file_name,
                            "page": int(page_num + 1),
                            "total_pages": int(total_pages),
                        }
                        results.append(
                            (
                                page_num,
                                Document(page_content=clean_text, metadata=metadata),
                            )
                        )
            except Exception as e:
                logger.warning(f"í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ë¬¸ì„œ ì˜¤í”ˆ ì‹¤íŒ¨: {e}")
    finally:
        if doc:
            doc.close()
    return results


def _load_pdf_docs(
    file_path: str,
    file_name: str,
    on_progress: Callable[[], None] | None = None,
    file_bytes: bytes | None = None,
    session_id: str | None = None,
) -> list[Document]:
    """
    PDF íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ë²„í¼ë§í•œ í›„ ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•´ ìµœê³  ì†ë„ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    [ìµœì í™”] 50MB ì´ìƒ íŒŒì¼ì€ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì „í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ í­ì¦ ë°©ì§€.
    """
    # ë©”ëª¨ë¦¬ ë¡œë”© ì œí•œ ì„¤ì • (50MB)
    MEMORY_LOAD_LIMIT = 50 * 1024 * 1024

    with monitor.track_operation(OperationType.PDF_LOADING, {"file": file_name}) as op:
        try:
            SessionManager.add_status_log("ë¬¸ì„œ ë¶„ì„ ì¤€ë¹„ ì¤‘", session_id=session_id)
            if on_progress:
                on_progress()

            file_size = os.path.getsize(file_path)
            use_memory_loading = file_size <= MEMORY_LOAD_LIMIT

            # 1. íŒŒì¼ ë¡œë”© ì „ëµ ê²°ì •
            # ì´ë¯¸ ë¡œë“œëœ ë°”ì´íŠ¸ê°€ ì—†ê³ , íŒŒì¼ì´ ì‘ìœ¼ë©´ ë©”ëª¨ë¦¬ì— ë¡œë“œ
            try:
                if file_bytes is None and use_memory_loading:
                    with open(file_path, "rb") as f:
                        file_bytes = f.read()

                # í° íŒŒì¼ì€ file_bytesë¥¼ Noneìœ¼ë¡œ ìœ ì§€í•˜ì—¬ ì›Œì»¤ê°€ ì§ì ‘ ì½ê²Œ í•¨
                if not use_memory_loading:
                    file_bytes = None
                    logger.info(
                        f"[PDF] ëŒ€ìš©ëŸ‰ íŒŒì¼ ê°ì§€({file_size / 1024 / 1024:.1f}MB). ì§ì ‘ ì ‘ê·¼ ëª¨ë“œ ì‚¬ìš©."
                    )

                # ë©”íƒ€ë°ì´í„° í™•ì¸ìš© (ê²½ëŸ‰ ì˜¤í”ˆ)
                doc_chk = fitz.open(file_path)
                total_pages = len(doc_chk)
                doc_chk.close()

            except Exception as e:
                raise PDFProcessingError(
                    filename=file_name,
                    details={"reason": f"íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŒ: {str(e)}"},
                ) from e

            if total_pages == 0:
                raise EmptyPDFError(
                    filename=file_name, details={"reason": "PDF í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."}
                )

            # [ìµœì í™”] í˜ì´ì§€ ìˆ˜ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë³‘ë ¬í™” ê²°ì •
            if total_pages <= 3:
                docs = []
                # ì‘ì€ ë¬¸ì„œëŠ” ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬
                doc = fitz.open(file_path)
                for i in range(total_pages):
                    page = doc[i]
                    text = page.get_text()
                    if text:
                        clean_text = preprocess_text(text)
                        if clean_text and len(clean_text) > 10:
                            docs.append(
                                Document(
                                    page_content=clean_text,
                                    metadata={
                                        "source": file_name,
                                        "page": i + 1,
                                        "total_pages": total_pages,
                                    },
                                )
                            )
                doc.close()
            else:
                # [ìµœì í™”] ì›Œì»¤ ìˆ˜ë¥¼ ë…¼ë¦¬ ì½”ì–´ ìˆ˜ì— ë§ì¶”ê³  ë°°ì¹˜ë¥¼ í¬ê²Œ ì„¤ì •í•˜ì—¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
                cpu_count = os.cpu_count() or 4
                max_workers = min(cpu_count, 16)

                # ë„ˆë¬´ ì¦ì€ ì“°ë ˆë“œ ìƒì„± ë°©ì§€ë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ìƒí–¥
                batch_size = max(4, total_pages // (max_workers * 2))
                batches = [
                    list(range(i, min(i + batch_size, total_pages)))
                    for i in range(0, total_pages, batch_size)
                ]

                all_results = [None] * total_pages
                completed_pages = 0

                with concurrent.futures.ThreadPoolExecutor(
                    max_workers=max_workers
                ) as executor:
                    futures = [
                        executor.submit(
                            _extract_pages_batch_worker,
                            file_bytes,  # í° íŒŒì¼ì´ë©´ None ì „ë‹¬
                            file_path,  # íŒŒì¼ ê²½ë¡œ ì „ë‹¬
                            batch,
                            total_pages,
                            file_name,
                        )
                        for batch in batches
                    ]

                    for future in concurrent.futures.as_completed(futures):
                        batch_results = future.result()
                        for page_idx, doc in batch_results:
                            all_results[page_idx] = doc

                        completed_pages += len(batch_results)
                        SessionManager.replace_last_status_log(
                            f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ({min(completed_pages, total_pages)}/{total_pages}p)",
                            session_id=session_id,
                        )
                        if on_progress:
                            import contextlib

                            with contextlib.suppress(Exception):
                                on_progress()

                docs = [r for r in all_results if r is not None]

            if not docs:
                raise EmptyPDFError(
                    filename=file_name,
                    details={"reason": "í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."},
                )

            SessionManager.replace_last_status_log(
                f"ì¶”ì¶œ ì™„ë£Œ ({len(docs)}p)", session_id=session_id
            )
            op.tokens = sum(len(doc.page_content.split()) for doc in docs)
            logger.info(
                f"[RAG] [LOAD] PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ | íŒŒì¼: {file_name} | í˜ì´ì§€: {len(docs)}"
            )
            return docs
        except (PDFProcessingError, EmptyPDFError):
            raise
        except Exception as e:
            logger.error(f"[RAG] [LOAD] PDF ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ | {e}")
            raise PDFProcessingError(
                filename=file_name, details={"reason": str(e)}
            ) from e


def _split_documents(
    docs: list[Document],
    embedder: HuggingFaceEmbeddings | None = None,
    session_id: str | None = None,
) -> tuple[list[Document], list[np.ndarray] | None]:
    """
    ì„¤ì •ì— ë”°ë¼ ì˜ë¯¸ë¡ ì  ë¶„í• ê¸° ë˜ëŠ” RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
    """
    SessionManager.add_status_log("ë¬¸ì¥ ë¶„í•  ì¤‘...", session_id=session_id)
    with monitor.track_operation(
        OperationType.SEMANTIC_CHUNKING, {"doc_count": len(docs)}
    ) as op:
        from langchain_text_splitters import RecursiveCharacterTextSplitter

        use_semantic = SEMANTIC_CHUNKER_CONFIG.get("enabled", False)
        split_docs = []
        vectors = None

        if use_semantic and embedder:
            # [ìµœì í™”] ë””ë°”ì´ìŠ¤ ë° VRAM ìƒí™©ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸° ë™ì  í• ë‹¹
            import torch

            if getattr(embedder, "model_kwargs", {}).get("device") == "cuda":
                # GPU í™˜ê²½: ê°€ìš© ë©”ëª¨ë¦¬ì— ë”°ë¼ 32~128 ì‚¬ì´ì—ì„œ ê²°ì •
                try:
                    total_mem = torch.cuda.get_device_properties(0).total_memory / (
                        1024**3
                    )  # GB
                    batch_size = (
                        128 if total_mem > 10 else (64 if total_mem > 4 else 32)
                    )
                except Exception:
                    batch_size = 32
            else:
                # CPU í™˜ê²½: ì½”ì–´ ìˆ˜ì— ë§ì¶° 4~16 ì‚¬ì´ì—ì„œ ê²°ì •
                batch_size = min(max(4, os.cpu_count() or 4), 16)

            semantic_chunker = EmbeddingBasedSemanticChunker(
                embedder=embedder,
                breakpoint_threshold_type=SEMANTIC_CHUNKER_CONFIG.get(
                    "breakpoint_threshold_type", "percentile"
                ),
                breakpoint_threshold_value=float(
                    SEMANTIC_CHUNKER_CONFIG.get("breakpoint_threshold_value", 95.0)
                ),
                sentence_split_regex=SEMANTIC_CHUNKER_CONFIG.get(
                    "sentence_split_regex", r"[.!?]\s+"
                ),
                min_chunk_size=int(SEMANTIC_CHUNKER_CONFIG.get("min_chunk_size", 100)),
                max_chunk_size=int(SEMANTIC_CHUNKER_CONFIG.get("max_chunk_size", 800)),
                similarity_threshold=float(
                    SEMANTIC_CHUNKER_CONFIG.get("similarity_threshold", 0.5)
                ),
                batch_size=batch_size,
            )

            split_docs, vectors = semantic_chunker.split_documents(docs)
            logger.info(
                f"[RAG] [CHUNKING] ì˜ë¯¸ë¡ ì  ë¶„í•  ì™„ë£Œ | ì›ë³¸: {len(docs)} | ì²­í¬: {len(split_docs)}"
            )
        else:
            chunker = RecursiveCharacterTextSplitter(
                chunk_size=TEXT_SPLITTER_CONFIG["chunk_size"],
                chunk_overlap=TEXT_SPLITTER_CONFIG["chunk_overlap"],
            )
            split_docs = chunker.split_documents(docs)
            logger.info(
                f"[RAG] [CHUNKING] ê¸°ë³¸ ë¶„í•  ì™„ë£Œ | ì›ë³¸: {len(docs)} | ì²­í¬: {len(split_docs)}"
            )

            # [ìµœì í™”] ê¸°ë³¸ ë¶„í•  ì‹œì—ë„ ë²¡í„°ë¥¼ ì¦‰ì‹œ ê³„ì‚°í•˜ì—¬ ì¤‘ë³µ ìš”ì²­ ë°©ì§€
            if embedder and split_docs:
                texts = [d.page_content for d in split_docs]
                vectors_list = embedder.embed_documents(texts)
                vectors = [np.array(v) for v in vectors_list]

        # ì²­í¬ ì¸ë±ìŠ¤ ë° ë³¸ë¬¸ ì—¬ë¶€ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        noise_keywords = ["index", "references", "bibliography", "doi:", "isbn"]
        for i, doc in enumerate(split_docs):
            doc.metadata = doc.metadata.copy()
            doc.metadata["chunk_index"] = i

            # [ì¶”ê°€] ë³¸ë¬¸ ì—¬ë¶€ íŒë³„ (ì‚¬ì „ í•„í„°ë§ìš©)
            content_lower = doc.page_content.lower()
            is_noise = any(kw in content_lower[:100] for kw in noise_keywords)
            # DOI ë§í¬ê°€ ë„ˆë¬´ ë§ê±°ë‚˜ ì½¤ë§ˆ/ìˆ«ìê°€ ìƒ‰ì¸ì²˜ëŸ¼ ë§ìœ¼ë©´ ë…¸ì´ì¦ˆë¡œ ê°„ì£¼
            if not is_noise and (
                content_lower.count("doi:") > 2 or content_lower.count(",") > 25
            ):
                is_noise = True

            doc.metadata["is_content"] = not is_noise

        op.tokens = sum(len(doc.page_content.split()) for doc in split_docs)
        return split_docs, vectors


def _serialize_docs(docs: DocumentList) -> DocumentDictList:
    """[ìµœì í™”] Pydanticì˜ ë¬´ê±°ìš´ dict() ëŒ€ì‹  ì§ì ‘ í•„ìš”í•œ í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ"""
    return [
        {"page_content": doc.page_content, "metadata": doc.metadata} for doc in docs
    ]


def _deserialize_docs(docs_as_dicts: DocumentDictList) -> DocumentList:
    from langchain_core.documents import Document

    return [Document(**d) for d in docs_as_dicts]


@functools.lru_cache(maxsize=1)
def _compute_config_hash() -> str:
    """ì„¤ì • ë³€ê²½ ê°ì§€ìš© í•´ì‹œ ìƒì„± (ë³´ì•ˆìš© ì•„ë‹˜)"""
    config_dict = {
        "version": "2.1",  # ğŸš€ ì „ì²˜ë¦¬ ë¡œì§ ë³€ê²½ìœ¼ë¡œ ì¸í•œ ìºì‹œ ë¬´íš¨í™” ê°•ì œ
        "semantic_chunker": SEMANTIC_CHUNKER_CONFIG,
        "text_splitter": TEXT_SPLITTER_CONFIG,
        "retriever": RETRIEVER_CONFIG,
    }
    config_str = json.dumps(config_dict, sort_keys=True, default=str)
    # usedforsecurity=False: ì´ í•´ì‹œëŠ” ë³´ì•ˆ ëª©ì ì´ ì•„ë‹Œ ì„¤ì • ë³€ê²½ ê°ì§€ìš©ì„ì„ ëª…ì‹œ
    return hashlib.sha256(config_str.encode()).hexdigest()[:12]


class VectorStoreCache:
    """ë²¡í„° ì €ì¥ì†Œ ë° ë¦¬íŠ¸ë¦¬ë²„ ìºì‹œ ê´€ë¦¬ì"""

    # [ì¶”ê°€] í”„ë¡œì„¸ìŠ¤ ë‚´ ìŠ¤ë ˆë“œ ê°„ ê²½ìŸ ë°©ì§€ë¥¼ ìœ„í•œ ê³µìœ  ë½
    _global_write_lock = threading.Lock()

    def __init__(
        self, file_path: str, embedding_model_name: str, file_hash: str | None = None
    ):
        self._file_hash = file_hash or _compute_file_hash(file_path)
        (
            self.cache_dir,
            self.doc_splits_path,
            self.faiss_index_path,
            self.bm25_retriever_path,
        ) = self._get_cache_paths(self._file_hash, embedding_model_name)

        # ìºì‹œ ë³´ì•ˆ ê´€ë¦¬ì ì´ˆê¸°í™”
        self.security_manager = CacheSecurityManager(
            security_level=CACHE_SECURITY_LEVEL,
            hmac_secret=CACHE_HMAC_SECRET,
            trusted_paths=CACHE_TRUSTED_PATHS,
            check_permissions=CACHE_CHECK_PERMISSIONS,
        )

    def _get_cache_paths(
        self, file_hash: str, embedding_model_name: str
    ) -> tuple[str, str, str, str]:
        # íŒŒì¼ ê²½ë¡œì— ì•ˆì „í•˜ì§€ ì•Šì€ ë¬¸ì ì œê±°
        model_name_slug = embedding_model_name.replace("/", "_").replace("\\", "_")
        config_hash = _compute_config_hash()

        cache_dir = os.path.join(
            VECTOR_STORE_CACHE_DIR, f"{file_hash}_{model_name_slug}_{config_hash}"
        )

        return (
            cache_dir,
            os.path.join(cache_dir, "doc_splits.json"),  # [.pkl -> .json]
            os.path.join(cache_dir, "faiss_index"),
            os.path.join(cache_dir, "bm25_docs.json"),  # [.pkl -> .json]
        )

    def _purge_cache(self, reason: str):
        """ë³´ì•ˆ ìœ„í˜‘ì´ë‚˜ ì†ìƒì´ ê°ì§€ëœ ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ì™„ì „íˆ ì‚­ì œí•©ë‹ˆë‹¤."""
        if os.path.exists(self.cache_dir):
            try:
                import shutil

                shutil.rmtree(self.cache_dir)
                logger.critical(
                    f"[Security] ìºì‹œ ê°•ì œ ì‚­ì œë¨ ({reason}): {self.cache_dir}"
                )
            except Exception as e:
                logger.error(f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def load(
        self,
        embedder: HuggingFaceEmbeddings,
    ) -> tuple[list[Document] | None, FAISS | None, BM25Retriever | None]:
        """
        ìºì‹œëœ RAG ì»´í¬ë„ŒíŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        ë³´ì•ˆ ì •ì±…:
        - ì‹ ë¢° ê²½ë¡œ ìœ„ë°˜(CacheTrustError) ë˜ëŠ” ë¬´ê²°ì„± ìœ„ë°˜(CacheIntegrityError) ì‹œ
          ë³´ì•ˆ ìˆ˜ì¤€ì— ìƒê´€ì—†ì´ ìºì‹œë¥¼ ì¦‰ì‹œ ì‚­ì œí•˜ê³  ë¡œë“œë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
        """
        if not all(
            os.path.exists(p)
            for p in [
                self.doc_splits_path,
                self.faiss_index_path,
                self.bm25_retriever_path,
            ]
        ):
            return None, None, None

        try:
            from langchain_community.vectorstores import FAISS

            # --- 1. ë³´ì•ˆ ë° ì‹ ë¢° ê²€ì¦ ---
            # ëª¨ë“  ì£¼ìš” ìºì‹œ ì»´í¬ë„ŒíŠ¸ì— ëŒ€í•´ ë£¨í”„ë¥¼ ëŒë©° ê²€ì¦ ìˆ˜í–‰
            paths_to_verify = [
                (self.doc_splits_path, "ë¬¸ì„œ ë°ì´í„°"),
                (self.faiss_index_path, "FAISS ì¸ë±ìŠ¤"),
                (self.bm25_retriever_path, "BM25 ë¦¬íŠ¸ë¦¬ë²„"),
            ]

            for path, desc in paths_to_verify:
                try:
                    # ì‹ ë¢° ê²½ë¡œ ê²€ì¦ (ëª¨ë“  íŒŒì¼/í´ë”)
                    self.security_manager.verify_cache_trust(path)

                    # ë¬´ê²°ì„± ê²€ì¦ (íŒŒì¼ì¸ ê²½ìš°ì—ë§Œ SHA256 ì²´í¬)
                    if os.path.isfile(path):
                        # [ë³´ì•ˆ ê°•í™”] ê³ ë³´ì•ˆ ë ˆë²¨ì—ì„œëŠ” HMAC ê²€ì¦ì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ ë¹„ë°€í‚¤ê°€ ì—†ìœ¼ë©´ ë¡œë“œë¥¼ ê±°ë¶€í•¨
                        if CACHE_SECURITY_LEVEL == "high" and not CACHE_HMAC_SECRET:
                            raise CacheIntegrityError(
                                f"ë³´ì•ˆ ë ˆë²¨ 'high'ì—ì„œëŠ” HMAC ë¹„ë°€í‚¤ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤: {path}"
                            )
                        self.security_manager.verify_cache_integrity(path)
                    elif os.path.isdir(path):
                        # FAISS ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ë‚´ì˜ í•µì‹¬ íŒŒì¼ ê²€ì¦
                        index_file = os.path.join(path, "index.faiss")
                        if os.path.exists(index_file):
                            self.security_manager.verify_cache_integrity(index_file)

                except (CacheTrustError, CacheIntegrityError) as e:
                    self._purge_cache(
                        reason=f"Security Violation in {desc}: {type(e).__name__}"
                    )
                    return None, None, None
                except CachePermissionError as e:
                    if CACHE_SECURITY_LEVEL == "high":
                        self._purge_cache(reason=f"Permission Violation in {desc}")
                        return None, None, None
                    logger.warning(f"ìºì‹œ ê¶Œí•œ ê²½ê³  ({desc}): {e}")

            # --- 2. ë°ì´í„° ë¡œë“œ (ëª¨ë“  ê²€ì¦ í†µê³¼ í›„) ---
            # 1. ë¬¸ì„œ ë¡œë“œ (orjson ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ)
            import orjson

            with open(self.doc_splits_path, "rb") as f:
                doc_dicts = orjson.loads(f.read())
            doc_splits = _deserialize_docs(doc_dicts)

            # 2. FAISS ë¡œë“œ
            # ì´ë¯¸ ìœ„ì—ì„œ ë¬´ê²°ì„±/ì‹ ë¢° ê²€ì¦ì„ ë§ˆì³¤ìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ë¡œë“œ
            vector_store = FAISS.load_local(
                self.faiss_index_path,
                embedder,
                allow_dangerous_deserialization=True,  # [Security] ìœ„ì—ì„œ ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œë¨
            )

            # 3. BM25 ë¡œë“œ (ë³´ì•ˆì„ ìœ„í•´ Pickle ëŒ€ì‹  JSONìœ¼ë¡œë¶€í„° ì¬êµ¬ì¶•)
            # [ìˆ˜ì •] RCE ìœ„í—˜ì´ ìˆëŠ” pickle.load() ì œê±°
            with open(self.bm25_retriever_path, "rb") as f:
                bm25_doc_dicts = orjson.loads(f.read())
            bm25_docs = _deserialize_docs(bm25_doc_dicts)

            from langchain_community.retrievers import BM25Retriever

            # [ìµœì í™”] ëª¨ë“ˆ ë ˆë²¨ì— ì •ì˜ëœ bm25_tokenizer ì‚¬ìš©
            bm25_retriever = BM25Retriever.from_documents(
                bm25_docs, preprocess_func=bm25_tokenizer
            )

            bm25_retriever.k = RETRIEVER_CONFIG["search_kwargs"]["k"]

            logger.info(f"RAG ìºì‹œ ì•ˆì „ ë¡œë“œ ì™„ë£Œ (JSON ê¸°ë°˜): '{self.cache_dir}'")
            return doc_splits, vector_store, bm25_retriever

        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}. ìºì‹œë¥¼ íê¸°í•©ë‹ˆë‹¤.")
            self._purge_cache(reason="Load Error / Corruption")
            return None, None, None

    def save(
        self,
        doc_splits: DocumentList,
        vector_store: FAISS,
        bm25_retriever: BM25Retriever,
    ) -> None:
        """
        RAG ì»´í¬ë„ŒíŠ¸ë¥¼ ìºì‹œì— ì•ˆì „í•˜ê²Œ ì €ì¥í•©ë‹ˆë‹¤. (ì›ìì  ì €ì¥ ë°©ì‹ ì ìš©)
        """
        import shutil
        import uuid

        # 1. ì´ë¯¸ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤/ìŠ¤ë ˆë“œì— ì˜í•´ ì™„ì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if os.path.exists(self.cache_dir):
            logger.info(f"[Cache] ìºì‹œê°€ ì´ë¯¸ ì¡´ì¬í•¨: {self.cache_dir}")
            return

        # 2. ì„ì‹œ ìŠ¤í…Œì´ì§• ë””ë ‰í„°ë¦¬ ìƒì„± (ê³ ìœ  ì´ë¦„ ë¶€ì—¬)
        staging_dir = f"{self.cache_dir}.tmp.{uuid.uuid4().hex[:8]}"

        # ìŠ¤í…Œì´ì§•ìš© ê²½ë¡œë“¤ ì¬êµ¬ì„±
        stg_doc_splits_path = os.path.join(staging_dir, "doc_splits.json")
        stg_faiss_index_path = os.path.join(staging_dir, "faiss_index")
        stg_bm25_retriever_path = os.path.join(staging_dir, "bm25_docs.json")

        try:
            os.makedirs(staging_dir, exist_ok=True)
            import orjson

            # --- A. ìŠ¤í…Œì´ì§• ë””ë ‰í„°ë¦¬ì— ë°ì´í„° ì‘ì„± ---

            # 1. ë¬¸ì„œ ì €ì¥ (orjson ì‚¬ìš©)
            serialized_splits = _serialize_docs(doc_splits)
            with open(stg_doc_splits_path, "wb") as f:
                f.write(orjson.dumps(serialized_splits))

            doc_meta = self.security_manager.create_metadata_for_file(
                stg_doc_splits_path, description="Document splits cache (JSON)"
            )
            self.security_manager.save_cache_metadata(
                stg_doc_splits_path + ".meta", doc_meta
            )

            # 2. FAISS ì €ì¥
            vector_store.save_local(stg_faiss_index_path)
            for filename in ["index.faiss", "index.pkl"]:
                file_p = os.path.join(stg_faiss_index_path, filename)
                if os.path.exists(file_p):
                    meta = self.security_manager.create_metadata_for_file(
                        file_p, description=f"FAISS index part: {filename}"
                    )
                    self.security_manager.save_cache_metadata(file_p + ".meta", meta)

            # 3. BM25 ì €ì¥
            bm25_docs = getattr(bm25_retriever, "docs", doc_splits)
            serialized_bm25 = _serialize_docs(bm25_docs)
            with open(stg_bm25_retriever_path, "wb") as f:
                f.write(orjson.dumps(serialized_bm25))

            # [ë³´ì•ˆ] RCE ìœ„í—˜ì´ ìˆëŠ” Pickle ì €ì¥ ë¡œì§ ì œê±°ë¨

            # ë©”íƒ€ë°ì´í„° ìƒì„± (JSON íŒŒì¼ì— ëŒ€í•´)
            bm25_meta = self.security_manager.create_metadata_for_file(
                stg_bm25_retriever_path, description="BM25 retriever data (JSON)"
            )
            self.security_manager.save_cache_metadata(
                stg_bm25_retriever_path + ".meta", bm25_meta
            )

            # --- B. ì›ìì  êµì²´ (Atomic Rename) ---
            with self._global_write_lock:
                if not os.path.exists(self.cache_dir):
                    try:
                        os.rename(staging_dir, self.cache_dir)
                        logger.info(
                            f"RAG ìºì‹œ ì›ìì  ì €ì¥ ë° ê²€ì¦ ì™„ë£Œ: '{self.cache_dir}'"
                        )
                    except Exception as e:
                        logger.error(f"ìºì‹œ ìµœì¢… êµì²´ ì‹¤íŒ¨: {e}")
                        raise
                else:
                    # ë½ íšë“ ëŒ€ê¸° ì¤‘ì— ë‹¤ë¥¸ ìŠ¤ë ˆë“œê°€ ë¨¼ì € ì €ì¥í•œ ê²½ìš°
                    logger.info(
                        "[Cache] ë‹¤ë¥¸ ì„¸ì…˜ì— ì˜í•´ ìºì‹œê°€ ì´ë¯¸ ìƒì„±ë¨. ìŠ¤í…Œì´ì§• ì‚­ì œ."
                    )
                    shutil.rmtree(staging_dir)

        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            if os.path.exists(staging_dir):
                shutil.rmtree(staging_dir)
            raise


@log_operation("FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±")
def _create_vector_store(
    docs: list[Document],
    embedder: HuggingFaceEmbeddings,
    vectors: Any = None,
) -> FAISS:
    """
    FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìµœì í™”ëœ ë°©ì‹ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    [ìµœì í™”] Index Factoryë¥¼ í†µí•´ SQ8(ì–‘ìí™”) + HNSW êµ¬ì¡°ë¥¼ ì ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆê° ë° ì†ë„ í–¥ìƒ.
    """
    import uuid

    import faiss
    import numpy as np
    from langchain_community.docstore.in_memory import InMemoryDocstore
    from langchain_community.vectorstores import FAISS
    from langchain_community.vectorstores.utils import DistanceStrategy

    # 1. ì„ë² ë”© ë°ì´í„° ì¤€ë¹„ ë° ì •ê·œí™”
    if vectors is None:
        logger.warning(
            "[FAISS] ì „ë‹¬ëœ ë²¡í„°ê°€ ì—†ì–´ ì„ë² ë”©ì„ ë‹¤ì‹œ ìˆ˜í–‰í•©ë‹ˆë‹¤ (ë¹„íš¨ìœ¨ì )."
        )
        texts = [d.page_content for d in docs]
        vectors_list = embedder.embed_documents(texts)
        vectors = np.array(vectors_list).astype("float32")
    else:
        vectors = np.array(vectors).astype("float32")

    faiss.normalize_L2(vectors)
    chunk_count = len(docs)
    d = vectors.shape[1]

    # 2. ìµœì í™”ëœ ì¸ë±ìŠ¤ íŒ©í† ë¦¬ ì„¤ì •
    # - Flat: ì •ë°€ë„ê°€ ì¤‘ìš”í•œ ì†Œê·œëª¨ ë°ì´í„°
    # - HNSW32,SQ8: ë©”ëª¨ë¦¬ íš¨ìœ¨ê³¼ ì†ë„ê°€ ì¤‘ìš”í•œ ëŒ€ê·œëª¨ ë°ì´í„°
    index_type = "Flat" if chunk_count < 1000 else "HNSW32,SQ8"

    # Inner Product(IP)ëŠ” ì •ê·œí™”ëœ ë²¡í„°ì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ì¼í•¨
    index = faiss.index_factory(d, index_type, faiss.METRIC_INNER_PRODUCT)

    # 3. ì¸ë±ìŠ¤ í•™ìŠµ (ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í•™ìŠµ ë‹¨ê³„ í•„ìš”)
    if "SQ" in index_type or "IVF" in index_type:
        logger.info(f"[FAISS] ì¸ë±ìŠ¤ í•™ìŠµ ì‹œì‘ ({index_type})")
        index.train(vectors)

    # ë°ì´í„° ì¶”ê°€
    index.add(vectors)

    # 4. HNSW ì„¸ë¶€ íŠœë‹ (ê²€ìƒ‰ ì‹œ ì •ë°€ë„ í–¥ìƒ)
    if "HNSW" in index_type:
        # efSearch: ê²€ìƒ‰ ì‹œ í›„ë³´ íƒìƒ‰ ë²”ìœ„ (ê°’ì´ í´ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë ¤ì§)
        faiss.downcast_index(index).hnsw.efSearch = 128

    # 5. LangChain FAISS ê°ì²´ë¡œ ë˜í•‘
    doc_ids = [str(uuid.uuid4()) for _ in range(chunk_count)]
    new_docs = {
        doc_id: Document(page_content=doc.page_content, metadata=doc.metadata)
        for doc_id, doc in zip(doc_ids, docs, strict=False)
    }
    docstore = InMemoryDocstore(new_docs)
    index_to_docstore_id = dict(enumerate(doc_ids))

    return FAISS(
        embedding_function=embedder,
        index=index,
        docstore=docstore,
        index_to_docstore_id=index_to_docstore_id,
        distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT,
    )


def _create_bm25_retriever(docs: list[Document]) -> BM25Retriever:
    from langchain_community.retrievers import BM25Retriever

    # [ìµœì í™”] BM25 ìƒì„± ì‹œ í˜•íƒœì†Œ ë¶„ì„ê¸° ëŒ€ì‹  ëª¨ë“ˆ ë ˆë²¨ì— ì •ì˜ëœ bm25_tokenizer ì‚¬ìš©
    retriever = BM25Retriever.from_documents(docs, preprocess_func=bm25_tokenizer)
    retriever.k = RETRIEVER_CONFIG["search_kwargs"]["k"]
    return retriever


def _create_ensemble_retriever(
    vector_store: FAISS,
    bm25_retriever: BM25Retriever,
) -> EnsembleRetriever:
    from langchain_classic.retrievers import EnsembleRetriever

    faiss_retriever = vector_store.as_retriever(
        search_type=RETRIEVER_CONFIG["search_type"],
        search_kwargs=RETRIEVER_CONFIG["search_kwargs"],
    )
    return EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=RETRIEVER_CONFIG["ensemble_weights"],
    )


@log_operation("ê²€ìƒ‰ ì»´í¬ë„ŒíŠ¸ ë¡œë“œ/ìƒì„±")
def _load_and_build_retrieval_components(
    file_path: str,
    file_name: str,
    _embedder: HuggingFaceEmbeddings,
    embedding_model_name: str,
    _on_progress=None,
    _file_hash: str | None = None,
    session_id: str | None = None,
) -> tuple[DocumentList, FAISS, BM25Retriever, bool]:
    # 1. [ìµœì í™”] íŒŒì¼ í†µí•© ë¡œë“œ ë° í•´ì‹œ ê³„ì‚°
    # í•´ì‹œê°€ ì£¼ì–´ì§€ì§€ ì•Šì•˜ë‹¤ë©´ íŒŒì¼ì„ ë¯¸ë¦¬ ì½ì–´ í•´ì‹œ ê³„ì‚°ê³¼ ë¡œë”©ì— ê³µìœ 
    file_bytes = None
    if _file_hash is None:
        try:
            with open(file_path, "rb") as f:
                file_bytes = f.read()
            _file_hash = _compute_file_hash(file_path, data=file_bytes)
        except Exception as e:
            logger.error(f"íŒŒì¼ í†µí•© ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹(íŒŒì¼ ê²½ë¡œ ê¸°ë°˜)ìœ¼ë¡œ í´ë°±í•˜ê¸° ìœ„í•´ None ìœ ì§€

    cache = VectorStoreCache(file_path, embedding_model_name, file_hash=_file_hash)
    doc_splits, vector_store, bm25_retriever = cache.load(_embedder)

    cache_used = all(x is not None for x in [doc_splits, vector_store, bm25_retriever])

    if not cache_used:
        import torch

        # [ìµœì í™”] ì´ë¯¸ ì½ì–´ë‘” ë°”ì´íŠ¸ê°€ ìˆë‹¤ë©´ í™œìš© (ì¤‘ë³µ I/O ì œê±°)
        docs = _load_pdf_docs(
            file_path,
            file_name,
            on_progress=_on_progress,
            file_bytes=file_bytes,
            session_id=session_id,
        )

        # [ì¶”ê°€] ë¬¸ì„œ ì–¸ì–´ ê°ì§€ (ì²« 1000ì ê¸°ì¤€)
        if docs:
            sample_text = docs[0].page_content[:1000]
            # í•œê¸€ í¬í•¨ ì—¬ë¶€ë¡œ ê°„ë‹¨íˆ íŒë³„ (í™•ì¥ ê°€ëŠ¥)
            has_korean = any("\uac00" <= char <= "\ud7a3" for char in sample_text)
            doc_lang = "Korean" if has_korean else "English"
            SessionManager.set("doc_language", doc_lang, session_id=session_id)
            logger.info(f"[RAG] [LANG] ë¬¸ì„œ ì–¸ì–´ ê°ì§€ë¨: {doc_lang}")

        # ë¹ˆ ë¬¸ì„œ ì²˜ë¦¬
        if not docs:
            raise EmptyPDFError(
                filename=file_name,
                details={"reason": "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."},
            )

        if _on_progress:
            _on_progress()

        # [ìµœì í™” 1] ë¬¸ì„œ í¬ê¸°ì— ë”°ë¥¸ ë°”ì´íŒ¨ìŠ¤ ì „ëµ (2000ì ë¯¸ë§Œì€ ê³ ì† ì²˜ë¦¬)
        total_text_len = sum(len(d.page_content) for d in docs)
        is_small_doc = total_text_len < 2000

        # [ìµœì í™” 2] 1ì°¨ ë¶„í•  ë° ì„ë² ë”© ìƒì„± (ë‹¨ì¼ íŒ¨ìŠ¤)
        doc_splits, precomputed_vectors = _split_documents(
            docs, _embedder, session_id=session_id
        )
        if _on_progress:
            _on_progress()

        if not doc_splits:
            raise InsufficientChunksError(chunk_count=0, min_required=1)

        # [ìµœì í™” 3] ë²¡í„° ì¬ì‚¬ìš©ì„ í†µí•œ ì¸ë±ìŠ¤ ìµœì í™”
        optimized_vectors: Any = precomputed_vectors
        q_meta = None
        optimizer = None

        if not is_small_doc:
            try:
                SessionManager.add_status_log("ì¸ë±ìŠ¤ ìµœì í™” ì¤‘", session_id=session_id)
                if _on_progress:
                    _on_progress()

                optimizer = get_index_optimizer()
                doc_splits, optimized_vectors, q_meta, stats = optimizer.optimize_index(
                    doc_splits, optimized_vectors
                )

                # [ìˆ˜ì •] ì–‘ìí™”ëœ ë²¡í„°ë¥¼ ì›ë˜ì˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ ë³´ì¥
                if optimizer and q_meta and q_meta.get("method") != "none":
                    optimized_vectors = optimizer.quantizer.dequantize_vectors(
                        optimized_vectors, q_meta
                    )

                logger.info(f"ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ: ì¤‘ë³µ {stats.pruned_documents}ê°œ ì œê±°")
                SessionManager.replace_last_status_log(
                    f"ì¤‘ë³µ ë‚´ìš© {stats.pruned_documents}ê°œ ì •ë¦¬", session_id=session_id
                )
                if _on_progress:
                    _on_progress()
            except Exception as e:
                logger.warning(f"ì¸ë±ìŠ¤ ìµœì í™” ë‹¨ê³„ ê±´ë„ˆëœ€ (ê²½ë¯¸í•œ ì˜¤ë¥˜): {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ doc_splitsì™€ optimized_vectorsëŠ” ìœ ì§€ë¨

        # [ìµœì í™” 4] ê³„ì‚°ëœ ë²¡í„°ë¥¼ FAISSì— ì§ì ‘ ì£¼ì… (GPU ì¶”ê°€ í˜¸ì¶œ 0íšŒ)
        vector_store = _create_vector_store(
            doc_splits, _embedder, vectors=optimized_vectors
        )
        bm25_retriever = _create_bm25_retriever(doc_splits or [])

        # ìºì‹œ ì €ì¥
        cache.save(doc_splits, vector_store, bm25_retriever)

        # [ìµœì í™” 5] GPU ìì› ì¦‰ì‹œ ë°˜í™˜ (Ollamaì™€ì˜ VRAM ê²½í•© ë°©ì§€)
        if torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
                logger.debug("[System] [Memory] CUDA ìºì‹œ ë¹„ìš°ê¸° ì™„ë£Œ")
            except Exception:
                pass

        SessionManager.add_status_log("ì‹ ê·œ ì¸ë±ì‹± ì™„ë£Œ", session_id=session_id)

    return doc_splits, vector_store, bm25_retriever, cache_used


@log_operation("RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")
def build_rag_pipeline(
    uploaded_file_name: str,
    file_path: str,
    embedder: HuggingFaceEmbeddings,
    on_progress=None,
    session_id: str | None = None,
) -> tuple[str, bool]:
    """
    RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ê³  ì„¸ì…˜ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    # [ìµœì í™”] íŒŒì¼ í•´ì‹œëŠ” ì—¬ê¸°ì„œ í•œ ë²ˆë§Œ ê³„ì‚°í•˜ì—¬ í•˜ìœ„ í•¨ìˆ˜ë¡œ ì „ë‹¬
    file_hash = _compute_file_hash(file_path)

    # [ìµœì í™”] embedder ê°ì²´ëŠ” í•´ì‹±ì—ì„œ ì œì™¸í•˜ê³ , ëª¨ë¸ëª…ê³¼ íŒŒì¼ í•´ì‹œë¥¼ ëª…ì‹œì  í‚¤ë¡œ ì „ë‹¬
    doc_splits, vector_store, bm25_retriever, cache_used = (
        _load_and_build_retrieval_components(
            file_path,
            uploaded_file_name,
            _embedder=embedder,
            embedding_model_name=embedder.model_name,
            _on_progress=on_progress,
            _file_hash=file_hash,
            session_id=session_id,
        )
    )

    if cache_used:
        SessionManager.add_status_log("ìºì‹œ ë°ì´í„° ë¡œë“œ", session_id=session_id)
        if on_progress:
            on_progress()

    # [ìµœì í™”] ë³‘ë ¬ ê²€ìƒ‰ì„ ìœ„í•´ ê°œë³„ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ë° ì„¸ì…˜ ì €ì¥
    faiss_retriever = vector_store.as_retriever(
        search_type=RETRIEVER_CONFIG["search_type"],
        search_kwargs=RETRIEVER_CONFIG["search_kwargs"],
    )

    SessionManager.set("faiss_retriever", faiss_retriever, session_id=session_id)
    SessionManager.set("bm25_retriever", bm25_retriever, session_id=session_id)

    # ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€ (EnsembleRetrieverë„ ìƒì„±)
    from langchain_classic.retrievers import EnsembleRetriever

    final_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=RETRIEVER_CONFIG["ensemble_weights"],
    )
    rag_engine = build_graph(retriever=final_retriever)

    SessionManager.set("vector_store", vector_store, session_id=session_id)
    SessionManager.set("rag_engine", rag_engine, session_id=session_id)
    SessionManager.set("pdf_processed", True, session_id=session_id)
    SessionManager.add_status_log("ì§ˆë¬¸ ê°€ëŠ¥", session_id=session_id)

    if on_progress:
        on_progress()

    logger.info(
        f"RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ: '{uploaded_file_name}' (ìºì‹œ ì‚¬ìš©: {cache_used}, Session: {session_id})"
    )

    if cache_used:
        return f"'{uploaded_file_name}' ë¬¸ì„œ ìºì‹œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ", True
    return (f"'{uploaded_file_name}' ì‹ ê·œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ"), False


@log_operation("íŒŒì´í”„ë¼ì¸ LLM ì—…ë°ì´íŠ¸")
def update_llm_in_pipeline(llm: T | None) -> None:
    """
    ì„¸ì…˜ì˜ LLMì„ êµì²´í•©ë‹ˆë‹¤.

    ê·¸ë˜í”„ê°€ ì„¸ì…˜ì—ì„œ LLMì„ ê°€ì ¸ì˜¤ë¯€ë¡œ ì¬ë¹Œë“œí•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.

    Args:
        llm: ì—…ë°ì´íŠ¸í•  ìƒˆë¡œìš´ LLM ëª¨ë¸.

    Raises:
        ValueError: RAG íŒŒì´í”„ë¼ì¸ì´ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ì„ ë•Œ.
    """
    if not SessionManager.get("pdf_processed"):
        raise ValueError("RAG íŒŒì´í”„ë¼ì¸ì´ êµ¬ì¶•ë˜ì§€ ì•Šì•„ LLMì„ ì—…ë°ì´íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    SessionManager.set("llm", llm)
    logger.info(f"ì„¸ì…˜ LLM ì—…ë°ì´íŠ¸ ì™„ë£Œ: '{getattr(llm, 'model', 'unknown')}'")
