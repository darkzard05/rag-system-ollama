"""
RAG íŒŒì´í”„ë¼ì¸ì˜ í•µì‹¬ ë¡œì§(ë°ì´í„° ì²˜ë¦¬, ì„ë² ë”©, ê²€ìƒ‰, ìƒì„±)ì„ ë‹´ë‹¹í•˜ëŠ” íŒŒì¼.
"""

from __future__ import annotations

import functools
import hashlib
import json
import logging
import os
import pickle
import threading
from collections.abc import Callable
from typing import TYPE_CHECKING, Any

import numpy as np

from common.typing_utils import (
    DocumentDictList,
    DocumentList,
    T,
)

if TYPE_CHECKING:
    from langchain.retrievers import EnsembleRetriever
    from langchain_community.retrievers import BM25Retriever
    from langchain_community.vectorstores import FAISS
    from langchain_core.documents import Document
    from langchain_huggingface import HuggingFaceEmbeddings

from common.config import (
    CACHE_CHECK_PERMISSIONS,
    CACHE_HMAC_SECRET,
    CACHE_SECURITY_LEVEL,
    CACHE_TRUSTED_PATHS,
    RETRIEVER_CONFIG,
    SEMANTIC_CHUNKER_CONFIG,
    TEXT_SPLITTER_CONFIG,
    VECTOR_STORE_CACHE_DIR,
)
from common.exceptions import (
    EmptyPDFError,
    InsufficientChunksError,
    PDFProcessingError,
    VectorStoreError,
)
from common.utils import log_operation, preprocess_text
from core.graph_builder import build_graph
from core.semantic_chunker import EmbeddingBasedSemanticChunker
from core.session import SessionManager
from security.cache_security import (
    CacheIntegrityError,
    CachePermissionError,
    CacheSecurityManager,
    CacheTrustError,
)
from services.monitoring.performance_monitor import (
    OperationType,
    get_performance_monitor,
)
from services.optimization.index_optimizer import get_index_optimizer

logger = logging.getLogger(__name__)
monitor = get_performance_monitor()


def bm25_tokenizer(text: str) -> list[str]:
    """BM25ìš© ê²½ëŸ‰ í† í¬ë‚˜ì´ì € (Pickle ì§€ì›ì„ ìœ„í•´ ëª¨ë“ˆ ë ˆë²¨ì— ì •ì˜)"""
    return text.lower().split()


class RAGSystem:
    """
    RAG ì‹œìŠ¤í…œì˜ í†µí•© ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ í´ë˜ìŠ¤.
    ì„¸ì…˜ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬ì™€ LangGraph ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì„ ì—°ê²°í•©ë‹ˆë‹¤.
    """

    def __init__(self, session_id: str = "default"):
        self.session_id = session_id
        SessionManager.init_session(session_id=session_id)

    def _ensure_session_context(self) -> None:
        """í˜„ì¬ ìŠ¤ë ˆë“œì˜ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤."""
        SessionManager.set_session_id(self.session_id)

    async def load_document(
        self, file_path: str, file_name: str, embedder: HuggingFaceEmbeddings
    ) -> tuple[str, bool]:
        """
        ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            file_path: PDF íŒŒì¼ì˜ ë¡œì»¬ ê²½ë¡œ
            file_name: ì‚¬ìš©ìì—ê²Œ í‘œì‹œë  íŒŒì¼ ì´ë¦„
            embedder: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

        Returns:
            Tuple[ì„±ê³µ ë©”ì‹œì§€, ìºì‹œ ì‚¬ìš© ì—¬ë¶€]
        """
        self._ensure_session_context()
        return build_rag_pipeline(
            uploaded_file_name=file_name, file_path=file_path, embedder=embedder
        )

    async def aquery(self, query: str, llm: T | None = None) -> dict[str, Any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            llm: ì‚¬ìš©í•  LLM ì¸ìŠ¤í„´ìŠ¤ (ìƒëµ ì‹œ ì„¸ì…˜ì— ì €ì¥ëœ ëª¨ë¸ ì‚¬ìš©)

        Returns:
            GraphOutput êµ¬ì¡°ì˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self._ensure_session_context()

        if llm:
            SessionManager.set("llm", llm)

        rag_engine = SessionManager.get("rag_engine")
        if not rag_engine:
            raise VectorStoreError(
                details={
                    "reason": "RAG ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”."
                }
            )

        current_llm = SessionManager.get("llm")
        config = {"configurable": {"llm": current_llm}}

        # LangGraph í˜¸ì¶œ
        return await rag_engine.ainvoke({"input": query}, config=config)

    def get_status(self) -> list[str]:
        """í˜„ì¬ ì„¸ì…˜ì˜ ì‘ì—… ë¡œê·¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        self._ensure_session_context()
        return SessionManager.get("status_logs", [])

    def clear_session(self) -> None:
        """ì„¸ì…˜ ë°ì´í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self._ensure_session_context()
        SessionManager.reset_all_state()

    def process_documents(self, documents: list[str]) -> None:
        """ì…ë ¥ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤. (í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„± ìœ ì§€)"""
        if not documents:
            raise EmptyPDFError(
                filename="(in-memory)",
                details={"reason": "documents list is empty"},
            )


def _compute_file_hash(file_path: str, data: bytes | None = None) -> str:
    """
    íŒŒì¼ ë˜ëŠ” ë°ì´í„°ì˜ SHA256 í•´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    [ìµœì í™”] ë°ì´í„°ê°€ ì´ë¯¸ ë©”ëª¨ë¦¬ì— ìˆë‹¤ë©´ íŒŒì¼ì„ ë‹¤ì‹œ ì½ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    sha256_hash = hashlib.sha256()
    try:
        if data is not None:
            sha256_hash.update(data)
        else:
            with open(file_path, "rb") as f:
                for byte_block in iter(lambda: f.read(8192), b""):
                    sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except Exception as e:
        logger.error(f"í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return ""


import concurrent.futures  # noqa: E402


def _extract_page_worker(
    file_path: str, page_num: int, total_pages: int, file_name: str
) -> Document | None:
    """ê°œë³„ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ (ìŠ¤ë ˆë“œ ì„¸ì´í”„)"""
    try:
        import fitz  # PyMuPDF
        from langchain_core.documents import Document

        # ê° ìŠ¤ë ˆë“œì—ì„œ íŒŒì¼ì„ ìƒˆë¡œ ì—´ì–´ ë…ë¦½ì ì¸ ë¬¸ì„œ ê°ì²´ ì‚¬ìš©
        with fitz.open(file_path) as doc:
            page = doc[page_num]
            text = page.get_text()
            if text:
                clean_text = preprocess_text(text)
                if clean_text and len(clean_text) > 10:
                    metadata = {
                        "source": file_name,
                        "page": int(page_num + 1),
                        "total_pages": int(total_pages),
                    }
                    return Document(page_content=clean_text, metadata=metadata)
    except Exception as e:
        logger.warning(f"í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return None


def _extract_pages_batch_worker(
    file_bytes: bytes, page_range: list[int], total_pages: int, file_name: str
) -> list[tuple[int, Document]]:
    """í˜ì´ì§€ ë²”ìœ„ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ (memoryview ì‚¬ìš©ìœ¼ë¡œ ë³µì‚¬ ë°©ì§€)"""
    results = []
    try:
        import fitz  # PyMuPDF
        from langchain_core.documents import Document

        # [ìµœì í™”] memoryviewë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ë°”ì´íŠ¸ì— ëŒ€í•œ zero-copy ì ‘ê·¼ ë³´ì¥
        mv = memoryview(file_bytes)
        with fitz.open(stream=mv, filetype="pdf") as doc:
            for page_num in page_range:
                try:
                    page = doc[page_num]
                    text = page.get_text()
                    if text:
                        clean_text = preprocess_text(text)
                        if clean_text and len(clean_text) > 10:
                            metadata = {
                                "source": file_name,
                                "page": int(page_num + 1),
                                "total_pages": int(total_pages),
                            }
                            results.append(
                                (
                                    page_num,
                                    Document(
                                        page_content=clean_text, metadata=metadata
                                    ),
                                )
                            )
                except Exception as e:
                    logger.warning(f"í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ë¬¸ì„œ ì˜¤í”ˆ ì‹¤íŒ¨: {e}")
    return results


def _load_pdf_docs(
    file_path: str, file_name: str, on_progress: Callable[[], None] | None = None
) -> list[Document]:
    """
    PDF íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ë²„í¼ë§í•œ í›„ ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•´ ìµœê³  ì†ë„ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    with monitor.track_operation(OperationType.PDF_LOADING, {"file": file_name}) as op:
        try:
            import fitz  # PyMuPDF

            SessionManager.add_status_log("ë¬¸ì„œ ë¶„ì„ ì¤€ë¹„ ì¤‘")
            if on_progress:
                on_progress()

            # 1. íŒŒì¼ ì „ì²´ë¥¼ ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ ë¡œë“œ (ê°€ì¥ ë¹ ë¥¸ ë°©ì‹)
            try:
                with open(file_path, "rb") as f:
                    file_bytes = f.read()

                # ë©”íƒ€ë°ì´í„° í™•ì¸ì„ ìœ„í•´ ì„ì‹œ ì˜¤í”ˆ
                with fitz.open(stream=file_bytes, filetype="pdf") as doc_file:
                    total_pages = len(doc_file)
            except Exception as e:
                raise PDFProcessingError(
                    filename=file_name,
                    details={"reason": f"íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŒ: {str(e)}"},
                ) from e

            if total_pages == 0:
                raise EmptyPDFError(
                    filename=file_name, details={"reason": "PDF í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."}
                )

            # [ìµœì í™”] í˜ì´ì§€ ìˆ˜ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë³‘ë ¬í™” ê²°ì •
            if total_pages <= 3:
                docs = []
                with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                    for i in range(total_pages):
                        page = doc[i]
                        text = page.get_text()
                        if text:
                            clean_text = preprocess_text(text)
                            if clean_text and len(clean_text) > 10:
                                docs.append(
                                    Document(
                                        page_content=clean_text,
                                        metadata={
                                            "source": file_name,
                                            "page": i + 1,
                                            "total_pages": total_pages,
                                        },
                                    )
                                )
            else:
                # [ìµœì í™”] ì›Œì»¤ ìˆ˜ë¥¼ ë…¼ë¦¬ ì½”ì–´ ìˆ˜ì— ë§ì¶”ê³  ë°°ì¹˜ë¥¼ í¬ê²Œ ì„¤ì •í•˜ì—¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
                cpu_count = os.cpu_count() or 4
                max_workers = min(cpu_count, 16)

                # ë„ˆë¬´ ì¦ì€ ì“°ë ˆë“œ ìƒì„± ë°©ì§€ë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ìƒí–¥
                batch_size = max(4, total_pages // (max_workers * 2))
                batches = [
                    list(range(i, min(i + batch_size, total_pages)))
                    for i in range(0, total_pages, batch_size)
                ]

                all_results = [None] * total_pages
                completed_pages = 0

                with concurrent.futures.ThreadPoolExecutor(
                    max_workers=max_workers
                ) as executor:
                    futures = [
                        executor.submit(
                            _extract_pages_batch_worker,
                            file_bytes,
                            batch,
                            total_pages,
                            file_name,
                        )
                        for batch in batches
                    ]

                    for future in concurrent.futures.as_completed(futures):
                        batch_results = future.result()
                        for page_idx, doc in batch_results:
                            all_results[page_idx] = doc

                        completed_pages += len(batch_results)
                        SessionManager.replace_last_status_log(
                            f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ({min(completed_pages, total_pages)}/{total_pages}p)"
                        )
                        if on_progress:
                            import contextlib
                            with contextlib.suppress(Exception):
                                on_progress()

                docs = [r for r in all_results if r is not None]

            if not docs:
                raise EmptyPDFError(
                    filename=file_name,
                    details={"reason": "í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                )

            SessionManager.replace_last_status_log(f"ì¶”ì¶œ ì™„ë£Œ ({len(docs)}p)")
            op.tokens = sum(len(doc.page_content.split()) for doc in docs)
            logger.info(f"[RAG] [LOAD] PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ | íŒŒì¼: {file_name} | í˜ì´ì§€: {len(docs)}")
            return docs
        except (PDFProcessingError, EmptyPDFError):
            raise
        except Exception as e:
            logger.error(f"[RAG] [LOAD] PDF ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ | {e}")
            raise PDFProcessingError(
                filename=file_name, details={"reason": str(e)}
            ) from e


def _split_documents(
    docs: list[Document],
    embedder: HuggingFaceEmbeddings | None = None,
) -> tuple[list[Document], list[np.ndarray] | None]:
    """
    ì„¤ì •ì— ë”°ë¼ ì˜ë¯¸ë¡ ì  ë¶„í• ê¸° ë˜ëŠ” RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
    """
    SessionManager.add_status_log("ë¬¸ì¥ ë¶„í•  ì¤‘...")
    with monitor.track_operation(
        OperationType.SEMANTIC_CHUNKING, {"doc_count": len(docs)}
    ) as op:
        from langchain.text_splitter import RecursiveCharacterTextSplitter

        use_semantic = SEMANTIC_CHUNKER_CONFIG.get("enabled", False)
        split_docs = []
        vectors = None

        if use_semantic and embedder:
            # [ìµœì í™”] ë””ë°”ì´ìŠ¤ ë° VRAM ìƒí™©ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸° ë™ì  í• ë‹¹
            import torch
            if getattr(embedder, "model_kwargs", {}).get("device") == "cuda":
                # GPU í™˜ê²½: ê°€ìš© ë©”ëª¨ë¦¬ì— ë”°ë¼ 32~128 ì‚¬ì´ì—ì„œ ê²°ì •
                try:
                    total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3) # GB
                    batch_size = 128 if total_mem > 10 else (64 if total_mem > 4 else 32)
                except Exception:
                    batch_size = 32
            else:
                # CPU í™˜ê²½: ì½”ì–´ ìˆ˜ì— ë§ì¶° 4~16 ì‚¬ì´ì—ì„œ ê²°ì •
                batch_size = min(max(4, os.cpu_count() or 4), 16)

            semantic_chunker = EmbeddingBasedSemanticChunker(
                embedder=embedder,
                breakpoint_threshold_type=SEMANTIC_CHUNKER_CONFIG.get(
                    "breakpoint_threshold_type", "percentile"
                ),
                breakpoint_threshold_value=float(
                    SEMANTIC_CHUNKER_CONFIG.get("breakpoint_threshold_value", 95.0)
                ),
                sentence_split_regex=SEMANTIC_CHUNKER_CONFIG.get(
                    "sentence_split_regex", r"[.!?]\s+"
                ),
                min_chunk_size=int(SEMANTIC_CHUNKER_CONFIG.get("min_chunk_size", 100)),
                max_chunk_size=int(SEMANTIC_CHUNKER_CONFIG.get("max_chunk_size", 800)),
                similarity_threshold=float(
                    SEMANTIC_CHUNKER_CONFIG.get("similarity_threshold", 0.5)
                ),
                batch_size=batch_size,
            )

            split_docs, vectors = semantic_chunker.split_documents(docs)
            logger.info(
                f"[RAG] [CHUNKING] ì˜ë¯¸ë¡ ì  ë¶„í•  ì™„ë£Œ | ì›ë³¸: {len(docs)} | ì²­í¬: {len(split_docs)}"
            )
        else:
            chunker = RecursiveCharacterTextSplitter(
                chunk_size=TEXT_SPLITTER_CONFIG["chunk_size"],
                chunk_overlap=TEXT_SPLITTER_CONFIG["chunk_overlap"],
            )
            split_docs = chunker.split_documents(docs)
            logger.info(f"[RAG] [CHUNKING] ê¸°ë³¸ ë¶„í•  ì™„ë£Œ | ì›ë³¸: {len(docs)} | ì²­í¬: {len(split_docs)}")

        # ì²­í¬ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for i, doc in enumerate(split_docs):
            doc.metadata = doc.metadata.copy()
            doc.metadata["chunk_index"] = i

        op.tokens = sum(len(doc.page_content.split()) for doc in split_docs)
        return split_docs, vectors


def _serialize_docs(docs: DocumentList) -> DocumentDictList:
    return [
        doc.model_dump() if hasattr(doc, "model_dump") else doc.dict() for doc in docs
    ]


def _deserialize_docs(docs_as_dicts: DocumentDictList) -> DocumentList:
    from langchain_core.documents import Document

    return [Document(**d) for d in docs_as_dicts]


@functools.lru_cache(maxsize=1)
def _compute_config_hash() -> str:
    """ì„¤ì • ë³€ê²½ ê°ì§€ìš© í•´ì‹œ ìƒì„± (ë³´ì•ˆìš© ì•„ë‹˜)"""
    config_dict = {
        "version": "2.1",  # ğŸš€ ì „ì²˜ë¦¬ ë¡œì§ ë³€ê²½ìœ¼ë¡œ ì¸í•œ ìºì‹œ ë¬´íš¨í™” ê°•ì œ
        "semantic_chunker": SEMANTIC_CHUNKER_CONFIG,
        "text_splitter": TEXT_SPLITTER_CONFIG,
        "retriever": RETRIEVER_CONFIG,
    }
    config_str = json.dumps(config_dict, sort_keys=True, default=str)
    # usedforsecurity=False: ì´ í•´ì‹œëŠ” ë³´ì•ˆ ëª©ì ì´ ì•„ë‹Œ ì„¤ì • ë³€ê²½ ê°ì§€ìš©ì„ì„ ëª…ì‹œ
    return hashlib.sha256(config_str.encode()).hexdigest()[:12]


class VectorStoreCache:
    """ë²¡í„° ì €ì¥ì†Œ ë° ë¦¬íŠ¸ë¦¬ë²„ ìºì‹œ ê´€ë¦¬ì"""

    # [ì¶”ê°€] í”„ë¡œì„¸ìŠ¤ ë‚´ ìŠ¤ë ˆë“œ ê°„ ê²½ìŸ ë°©ì§€ë¥¼ ìœ„í•œ ê³µìœ  ë½
    _global_write_lock = threading.Lock()

    def __init__(self, file_path: str, embedding_model_name: str, file_hash: str | None = None):
        self._file_hash = file_hash or _compute_file_hash(file_path)
        (
            self.cache_dir,
            self.doc_splits_path,
            self.faiss_index_path,
            self.bm25_retriever_path,
        ) = self._get_cache_paths(self._file_hash, embedding_model_name)

        # ìºì‹œ ë³´ì•ˆ ê´€ë¦¬ì ì´ˆê¸°í™”
        self.security_manager = CacheSecurityManager(
            security_level=CACHE_SECURITY_LEVEL,
            hmac_secret=CACHE_HMAC_SECRET,
            trusted_paths=CACHE_TRUSTED_PATHS,
            check_permissions=CACHE_CHECK_PERMISSIONS,
        )

    def _get_cache_paths(
        self, file_hash: str, embedding_model_name: str
    ) -> tuple[str, str, str, str]:
        # íŒŒì¼ ê²½ë¡œì— ì•ˆì „í•˜ì§€ ì•Šì€ ë¬¸ì ì œê±°
        model_name_slug = embedding_model_name.replace("/", "_").replace("\\", "_")
        config_hash = _compute_config_hash()

        cache_dir = os.path.join(
            VECTOR_STORE_CACHE_DIR, f"{file_hash}_{model_name_slug}_{config_hash}"
        )

        return (
            cache_dir,
            os.path.join(cache_dir, "doc_splits.json"),  # [.pkl -> .json]
            os.path.join(cache_dir, "faiss_index"),
            os.path.join(cache_dir, "bm25_docs.json"),   # [.pkl -> .json]
        )

    def _purge_cache(self, reason: str):
        """ë³´ì•ˆ ìœ„í˜‘ì´ë‚˜ ì†ìƒì´ ê°ì§€ëœ ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ì™„ì „íˆ ì‚­ì œí•©ë‹ˆë‹¤."""
        if os.path.exists(self.cache_dir):
            try:
                import shutil

                shutil.rmtree(self.cache_dir)
                logger.critical(
                    f"[Security] ìºì‹œ ê°•ì œ ì‚­ì œë¨ ({reason}): {self.cache_dir}"
                )
            except Exception as e:
                logger.error(f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def load(
        self,
        embedder: HuggingFaceEmbeddings,
    ) -> tuple[list[Document] | None, FAISS | None, BM25Retriever | None]:
        """
        ìºì‹œëœ RAG ì»´í¬ë„ŒíŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        ë³´ì•ˆ ì •ì±…:
        - ì‹ ë¢° ê²½ë¡œ ìœ„ë°˜(CacheTrustError) ë˜ëŠ” ë¬´ê²°ì„± ìœ„ë°˜(CacheIntegrityError) ì‹œ
          ë³´ì•ˆ ìˆ˜ì¤€ì— ìƒê´€ì—†ì´ ìºì‹œë¥¼ ì¦‰ì‹œ ì‚­ì œí•˜ê³  ë¡œë“œë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
        """
        if not all(
            os.path.exists(p)
            for p in [
                self.doc_splits_path,
                self.faiss_index_path,
                self.bm25_retriever_path,
            ]
        ):
            return None, None, None

        try:
            from langchain_community.vectorstores import FAISS

            # --- 1. ë³´ì•ˆ ë° ì‹ ë¢° ê²€ì¦ ---
            # ëª¨ë“  ì£¼ìš” ìºì‹œ ì»´í¬ë„ŒíŠ¸ì— ëŒ€í•´ ë£¨í”„ë¥¼ ëŒë©° ê²€ì¦ ìˆ˜í–‰
            paths_to_verify = [
                (self.doc_splits_path, "ë¬¸ì„œ ë°ì´í„°"),
                (self.faiss_index_path, "FAISS ì¸ë±ìŠ¤"),
                (self.bm25_retriever_path, "BM25 ë¦¬íŠ¸ë¦¬ë²„"),
            ]

            for path, desc in paths_to_verify:
                try:
                    # ì‹ ë¢° ê²½ë¡œ ê²€ì¦ (ëª¨ë“  íŒŒì¼/í´ë”)
                    self.security_manager.verify_cache_trust(path)

                    # ë¬´ê²°ì„± ê²€ì¦ (íŒŒì¼ì¸ ê²½ìš°ì—ë§Œ SHA256 ì²´í¬)
                    if os.path.isfile(path):
                        # [ë³´ì•ˆ ê°•í™”] ê³ ë³´ì•ˆ ë ˆë²¨ì—ì„œëŠ” HMAC ê²€ì¦ì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ ë¹„ë°€í‚¤ê°€ ì—†ìœ¼ë©´ ë¡œë“œë¥¼ ê±°ë¶€í•¨
                        if CACHE_SECURITY_LEVEL == "high" and not CACHE_HMAC_SECRET:
                            raise CacheIntegrityError(
                                f"ë³´ì•ˆ ë ˆë²¨ 'high'ì—ì„œëŠ” HMAC ë¹„ë°€í‚¤ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤: {path}"
                            )
                        self.security_manager.verify_cache_integrity(path)
                    elif os.path.isdir(path):
                        # FAISS ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ë‚´ì˜ í•µì‹¬ íŒŒì¼ ê²€ì¦
                        index_file = os.path.join(path, "index.faiss")
                        if os.path.exists(index_file):
                            self.security_manager.verify_cache_integrity(index_file)

                except (CacheTrustError, CacheIntegrityError) as e:
                    self._purge_cache(
                        reason=f"Security Violation in {desc}: {type(e).__name__}"
                    )
                    return None, None, None
                except CachePermissionError as e:
                    if CACHE_SECURITY_LEVEL == "high":
                        self._purge_cache(reason=f"Permission Violation in {desc}")
                        return None, None, None
                    logger.warning(f"ìºì‹œ ê¶Œí•œ ê²½ê³  ({desc}): {e}")

            # --- 2. ë°ì´í„° ë¡œë“œ (ëª¨ë“  ê²€ì¦ í†µê³¼ í›„) ---
            # 1. ë¬¸ì„œ ë¡œë“œ (Pickle ëŒ€ì‹  JSON ì‚¬ìš©)
            with open(self.doc_splits_path, encoding="utf-8") as f:
                doc_dicts = json.load(f)
            doc_splits = _deserialize_docs(doc_dicts)

            # 2. FAISS ë¡œë“œ
            # ì´ë¯¸ ìœ„ì—ì„œ ë¬´ê²°ì„±/ì‹ ë¢° ê²€ì¦ì„ ë§ˆì³¤ìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ë¡œë“œ
            vector_store = FAISS.load_local(
                self.faiss_index_path,
                embedder,
                allow_dangerous_deserialization=True,
            )

            # 3. BM25 ë¡œë“œ (Pickleì„ í†µí•œ ìƒíƒœ ë³µì›)
            # JSON ëŒ€ì‹  .pkl íŒŒì¼ì´ ìˆìœ¼ë©´ ì´ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ì—¬ ì¸ë±ì‹± ì‹œê°„ ë‹¨ì¶•
            bm25_retriever_path_pkl = self.bm25_retriever_path.replace(".json", ".pkl")
            if os.path.exists(bm25_retriever_path_pkl):
                with open(bm25_retriever_path_pkl, "rb") as f:
                    bm25_retriever = pickle.load(f)  # nosec
            else:
                # í´ë°±: JSONìœ¼ë¡œë¶€í„° ì¬êµ¬ì¶•
                with open(self.bm25_retriever_path, encoding="utf-8") as f:
                    bm25_doc_dicts = json.load(f)
                bm25_docs = _deserialize_docs(bm25_doc_dicts)

                from langchain_community.retrievers import BM25Retriever
                # [ìµœì í™”] ëª¨ë“ˆ ë ˆë²¨ì— ì •ì˜ëœ bm25_tokenizer ì‚¬ìš©
                bm25_retriever = BM25Retriever.from_documents(bm25_docs, preprocess_func=bm25_tokenizer)

            bm25_retriever.k = RETRIEVER_CONFIG["search_kwargs"]["k"]

            logger.info(f"RAG ìºì‹œ ì•ˆì „ ë¡œë“œ ì™„ë£Œ: '{self.cache_dir}'")
            return doc_splits, vector_store, bm25_retriever

        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}. ìºì‹œë¥¼ íê¸°í•©ë‹ˆë‹¤.")
            self._purge_cache(reason="Load Error / Corruption")
            return None, None, None

    def save(
        self,
        doc_splits: DocumentList,
        vector_store: FAISS,
        bm25_retriever: BM25Retriever,
    ) -> None:
        """
        RAG ì»´í¬ë„ŒíŠ¸ë¥¼ ìºì‹œì— ì•ˆì „í•˜ê²Œ ì €ì¥í•©ë‹ˆë‹¤. (ì›ìì  ì €ì¥ ë°©ì‹ ì ìš©)
        """
        import shutil
        import uuid

        # 1. ì´ë¯¸ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤/ìŠ¤ë ˆë“œì— ì˜í•´ ì™„ì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if os.path.exists(self.cache_dir):
            logger.info(f"[Cache] ìºì‹œê°€ ì´ë¯¸ ì¡´ì¬í•¨: {self.cache_dir}")
            return

        # 2. ì„ì‹œ ìŠ¤í…Œì´ì§• ë””ë ‰í„°ë¦¬ ìƒì„± (ê³ ìœ  ì´ë¦„ ë¶€ì—¬)
        staging_dir = f"{self.cache_dir}.tmp.{uuid.uuid4().hex[:8]}"

        # ìŠ¤í…Œì´ì§•ìš© ê²½ë¡œë“¤ ì¬êµ¬ì„±
        stg_doc_splits_path = os.path.join(staging_dir, "doc_splits.json")
        stg_faiss_index_path = os.path.join(staging_dir, "faiss_index")
        stg_bm25_retriever_path = os.path.join(staging_dir, "bm25_docs.json")

        try:
            os.makedirs(staging_dir, exist_ok=True)

            # --- A. ìŠ¤í…Œì´ì§• ë””ë ‰í„°ë¦¬ì— ë°ì´í„° ì‘ì„± ---

            # 1. ë¬¸ì„œ ì €ì¥
            serialized_splits = _serialize_docs(doc_splits)
            with open(stg_doc_splits_path, "w", encoding="utf-8") as f:
                json.dump(serialized_splits, f, ensure_ascii=False)

            doc_meta = self.security_manager.create_metadata_for_file(
                stg_doc_splits_path, description="Document splits cache (JSON)"
            )
            self.security_manager.save_cache_metadata(stg_doc_splits_path + ".meta", doc_meta)

            # 2. FAISS ì €ì¥
            vector_store.save_local(stg_faiss_index_path)
            for filename in ["index.faiss", "index.pkl"]:
                file_p = os.path.join(stg_faiss_index_path, filename)
                if os.path.exists(file_p):
                    meta = self.security_manager.create_metadata_for_file(
                        file_p, description=f"FAISS index part: {filename}"
                    )
                    self.security_manager.save_cache_metadata(file_p + ".meta", meta)

            # 3. BM25 ì €ì¥
            # JSONì€ í˜¸í™˜ì„± ë° ë¬´ê²°ì„± ì²´í¬ìš©ìœ¼ë¡œ ìœ ì§€
            bm25_docs = getattr(bm25_retriever, "docs", doc_splits)
            serialized_bm25 = _serialize_docs(bm25_docs)
            with open(stg_bm25_retriever_path, "w", encoding="utf-8") as f:
                json.dump(serialized_bm25, f, ensure_ascii=False)

            # [ì¶”ê°€] ì¸ë±ìŠ¤ ìƒíƒœê°€ í¬í•¨ëœ Pickle ì €ì¥ (ì´ˆê³ ì† ë¡œë”©ìš©)
            stg_bm25_retriever_path_pkl = stg_bm25_retriever_path.replace(".json", ".pkl")
            with open(stg_bm25_retriever_path_pkl, "wb") as f:
                pickle.dump(bm25_retriever, f)

            # ë©”íƒ€ë°ì´í„° ìƒì„± (Pickle íŒŒì¼ í¬í•¨)
            for p in [stg_bm25_retriever_path, stg_bm25_retriever_path_pkl]:
                bm25_meta = self.security_manager.create_metadata_for_file(
                    p, description=f"BM25 retriever data ({os.path.basename(p)})"
                )
                self.security_manager.save_cache_metadata(p + ".meta", bm25_meta)

            # --- B. ì›ìì  êµì²´ (Atomic Rename) ---
            with self._global_write_lock:
                if not os.path.exists(self.cache_dir):
                    try:
                        os.rename(staging_dir, self.cache_dir)
                        logger.info(f"RAG ìºì‹œ ì›ìì  ì €ì¥ ë° ê²€ì¦ ì™„ë£Œ: '{self.cache_dir}'")
                    except Exception as e:
                        logger.error(f"ìºì‹œ ìµœì¢… êµì²´ ì‹¤íŒ¨: {e}")
                        raise
                else:
                    # ë½ íšë“ ëŒ€ê¸° ì¤‘ì— ë‹¤ë¥¸ ìŠ¤ë ˆë“œê°€ ë¨¼ì € ì €ì¥í•œ ê²½ìš°
                    logger.info("[Cache] ë‹¤ë¥¸ ì„¸ì…˜ì— ì˜í•´ ìºì‹œê°€ ì´ë¯¸ ìƒì„±ë¨. ìŠ¤í…Œì´ì§• ì‚­ì œ.")
                    shutil.rmtree(staging_dir)

        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            if os.path.exists(staging_dir):
                shutil.rmtree(staging_dir)
            raise


@log_operation("FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±")
def _create_vector_store(
    docs: list[Document],
    embedder: HuggingFaceEmbeddings,
    vectors: Any = None,
) -> FAISS:
    """
    FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    [ìµœì í™”] ë°ì´í„° ê·œëª¨ì— ë”°ë¼ Flat ë˜ëŠ” HNSW ì¸ë±ìŠ¤ë¥¼ ìë™ ì„ íƒí•˜ê³  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    """
    from langchain_community.vectorstores import FAISS
    from langchain_community.vectorstores.utils import DistanceStrategy

    # 1. ì„ë² ë”© ë°ì´í„° ì¤€ë¹„ ë° ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©)
    if vectors is None:
        texts = [d.page_content for d in docs]
        vectors_list = embedder.embed_documents(texts)
        vectors = np.array(vectors_list).astype("float32")
        # [ìµœì í™”] ì™¸ë¶€ì—ì„œ ê³„ì‚°ëœ ë²¡í„°ê°€ ì—†ì„ ë•Œë§Œ ì •ê·œí™” ìˆ˜í–‰
        import faiss
        faiss.normalize_L2(vectors)
    else:
        # [ìµœì í™”] ì´ë¯¸ ì •ê·œí™”ëœ numpy arrayë¡œ ë³€í™˜ (ì¶”ê°€ ì •ê·œí™” ìƒëµ)
        vectors = np.array(vectors).astype("float32")

    normalized_vectors: np.ndarray = vectors

    text_embeddings = list(
        zip([d.page_content for d in docs], normalized_vectors, strict=False)
    )
    metadatas = [d.metadata for d in docs]

    # 2. ë°ì´í„° ê·œëª¨ì— ë”°ë¥¸ ì¸ë±ìŠ¤ ì „ëµ ê²°ì •
    chunk_count = len(docs)

    # LangChain FAISSì—ì„œëŠ” distance_strategyë¥¼ í†µí•´ ë‚´ë¶€ ë©”íŠ¸ë¦­ ê²°ì •
    # METRIC_INNER_PRODUCT + Normalized Vectors = Cosine Similarity
    distance_strategy = DistanceStrategy.MAX_INNER_PRODUCT

    if chunk_count >= 1000:
        # HNSW ì ìš© (ëŒ€ê·œëª¨ ê³ ì† ê²€ìƒ‰)
        logger.info(f"[FAISS] HNSW ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘ (Chunks: {chunk_count})")

        import uuid

        import faiss
        from langchain_community.docstore.in_memory import InMemoryDocstore
        from langchain_core.documents import Document

        d = normalized_vectors.shape[1]
        index = faiss.IndexHNSWFlat(d, 32, faiss.METRIC_INNER_PRODUCT)
        index.hnsw.efConstruction = 128

        index.add(normalized_vectors)

        # [ìµœì í™”] ë£¨í”„ ë‚´ add ëŒ€ì‹  ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ ë²Œí¬ ìƒì„±
        doc_ids = [str(uuid.uuid4()) for _ in range(chunk_count)]
        new_docs = {
            doc_id: Document(page_content=content, metadata=metadata)
            for doc_id, content, metadata in zip(
                doc_ids, [d.page_content for d in docs], metadatas, strict=False
            )
        }
        docstore = InMemoryDocstore(new_docs)
        index_to_docstore_id = dict(enumerate(doc_ids))

        vector_store = FAISS(
            embedding_function=embedder,
            index=index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id,
            distance_strategy=distance_strategy
        )
        logger.info(f"[RAG] [INDEX] FAISS HNSW ë²Œí¬ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ | ì²­í¬: {chunk_count}")

    else:
        # ì†Œê·œëª¨ëŠ” ì •í™•í•œ ê²€ìƒ‰(Flat) ìˆ˜í–‰
        logger.debug(f"[RAG] [INDEX] FAISS Flat ì¸ë±ìŠ¤ ì‚¬ìš© | ì²­í¬: {chunk_count}")

        vector_store = FAISS.from_embeddings(
            text_embeddings=text_embeddings,
            embedding=embedder,
            metadatas=metadatas,
            distance_strategy=distance_strategy
        )

    return vector_store


def _create_bm25_retriever(docs: list[Document]) -> BM25Retriever:
    from langchain_community.retrievers import BM25Retriever

    # [ìµœì í™”] BM25 ìƒì„± ì‹œ í˜•íƒœì†Œ ë¶„ì„ê¸° ëŒ€ì‹  ëª¨ë“ˆ ë ˆë²¨ì— ì •ì˜ëœ bm25_tokenizer ì‚¬ìš©
    retriever = BM25Retriever.from_documents(docs, preprocess_func=bm25_tokenizer)
    retriever.k = RETRIEVER_CONFIG["search_kwargs"]["k"]
    return retriever


def _create_ensemble_retriever(
    vector_store: FAISS,
    bm25_retriever: BM25Retriever,
) -> EnsembleRetriever:
    from langchain.retrievers import EnsembleRetriever

    faiss_retriever = vector_store.as_retriever(
        search_type=RETRIEVER_CONFIG["search_type"],
        search_kwargs=RETRIEVER_CONFIG["search_kwargs"],
    )
    return EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=RETRIEVER_CONFIG["ensemble_weights"],
    )


@log_operation("ê²€ìƒ‰ ì»´í¬ë„ŒíŠ¸ ë¡œë“œ/ìƒì„±")
def _load_and_build_retrieval_components(
    file_path: str,
    file_name: str,
    _embedder: HuggingFaceEmbeddings,
    embedding_model_name: str,
    _on_progress=None,
    _file_hash: str | None = None
) -> tuple[DocumentList, FAISS, BM25Retriever, bool]:
    # 1. [ìµœì í™”] íŒŒì¼ í†µí•© ë¡œë“œ ë° í•´ì‹œ ê³„ì‚°
    # í•´ì‹œê°€ ì£¼ì–´ì§€ì§€ ì•Šì•˜ë‹¤ë©´ íŒŒì¼ì„ ë¯¸ë¦¬ ì½ì–´ í•´ì‹œ ê³„ì‚°ê³¼ ë¡œë”©ì— ê³µìœ 
    file_bytes = None
    if _file_hash is None:
        try:
            with open(file_path, "rb") as f:
                file_bytes = f.read()
            _file_hash = _compute_file_hash(file_path, data=file_bytes)
        except Exception as e:
            logger.error(f"íŒŒì¼ í†µí•© ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹(íŒŒì¼ ê²½ë¡œ ê¸°ë°˜)ìœ¼ë¡œ í´ë°±í•˜ê¸° ìœ„í•´ None ìœ ì§€

    cache = VectorStoreCache(file_path, embedding_model_name, file_hash=_file_hash)
    doc_splits, vector_store, bm25_retriever = cache.load(_embedder)

    cache_used = all(x is not None for x in [doc_splits, vector_store, bm25_retriever])

    if not cache_used:
        import numpy as np
        import torch

        # [ìµœì í™”] ì´ë¯¸ ì½ì–´ë‘” ë°”ì´íŠ¸ê°€ ìˆë‹¤ë©´ í™œìš©
        if file_bytes is not None:
            # _load_pdf_docsë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ëŒ€ì‹  ë°”ì´íŠ¸ ê¸°ë°˜ ë¡œì§ ìˆ˜í–‰ ê°€ëŠ¥í•˜ì§€ë§Œ,
            # êµ¬ì¡° ìœ ì§€ë¥¼ ìœ„í•´ _load_pdf_docs ë‚´ë¶€ì— ë°”ì´íŠ¸ ì§€ì› ë¡œì§ ì¶”ê°€ í•„ìš”
            # ì—¬ê¸°ì„œëŠ” ì¼ê´€ì„±ì„ ìœ„í•´ _load_pdf_docs í˜¸ì¶œ (ì´ë¯¸ ë‚´ë¶€ì—ì„œ ìµœì í™”ë¨)
            docs = _load_pdf_docs(file_path, file_name, on_progress=_on_progress)
        else:
            docs = _load_pdf_docs(file_path, file_name, on_progress=_on_progress)
        # ë¹ˆ ë¬¸ì„œ ì²˜ë¦¬
        if not docs:
            raise EmptyPDFError(
                filename=file_name,
                details={"reason": "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."},
            )

        if _on_progress:
            _on_progress()

        # [ìµœì í™” 1] ë¬¸ì„œ í¬ê¸°ì— ë”°ë¥¸ ë°”ì´íŒ¨ìŠ¤ ì „ëµ (2000ì ë¯¸ë§Œì€ ê³ ì† ì²˜ë¦¬)
        total_text_len = sum(len(d.page_content) for d in docs)
        is_small_doc = total_text_len < 2000

        # [ìµœì í™” 2] 1ì°¨ ë¶„í•  ë° ì„ë² ë”© ìƒì„± (ë‹¨ì¼ íŒ¨ìŠ¤)
        doc_splits, precomputed_vectors = _split_documents(docs, _embedder)
        if _on_progress:
            _on_progress()

        if not doc_splits:
            raise InsufficientChunksError(chunk_count=0, min_required=1)

        # [ìµœì í™” 3] ë²¡í„° ì¬ì‚¬ìš©ì„ í†µí•œ ì¸ë±ìŠ¤ ìµœì í™”
        optimized_vectors: Any = precomputed_vectors
        q_meta = None
        optimizer = None

        if not is_small_doc:
            try:
                SessionManager.add_status_log("ì¸ë±ìŠ¤ ìµœì í™” ì¤‘")
                if _on_progress:
                    _on_progress()

                # ë²¡í„°ê°€ ì—†ìœ¼ë©´(ê¸°ë³¸ ë¶„í• ê¸° ì‚¬ìš© ì‹œ) í•œ ë²ˆë§Œ ìƒì„±
                if optimized_vectors is None:
                    texts = [d.page_content for d in doc_splits]
                    vectors = _embedder.embed_documents(texts)
                    optimized_vectors = [np.array(v) for v in vectors]

                optimizer = get_index_optimizer()
                doc_splits, optimized_vectors, q_meta, stats = optimizer.optimize_index(
                    doc_splits, optimized_vectors
                )

                # [ìˆ˜ì •] ì–‘ìí™”ëœ ë²¡í„°ë¥¼ ì›ë˜ì˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ ë³´ì¥
                if optimizer and q_meta and q_meta.get("method") != "none":
                    optimized_vectors = optimizer.quantizer.dequantize_vectors(
                        optimized_vectors, q_meta
                    )

                logger.info(f"ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ: ì¤‘ë³µ {stats.pruned_documents}ê°œ ì œê±°")
                SessionManager.replace_last_status_log(
                    f"ì¤‘ë³µ ë‚´ìš© {stats.pruned_documents}ê°œ ì •ë¦¬"
                )
                if _on_progress:
                    _on_progress()
            except Exception as e:
                logger.warning(f"ì¸ë±ìŠ¤ ìµœì í™” ë‹¨ê³„ ê±´ë„ˆëœ€ (ê²½ë¯¸í•œ ì˜¤ë¥˜): {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ doc_splitsì™€ optimized_vectorsëŠ” ìœ ì§€ë¨

        # [ìµœì í™” 4] ê³„ì‚°ëœ ë²¡í„°ë¥¼ FAISSì— ì§ì ‘ ì£¼ì… (GPU ì¶”ê°€ í˜¸ì¶œ 0íšŒ)
        vector_store = _create_vector_store(
            doc_splits, _embedder, vectors=optimized_vectors
        )
        bm25_retriever = _create_bm25_retriever(doc_splits or [])

        # ìºì‹œ ì €ì¥
        cache.save(doc_splits, vector_store, bm25_retriever)

        # [ìµœì í™” 5] GPU ìì› ì¦‰ì‹œ ë°˜í™˜ (Ollamaì™€ì˜ VRAM ê²½í•© ë°©ì§€)
        if torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
                logger.debug("[System] [Memory] CUDA ìºì‹œ ë¹„ìš°ê¸° ì™„ë£Œ")
            except Exception:
                pass

        SessionManager.add_status_log("ì‹ ê·œ ì¸ë±ì‹± ì™„ë£Œ")

    return doc_splits, vector_store, bm25_retriever, cache_used


@log_operation("RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")
def build_rag_pipeline(
    uploaded_file_name: str,
    file_path: str,
    embedder: HuggingFaceEmbeddings,
    on_progress=None,
) -> tuple[str, bool]:
    """
    RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ê³  ì„¸ì…˜ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    # [ìµœì í™”] íŒŒì¼ í•´ì‹œëŠ” ì—¬ê¸°ì„œ í•œ ë²ˆë§Œ ê³„ì‚°í•˜ì—¬ í•˜ìœ„ í•¨ìˆ˜ë¡œ ì „ë‹¬
    file_hash = _compute_file_hash(file_path)

    # [ìµœì í™”] embedder ê°ì²´ëŠ” í•´ì‹±ì—ì„œ ì œì™¸í•˜ê³ , ëª¨ë¸ëª…ê³¼ íŒŒì¼ í•´ì‹œë¥¼ ëª…ì‹œì  í‚¤ë¡œ ì „ë‹¬
    doc_splits, vector_store, bm25_retriever, cache_used = (
        _load_and_build_retrieval_components(
            file_path,
            uploaded_file_name,
            _embedder=embedder,
            embedding_model_name=embedder.model_name,
            _on_progress=on_progress,
            _file_hash=file_hash
        )
    )

    if cache_used:
        SessionManager.add_status_log("ìºì‹œ ë°ì´í„° ë¡œë“œ")
        if on_progress:
            on_progress()

    # [ìµœì í™”] ë³‘ë ¬ ê²€ìƒ‰ì„ ìœ„í•´ ê°œë³„ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ë° ì„¸ì…˜ ì €ì¥
    faiss_retriever = vector_store.as_retriever(
        search_type=RETRIEVER_CONFIG["search_type"],
        search_kwargs=RETRIEVER_CONFIG["search_kwargs"],
    )

    SessionManager.set("faiss_retriever", faiss_retriever)
    SessionManager.set("bm25_retriever", bm25_retriever)

    # ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€ (EnsembleRetrieverë„ ìƒì„±)
    from langchain.retrievers import EnsembleRetriever

    final_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=RETRIEVER_CONFIG["ensemble_weights"],
    )
    rag_engine = build_graph(retriever=final_retriever)

    SessionManager.set("vector_store", vector_store)
    SessionManager.set("rag_engine", rag_engine)
    SessionManager.set("pdf_processed", True)
    SessionManager.add_status_log("ì§ˆë¬¸ ê°€ëŠ¥")

    if on_progress:
        on_progress()

    logger.info(
        f"RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ: '{uploaded_file_name}' (ìºì‹œ ì‚¬ìš©: {cache_used})"
    )

    if cache_used:
        return f"'{uploaded_file_name}' ë¬¸ì„œ ìºì‹œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ", True
    return (
        f"'{uploaded_file_name}' ì‹ ê·œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ"
    ), False


@log_operation("íŒŒì´í”„ë¼ì¸ LLM ì—…ë°ì´íŠ¸")
def update_llm_in_pipeline(llm: T | None) -> None:
    """
    ì„¸ì…˜ì˜ LLMì„ êµì²´í•©ë‹ˆë‹¤.

    ê·¸ë˜í”„ê°€ ì„¸ì…˜ì—ì„œ LLMì„ ê°€ì ¸ì˜¤ë¯€ë¡œ ì¬ë¹Œë“œí•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.

    Args:
        llm: ì—…ë°ì´íŠ¸í•  ìƒˆë¡œìš´ LLM ëª¨ë¸.

    Raises:
        ValueError: RAG íŒŒì´í”„ë¼ì¸ì´ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ì„ ë•Œ.
    """
    if not SessionManager.get("pdf_processed"):
        raise ValueError("RAG íŒŒì´í”„ë¼ì¸ì´ êµ¬ì¶•ë˜ì§€ ì•Šì•„ LLMì„ ì—…ë°ì´íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    SessionManager.set("llm", llm)
    logger.info(f"ì„¸ì…˜ LLM ì—…ë°ì´íŠ¸ ì™„ë£Œ: '{getattr(llm, 'model', 'unknown')}'")
