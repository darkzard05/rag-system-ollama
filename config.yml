# config.yml
# 애플리케이션의 주요 설정을 관리하는 파일입니다.

# --- 모델 설정 ---
models:
  # 기본으로 사용할 Ollama 모델 이름
  default_ollama: "qwen3:4b-instruct-2507-q4_K_M"
  
  # Ollama 서버 주소 (로컬: http://127.0.0.1:11434, 도커: http://host.docker.internal:11434)
  base_url: "http://127.0.0.1:11434"

  # Ollama 모델의 토큰 생성 제한 (기본값: 4096)
  # -1로 설정하면 모델의 기본 설정 또는 무제한(컨텍스트 윈도우까지)으로 생성합니다.
  ollama_num_predict: -1

  # Ollama 모델의 온도 설정
  temperature: 0.1

  # Ollama 모델의 컨텍스트 윈도우 설정
  num_ctx: 4096

  # Ollama 모델의 탑-피 설정
  top_p: 0.8

  # 사용 가능한 임베딩 모델 목록
  available_embeddings:
    - "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    - "intfloat/multilingual-e5-large-instruct"
    - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    - "sentence-transformers/all-MiniLM-L6-v2"
  
  # 모델 캐시 디렉터리
  cache_dir: ".model_cache"

  # 임베딩 모델의 배치 크기
  embedding_batch_size: "auto"

  # 임베딩 모델 실행 디바이스
  embedding_device: "auto"


# --- RAG 파이프라인 설정 ---
rag:
  # 벡터 저장소 캐시 디렉터리
  vector_store_cache_dir: ".model_cache/vector_store_cache"
  
  # 리트리버 설정
  retriever:
    search_type: "similarity"
    search_kwargs:
      k: 20
    # BM25와 FAISS의 가중치
    ensemble_weights: [0.4, 0.6]

  # 재순위화(Reranking) 설정
  reranker:
    enabled: false
    model_name: "BAAI/bge-reranker-v2-m3" 
    top_k: 5
    bypass_threshold: 0.85
    min_score: 0.35
    max_rerank_docs: 15

  # 텍스트 분할 설정
  text_splitter:
    chunk_size: 500
    chunk_overlap: 100
  
  # 의미론적 분할 설정
  semantic_chunker:
    enabled: true
    breakpoint_threshold_type: "similarity_threshold"
    breakpoint_threshold_value: 0.6
    sentence_split_regex: "(?<=[.!?])\\s+" # Corrected escape for backslash
    min_chunk_size: 200
    max_chunk_size: 1000
    similarity_threshold: 0.6

  # 쿼리 확장 설정
  query_expansion:
    enabled: true

  # 프롬프트 설정
  prompts:
    # 1. 시스템 통합 프로토콜 (Core logic for PDF analysis)
    analysis_protocol: |
      당신은 "PDF 문서 정밀 분석관"입니다. 아래의 [엄격한 분석 규칙]을 반드시 준수하여 답변하십시오.

      [엄격한 분석 규칙]
      1. 답변 언어 원칙: 반드시 질문자가 사용한 언어와 동일한 언어로 답변하십시오. (예: 한국어 질문 -> 한국어 답변, 영어 질문 -> 영어 답변). 이는 문서의 언어와 상관없이 적용됩니다.
      2. 근거 우선 원칙: 오직 제공된 [문서 컨텍스트]의 정보만을 사용하십시오. 문서에 없는 내용은 "문서 내에서 관련 근거를 찾을 수 없습니다"라고 명확히 밝히십시오.
      3. 실시간 인용(Inline Citation): 모든 정보의 출처 페이지를 `[p.X]` 형식으로 사실 언급 직후에 붙이십시오. 문장 끝이 아닌, 수치나 고유명사 바로 뒤에 배치하는 것이 원칙입니다.
      4. 가독성 최적화: 자연스러운 문장과 불렛 포인트를 주로 사용하십시오.

    # 2. 의도별 특화 지시 (Persona-based specialized prompts)
    research_system_prompt: |
      [심층 분석 모드]
      여러 페이지에 흩어진 정보를 대조하고 연결하여 종합적인 보고서 형태로 작성하십시오. 
      단순 나열이 아닌 '현상-원인-결과' 또는 '장점-단점-시사점'의 구조를 갖추십시오. 모든 핵심 주장에는 인용 태그가 필수입니다.

    factoid_system_prompt: |
      [핵심 추출 모드]
      질문에 대한 직접적인 정답(수치, 이름, 날짜 등)을 찾아 가장 간결하게 답변하십시오. 
      불필요한 미사여구는 생략하고, 정답 뒤에 반드시 출처 페이지 `[p.X]`를 명시하십시오.

    greeting_system_prompt: |
      [안내 모드]
      사용자의 인사나 일상 대화에 친절하게 응답하십시오. 
      현재 분석 중인 문서가 있다면 "무엇을 도와드릴까요?"와 같이 분석 준비 완료 상태임을 알리십시오.

    out_of_context_system_prompt: |
      [범위 초과 안내]
      질문하신 내용이 현재 문서의 범위를 벗어나 있음을 정중히 안내하십시오. 
      대신 문서 내에서 가장 유사하거나 답변 가능한 주제가 있다면 대안으로 제안하십시오.

    # 3. 기본 및 휴먼 프롬프트 (Fallback & Structure)
    qa_system_prompt: |
      [일반 분석 모드]
      문서의 내용을 정확하게 요약하고 분석하십시오. 모든 사실적 진술 뒤에는 페이지 번호 `[p.X]`가 포함되어야 합니다.

    qa_human_prompt: |
      아래 [문서 컨텍스트]를 바탕으로 [사용자 질문]에 대해 전문적인 답변을 작성하십시오.

      ### [문서 컨텍스트] ###
      {context}

      ### [사용자 질문] ###
      {input}

    # 4. 쿼리 확장(Multi-Query) 최적화
    query_expansion_prompt: |
      사용자의 질문을 PDF 문서 검색에 최적화된 3개의 구체적인 키워드 또는 짧은 질문으로 변환하십시오.
      - 한 줄에 하나씩 출력
      - 수치나 고유명사가 있다면 반드시 포함
      - 부가 설명 없이 정답만 출력


# --- 캐시 보안 설정 ---
cache_security:
  security_level: "medium"
  hmac_secret: null
  trusted_paths:
    - ".model_cache"
  on_validation_failure: "regenerate"
  check_permissions: true
  expected_file_mode: 0o644
  expected_dir_mode: 0o755

# --- 채팅 UI 설정 ---
ui:
  container_height: 700
  messages:
    preparing_answer: "답변 생성 준비 중..."
    thinking: "🤔 생각을 정리하는 중입니다..."
    no_thought_process: "아직 생각 과정이 없습니다."
    no_related_info: |
      제공된 문서에서 질문에 대한 명확한 정보를 찾기 어렵습니다. 😥

      **다음을 시도해 보세요:**
      - 질문을 좀 더 명확하게 하거나 다른 키워드를 사용해 보세요.
      - 좀 더 일반적이거나 넓은 범위의 질문을 해보세요.

    sidebar_title: "⚙️ 설정"
    pdf_uploader_label: "PDF 파일 업로드"
    model_selector_label: "추론 모델 선택"
    embedding_selector_label: "임베딩 모델 선택"
    system_status_title: "시스템 상태"
    loading_models: "추론 모델 목록을 불러오는 중..."

    pdf_viewer_title: "📄 PDF 미리보기"
    pdf_viewer_no_file: "미리볼 PDF가 없습니다. 사이드바에서 파일을 업로드해주세요."
    pdf_viewer_prev_button: "← 이전"
    pdf_viewer_next_button: "다음 →"
    pdf_viewer_page_slider: "페이지 이동"
    pdf_viewer_error: "PDF를 표시하는 중 오류가 발생했습니다: {e}"

    chat_title: "💬 채팅"
    chat_input_placeholder: "PDF 내용에 대해 질문해보세요."
    chat_no_qa_system: "QA 시스템이 준비되지 않았습니다. PDF를 먼저 처리해주세요."
    chat_guide: |
      **RAG-Chat에 오신 것을 환영합니다!**

      **💡 사용 가이드**
      - **PDF 업로드:** 좌측 사이드바에서 분석하고 싶은 PDF를 업로드하세요.
      - **모델 선택:** 로컬 추론 모델을 선택할 수 있습니다.
      - **질문하기:** 문서 처리가 완료되면, 내용에 대해 자유롭게 질문할 수 있습니다.
      - **PDF 뷰어:** 우측에서 원본 문서를 함께 보며 대화할 수 있습니다.
    
    streaming_error: "스트리밍 답변 생성 중 오류 발생: {e}"
    generic_error: "오류가 발생했습니다: {error_msg}"
    retry_button: "재시도"
    errors:
      ollama_not_running: "Ollama 서버에 연결할 수 없습니다. 서버가 실행 중인지 확인해주세요."