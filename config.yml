# config.yml
# 애플리케이션의 주요 설정을 관리하는 파일입니다.

# --- 모델 설정 ---
models:
  # 기본으로 사용할 Ollama 모델 이름
  default_ollama: "qwen3:4b-instruct-2507-q4_K_M"

  # 기본으로 사용할 임베딩 모델 이름 (Ollama 또는 HuggingFace)
  default_embedding: "nomic-embed-text"
  
  # Ollama 서버 주소 (로컬: http://127.0.0.1:11434, 도커: http://host.docker.internal:11434)
  base_url: "http://127.0.0.1:11434"

  # Ollama 모델의 토큰 생성 제한 (기본값: 4096)
  # -1로 설정하면 모델의 기본 설정 또는 무제한(컨텍스트 윈도우까지)으로 생성합니다.
  ollama_num_predict: -1

  # Ollama 모델의 온도 설정
  temperature: 0.1

  # Ollama 모델의 컨텍스트 윈도우 설정
  num_ctx: 8192

  # Ollama 모델의 탑-피 설정
  top_p: 0.8
  
  # 모델 캐시 디렉터리
  cache_dir: ".model_cache"

  # 임베딩 모델의 배치 크기
  embedding_batch_size: "auto"

  # 임베딩 모델 실행 디바이스
  embedding_device: "cpu"


# --- RAG 파이프라인 설정 ---
rag:
  # 벡터 저장소 캐시 디렉터리
  vector_store_cache_dir: ".model_cache/vector_store_cache"
  
  # 리트리버 설정
  retriever:
    search_type: "similarity"
    search_kwargs:
      k: 25  # 충분한 후보군 확보를 위해 상향
    # BM25와 FAISS의 가중치
    ensemble_weights: [0.4, 0.6]

  # 재순위화(Reranking) 설정
  reranker:
    enabled: true
    # 더 정확한 MiniLM 모델로 업그레이드
    model_name: "ms-marco-MiniLM-L-12-v2" 
    top_k: 10 # 컨텍스트 풍부화
    bypass_threshold: 0.95
    min_score: 0.0001 # 매우 낮은 점수도 일단 수용 (순위 우선)
    max_rerank_docs: 25

  # 통합 RAG 파라미터 (기존의 의도별 파라미터를 하나로 통합)
  parameters:
    retrieval_k: 25
    rerank_top_k: 8
    rerank_threshold: 0.3
    sort_by_page: false
    use_strict_grading: false

  # 텍스트 분할 설정
  text_splitter:
    chunk_size: 500
    chunk_overlap: 100
  
  # 추출 엔진 설정 (IBM Docling)
  parsing:
    engine: "docling"
    do_ocr: false
    do_table_structure: true

  # 의미론적 분할 설정
  semantic_chunker:
    enabled: true
    breakpoint_threshold_type: "similarity_threshold"
    breakpoint_threshold_value: 0.6
    # 문장 분할 정규식
    sentence_split_regex: "([.!?]\\s+)" 
    min_chunk_size: 200
    max_chunk_size: 1000
    similarity_threshold: 0.6

  # 쿼리 확장 설정
  query_expansion:
    enabled: false

  # 프롬프트 설정
    # 1. 핵심 페르소나 및 분석 원칙 (The Core Identity)
    analysis_protocol: |
      당신은 업계 최고 수준의 '문서 분석 전문가'입니다. 
      제공된 자료를 바탕으로 질문에 대해 논리적이고 정확하며 신뢰할 수 있는 답변을 생성하는 것이 당신의 사명입니다.

      <Core_Guidelines>
      1. [Language]: 사용자의 질문 언어에 맞추어 답변하십시오. (문서 언어와 무관)
      2. [Groundedness]: 제공된 <Context>를 철저히 분석하십시오. 직접적인 정답이 없더라도 관련 기술 용어, 수치, 부분적 원리를 최대한 추출하여 '가용 정보 기반 분석'을 제공하십시오. 단, 없는 내용을 지어내지는 마십시오.
      3. [Citations]: 모든 정보의 출처 페이지를 `[p.X]` 형태로 인용하십시오. 가급적 특정 수치나 핵심 명사 바로 뒤에 붙이십시오.
      4. [Synthesis]: 흩어져 있는 정보들을 유기적으로 연결하여 질문에 대한 다각도 분석을 수행하십시오.
      </Core_Guidelines>

    # 2. 상황별 특화 전략 (Tailored Strategies)
    research_system_prompt: |
      현재 모드: [심층 분석 및 종합 요약]. 
      제공된 여러 페이지의 정보를 유기적으로 연결하여 '원인-결과' 또는 '비교-대조'의 구조를 가진 종합적인 보고서 형태로 작성하십시오.
      추상적인 설명보다는 컨텍스트에 포함된 구체적인 수치, 핵심 용어, 예시를 최대한 인용하며 상세히 기술하십시오.
    factoid_system_prompt: |
      현재 모드: [핵심 추출]. 질문에 대한 직접적인 정답(수치, 고유명사 등)을 가장 간결하게 제시하십시오. 부가 설명은 지양합니다.
    greeting_system_prompt: |
      현재 모드: [사용자 안내]. 친절하게 인사에 화답하고, 문서 분석을 시작할 준비가 되었음을 알리십시오.
    out_of_context_system_prompt: |
      현재 모드: [범위 외 안내]. 질문이 문서의 범위를 벗어났음을 정중히 알리고, 문서 내에서 답변 가능한 가장 유사한 주제를 제안하십시오.

    # 3. 데이터 구조화 템플릿 (Data Injection)
    qa_system_prompt: "" # 분석 프로토콜로 통합하여 비워둠
    qa_human_prompt: |
      제공된 컨텍스트를 정밀 분석하여 사용자의 질문에 답변하십시오.

      <Context>
      {context}
      </Context>

      <Question>
      {input}
      </Question>

    # 4. 검색 최적화 지시
    query_expansion_prompt: |
      사용자의 질문을 문서 검색에 최적화된 3개의 핵심 키워드 또는 구문으로 변환하십시오.
      - 불필요한 서술어 제외
      - 고유명사 및 수치 보존
      - JSON 형식을 엄격히 준수할 것


# --- 캐시 보안 설정 ---
cache_security:
  security_level: "medium"
  hmac_secret: null
  trusted_paths:
    - ".model_cache"
  on_validation_failure: "regenerate"
  check_permissions: true
  expected_file_mode: 0o644
  expected_dir_mode: 0o755

# --- 전역 캐시 활성화 설정 ---
global_cache:
  # 벡터 저장소(인덱싱 결과) 캐시 사용 여부
  enable_vector_cache: false
  # LLM 답변(Response) 캐시 사용 여부
  enable_response_cache: false

# --- 채팅 UI 설정 ---
ui:
  container_height: 700
  messages:
    preparing_answer: "답변 생성 준비 중..."
    thinking: "🤔 생각을 정리하는 중입니다..."
    no_thought_process: "아직 생각 과정이 없습니다."
    no_related_info: |
      제공된 문서에서 질문에 대한 명확한 정보를 찾기 어렵습니다. 😥

      **다음을 시도해 보세요:**
      - 질문을 좀 더 명확하게 하거나 다른 키워드를 사용해 보세요.
      - 좀 더 일반적이거나 넓은 범위의 질문을 해보세요.

    sidebar_title: "⚙️ 설정"
    pdf_uploader_label: "PDF 파일 업로드"
    model_selector_label: "추론 모델 선택"
    embedding_selector_label: "임베딩 모델 선택"
    system_status_title: "시스템 상태"
    loading_models: "추론 모델 목록을 불러오는 중..."

    pdf_viewer_title: "📄 PDF 미리보기"
    pdf_viewer_no_file: "미리볼 PDF가 없습니다. 사이드바에서 파일을 업로드해주세요."
    pdf_viewer_prev_button: "← 이전"
    pdf_viewer_next_button: "다음 →"
    pdf_viewer_page_slider: "페이지 이동"
    pdf_viewer_error: "PDF를 표시하는 중 오류가 발생했습니다: {e}"

    chat_title: "💬 채팅"
    chat_input_placeholder: "PDF 내용에 대해 질문해보세요."
    chat_no_qa_system: "QA 시스템이 준비되지 않았습니다. PDF를 먼저 처리해주세요."
    chat_guide: |
      **RAG-Chat에 오신 것을 환영합니다!**

      **💡 사용 가이드**
      - **PDF 업로드:** 좌측 사이드바에서 분석하고 싶은 PDF를 업로드하세요.
      - **모델 선택:** 로컬 추론 모델을 선택할 수 있습니다.
      - **질문하기:** 문서 처리가 완료되면, 내용에 대해 자유롭게 질문할 수 있습니다.
      - **PDF 뷰어:** 우측에서 원본 문서를 함께 보며 대화할 수 있습니다.
    
    streaming_error: "스트리밍 답변 생성 중 오류 발생: {e}"
    generic_error: "오류가 발생했습니다: {error_msg}"
    retry_button: "재시도"
    errors:
      ollama_not_running: "Ollama 서버에 연결할 수 없습니다. 서버가 실행 중인지 확인해주세요."
