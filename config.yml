# config.yml
# 애플리케이션의 주요 설정을 관리하는 파일입니다.

# --- 모델 설정 ---
models:
  # 기본으로 사용할 Ollama 모델 이름
  default_ollama: "gemma3:8b"
  
  # Ollama 모델의 토큰 생성 제한 (기본값: 2048)
  ollama_num_predict: 2048

  # Ollama 모델의 온도 설정 (기본값: 0.1)
  temperature: 0.1

  # Ollama 모델의 컨텍스트 윈도우 설정 (기본값: 4096)
  num_ctx: 4096

  # Ollama 모델의 탑-피 설정 (기본값: 0.8)
  top_p: 0.8

  # 사용 가능한 임베딩 모델 목록
  available_embeddings:
    - "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    - "intfloat/multilingual-e5-large-instruct"
    - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    - "sentence-transformers/all-MiniLM-L6-v2"
  
  # 모델 캐시 디렉터리
  cache_dir: ".model_cache"

  # 임베딩 모델의 배치 크기 ('auto'로 설정 시 VRAM에 따라 자동 조절, 숫자로 직접 지정 가능)
  embedding_batch_size: "auto"

# --- RAG 파이프라인 설정 ---
rag:
  # 벡터 저장소 캐시 디렉터리
  vector_store_cache_dir: ".model_cache/vector_store_cache"
  
  # 리트리버 설정
  retriever:
    search_type: "similarity"
    search_kwargs:
      k: 5
    # BM25와 FAISS의 가중치 (Ensemble Retriever용)
    ensemble_weights: [0.4, 0.6]

  # 텍스트 분할 설정
  text_splitter:
    chunk_size: 1500
    chunk_overlap: 150

    # 프롬프트 설정
    prompts:
      qa_system_prompt: | 
        당신은 주어진 컨텍스트(Context) 정보만을 사용하여 사용자의 질문에 답변하는 전문 AI 어시스턴트입니다.

        **##핵심 지시사항**
        1.  **컨텍스트 엄수:** 답변은 반드시, 오직 `[Context]`로 제공된 내용에만 근거해야 합니다. 당신의 사전 지식을 절대로 사용하지 마세요.
        2.  **정확한 출처 명시:** 답변에 포함된 모든 정보의 출처를 `(p.페이지, [문서번호])` 형식으로 문장 끝에 명확하게 인용하세요.
        3.  **정보 부재 시:** 만약 컨텍스트에서 질문에 대한 답을 찾을 수 없다면, "죄송하지만, 제공된 문서의 내용만으로는 답변을 찾을 수 없습니다."라고 답변하세요.
        4.  **언어:** 사용자의 질문과 같은 언어로 답변하세요.
# --- 채팅 UI 설정 ---
ui:
  # 채팅 및 PDF 뷰어 컨테이너의 높이
  container_height: 650
  
  # UI에 표시될 메시지
  messages:
    # --- 일반 메시지 ---
    preparing_answer: "답변 생성 준비 중..."
    thinking: "🤔 생각을 정리하는 중입니다..."
    no_thought_process: "아직 생각 과정이 없습니다."
    no_related_info: |
      제공된 문서에서 질문에 대한 명확한 정보를 찾기 어렵습니다. 😥

      **다음을 시도해 보세요:**
      - 질문을 좀 더 명확하게 하거나 다른 키워드를 사용해 보세요.
      - 좀 더 일반적이거나 넓은 범위의 질문을 해보세요.

    # --- 사이드바 ---
    sidebar_title: "⚙️ 설정"
    pdf_uploader_label: "PDF 파일 업로드"
    model_selector_label: "LLM 모델 선택"
    embedding_selector_label: "임베딩 모델 선택"
    system_status_title: "📊 시스템 상태"
    loading_models: "LLM 모델 목록을 불러오는 중..."

    # --- PDF 뷰어 ---
    pdf_viewer_title: "📄 PDF 미리보기"
    pdf_viewer_no_file: "미리볼 PDF가 없습니다. 사이드바에서 파일을 업로드해주세요."
    pdf_viewer_prev_button: "← 이전"
    pdf_viewer_next_button: "다음 →"
    pdf_viewer_page_slider: "페이지 이동"
    pdf_viewer_error: "PDF를 표시하는 중 오류가 발생했습니다: {e}"

    # --- 채팅 ---
    chat_title: "💬 채팅"
    chat_input_placeholder: "PDF 내용에 대해 질문해보세요."
    chat_no_qa_system: "QA 시스템이 준비되지 않았습니다. PDF를 먼저 처리해주세요."
    chat_welcome: |
      **RAG-Chat에 오신 것을 환영합니다!**

      사이드바에서 PDF 파일을 업로드하여 문서 내용에 대한 대화를 시작해보세요.
    chat_guide: |
      **💡 사용 가이드**
      - **PDF 업로드:** 좌측 사이드바에서 분석하고 싶은 PDF를 업로드하세요.
      - **모델 선택:** 로컬 `Ollama` 모델을 선택할 수 있습니다.
      - **질문하기:** 문서 처리가 완료되면, 내용에 대해 자유롭게 질문할 수 있습니다.
      - **PDF 뷰어:** 우측에서 원본 문서를 함께 보며 대화할 수 있습니다.
      
      **⚠️ 알아두실 점**
      - **답변의 정확성:** 답변은 업로드된 PDF 내용만을 기반으로 생성되며, 사실이 아닐 수 있습니다.
      - **초기 로딩:** 임베딩 모델을 처음 사용하면 다운로드에 몇 분이 소요될 수 있습니다.
    
    # --- 오류 및 상태 메시지 ---
    streaming_error: "스트리밍 답변 생성 중 오류 발생: {e}"
    generic_error: "오류가 발생했습니다: {error_msg}"
    retry_button: "재시도"
    errors:
      ollama_not_running: "Ollama 서버에 연결할 수 없습니다. 서버가 실행 중인지 확인해주세요."
