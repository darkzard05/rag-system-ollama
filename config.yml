# config.yml
# 애플리케이션의 주요 설정을 관리하는 파일입니다.

# --- 모델 설정 ---
models:
  # 기본으로 사용할 Ollama 및 Gemini 모델 이름
  default_ollama: "qwen3:4b"
  default_gemini: "gemini-2.5-flash"
  
  # Ollama 모델의 토큰 생성 제한 (기본값: 2048)
  ollama_num_predict: 2048
  
  # Gemini API에서 우선적으로 보여줄 모델 목록
  preferred_gemini:
    - "gemini-2.5-pro"
    - "gemini-2.5-flash"
    - "gemini-2.0-flash"
  
  # 사용 가능한 임베딩 모델 목록
  available_embeddings:
    - "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    - "intfloat/multilingual-e5-large-instruct"
    - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    - "sentence-transformers/all-MiniLM-L6-v2"
  
  # 모델 캐시 디렉터리
  cache_dir: ".model_cache"

  # 임베딩 모델의 배치 크기 ('auto'로 설정 시 VRAM에 따라 자동 조절, 숫자로 직접 지정 가능)
  embedding_batch_size: "auto"

# --- RAG 파이프라인 설정 ---
rag:
  # 벡터 저장소 캐시 디렉터리
  vector_store_cache_dir: ".model_cache/vector_store_cache"
  
  # 리트리버 설정
  retriever:
    search_type: "similarity"
    search_kwargs:
      k: 5
    # BM25와 FAISS의 가중치 (Ensemble Retriever용)
    ensemble_weights: [0.4, 0.6]

  # 텍스트 분할 설정
  text_splitter:
    chunk_size: 1500
    chunk_overlap: 150

    # 프롬프트 설정
    prompts:
      qa_system_prompt: | 
        당신은 주어진 컨텍스트(Context) 정보만을 사용하여 사용자의 질문에 답변하는 전문 AI 어시스턴트입니다.

        **##핵심 지시사항**
        1.  **컨텍스트 엄수:** 답변은 반드시, 오직 `[Context]`로 제공된 내용에만 근거해야 합니다. 당신의 사전 지식을 절대로 사용하지 마세요.
        2.  **정확한 출처 명시:** 답변에 포함된 모든 정보의 출처를 `(p.페이지, [문서번호])` 형식으로 문장 끝에 명확하게 인용하세요.
        3.  **정보 부재 시:** 만약 컨텍스트에서 질문에 대한 답을 찾을 수 없다면, "죄송하지만, 제공된 문서의 내용만으로는 답변을 찾을 수 없습니다."라고 답변하세요.
        4.  **언어:** 사용자의 질문과 같은 언어로 답변하세요.
# --- 채팅 UI 설정 ---
ui:
  # 채팅 및 PDF 뷰어 컨테이너의 높이
  container_height: 650
  
  # UI에 표시될 메시지
  messages:
    preparing_answer: "답변 생성 준비 중..."
    thinking: "🤔 생각을 정리하는 중입니다..."
    no_thought_process: "아직 생각 과정이 없습니다."
    no_related_info: |
      제공된 문서에서 질문에 대한 명확한 정보를 찾기 어렵습니다. 😥

      **다음을 시도해 보세요:**
      - 질문을 좀 더 명확하게 하거나 다른 키워드를 사용해 보세요.
      - 좀 더 일반적이거나 넓은 범위의 질문을 해보세요.
