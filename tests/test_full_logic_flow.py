import os
import sys
import asyncio
import pytest
from pathlib import Path
from unittest.mock import MagicMock

# src ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.join(os.getcwd(), "src"))

# Streamlit mock (SessionManagerê°€ st.session_stateë¥¼ ì‚¬ìš©í•¨)
import streamlit as st
if "session_state" not in st.__dict__:
    st.session_state = {}

from core.rag_core import build_rag_pipeline
from core.model_loader import load_embedding_model, load_llm
from core.session import SessionManager
from common.config import AVAILABLE_EMBEDDING_MODELS, OLLAMA_MODEL_NAME

@pytest.mark.asyncio
async def test_rag_system_full_flow():
    """
    LangGraph íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ PDF ë¡œë“œë¶€í„° ë‹µë³€ ìƒì„±ê¹Œì§€ì˜ ì „ì²´ ë¡œì§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    """
    print("\nğŸš€ RAG í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # 0. ì„¸ì…˜ ì´ˆê¸°í™”
    SessionManager.init_session()
    
    # 1. ëª¨ë¸ ë¡œë“œ (ì„ë² ë”© ë° LLM)
    embedding_model = AVAILABLE_EMBEDDING_MODELS[0]
    llm_model = OLLAMA_MODEL_NAME
    
    print(f"ğŸ§¬ ëª¨ë¸ ë¡œë”© ì¤‘: {embedding_model}, {llm_model}")
    embedder = await asyncio.to_thread(load_embedding_model, embedding_model)
    llm = await asyncio.to_thread(load_llm, llm_model)
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    # 2. PDF ë¬¸ì„œ ë¡œë“œ ë° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
    pdf_path = os.path.join("tests", "2201.07520v1.pdf")
    if not os.path.exists(pdf_path):
        # íŒŒì¼ì´ ì—†ì„ ê²½ìš° ë¹ˆ íŒŒì¼ì´ë¼ë„ ìƒì„±í•˜ê±°ë‚˜ ìŠ¤í‚µ (ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ íŒŒì¼ í™œìš© ê°€ì •)
        pytest.skip(f"í…ŒìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        
    print(f"ğŸ“‚ ë¬¸ì„œ ì¸ë±ì‹± ë° íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì‹œì‘: {pdf_path}")
    # SessionManagerì— LLM ë¯¸ë¦¬ ì„¤ì • (build_rag_pipelineì´ sessionì„ ì‚¬ìš©í•¨)
    SessionManager.set("llm", llm)
    
    msg, cache_used = await asyncio.to_thread(
        build_rag_pipeline,
        uploaded_file_name="2201.07520v1.pdf",
        file_path=pdf_path,
        embedder=embedder
    )
    print(f"âœ¨ {msg} (ìºì‹œ ì‚¬ìš©: {cache_used})")
    
    rag_engine = SessionManager.get("rag_engine")
    assert rag_engine is not None, "RAG ì—”ì§„(Graph)ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    # 3. ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸
    question = "What is the main contribution of this paper?"
    print(f"ğŸ’¬ ì§ˆë¬¸ ì…ë ¥: {question}")
    
    # LangGraph ì‹¤í–‰
    # configì— llmì„ ì „ë‹¬í•´ì•¼ í•¨ (graph_builder.pyì˜ generate_responseê°€ ì´ë¥¼ ê¸°ëŒ€í•¨)
    config = {"configurable": {"llm": llm}}
    
    # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì´ ì•„ë‹Œ ì¼ë°˜ ë¹„ë™ê¸° ì‹¤í–‰ (ainvoke)
    response = await rag_engine.ainvoke(
        {"input": question},
        config=config
    )
    
    print("\n" + "="*50)
    print("ğŸ¤– AI ë‹µë³€:")
    print(response.get("response", "ë‹µë³€ ì‹¤íŒ¨"))
    
    if response.get("thought"):
        print("\nğŸ’­ ì‚¬ê³  ê³¼ì •:")
        print(response.get("thought")[:200] + "...")
    print("-" * 50)
    
    # 4. ê²€ì¦
    ans_text = response.get("response", "")
    assert len(ans_text) > 10, "ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤."
    assert "documents" in response, "ì°¸ì¡° ë¬¸ì„œê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    print(f"ğŸ” ì°¸ì¡° ë¬¸ì„œ ìˆ˜: {len(response['documents'])}")
    print("="*50)
    print("\nâœ¨ ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    asyncio.run(test_rag_system_full_flow())
