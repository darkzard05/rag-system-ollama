import asyncio
import sys
import io
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent / "src"))

from common.config import OLLAMA_MODEL_NAME
from core.model_loader import load_llm
from core.graph_builder import build_graph
from langchain_core.documents import Document

# Windows ì¸ì½”ë”© ëŒ€ì‘
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")


async def verify_language(text: str, expected_lang: str) -> bool:
    """ì–¸ì–´ íŒë³„ (í•œê¸€ ê¸€ì ë¹„ìœ¨ ê¸°ë°˜)"""
    if not text:
        return False
    hangul_chars = [char for char in text if "\\\\uac00" <= char <= "\\\\ud7a3"]
    hangul_ratio = len(hangul_chars) / len(text)

    if expected_lang == "ko":
        return hangul_ratio > 0.05  # í•œê¸€ì´ 5% ì´ìƒì´ë©´ í•œêµ­ì–´ë¡œ ê°„ì£¼
    else:
        return hangul_ratio < 0.01  # í•œê¸€ì´ 1% ë¯¸ë§Œì´ë©´ ì˜ì–´(ë˜ëŠ” ê¸°íƒ€)ë¡œ ê°„ì£¼


async def test_answer_generation_p0():
    print("ğŸš€ [P0] ë‹µë³€ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # 1. í™˜ê²½ ì¤€ë¹„
    try:
        llm = load_llm(OLLAMA_MODEL_NAME)
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    mock_docs = [
        Document(
            page_content="The capital of France is Paris.",
            metadata={"page": 1, "source": "geo.pdf"},
        ),
        Document(
            page_content="í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.",
            metadata={"page": 2, "source": "geo.pdf"},
        ),
    ]

    class MockRetriever:
        async def ainvoke(self, query):
            return mock_docs

        def invoke(self, query):
            return mock_docs

    app = build_graph(retriever=MockRetriever())
    config = {"configurable": {"llm": llm}}

    # 2. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
    test_cases = [
        {
            "name": "ì–¸ì–´ ì¼ê´€ì„± (í•œêµ­ì–´)",
            "input": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
            "expected_lang": "ko",
            "must_include": ["ì„œìš¸", "[p.2]"],
        },
        {
            "name": "ì–¸ì–´ ì¼ê´€ì„± (ì˜ì–´)",
            "input": "What is the capital of France?",
            "expected_lang": "en",
            "must_include": ["Paris", "[p.1]"],
        },
        {
            "name": "í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ (ì •ë³´ ì—†ìŒ)",
            "input": "ì¼ë³¸ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
            "check_groundedness": True,
        },
    ]

    for case in test_cases:
        print(f"\n--- í…ŒìŠ¤íŠ¸ í•­ëª©: {case['name']} ---")
        print(f"ì§ˆë¬¸: {case['input']}")

        full_response = ""
        events_count = 0
        metadata_received = False

        # 3. ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ê²€ì¦
        async for event in app.astream_events(
            {"input": case["input"]}, config=config, version="v2"
        ):
            kind = event["event"]

            # ë©”íƒ€ë°ì´í„° ì´ë²¤íŠ¸ í™•ì¸ (ì‚¬ìš©ì ì •ì˜ ì´ë²¤íŠ¸)
            if kind == "on_custom_event" and event["name"] == "metadata_update":
                metadata_received = True

            # ì±„íŒ… ëª¨ë¸ ìŠ¤íŠ¸ë¦¼ í™•ì¸
            elif kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    full_response += content
                    events_count += 1
                    if events_count % 10 == 0:
                        print(".", end="", flush=True)

        print(f"\nì‘ë‹µ ì™„ë£Œ ({events_count} chunks)")
        print("-" * 30)
        print(full_response)
        print("-" * 30)

        # 4. ê²€ì¦ ë¡œì§
        results = []

        # ì–¸ì–´ ê²€ì¦
        if "expected_lang" in case:
            lang_ok = await verify_language(full_response, case["expected_lang"])
            results.append(("ì–¸ì–´ ì¼ì¹˜", lang_ok))

        # í•„ìˆ˜ í¬í•¨ ë‹¨ì–´ ë° ì¸ìš© ê²€ì¦
        if "must_include" in case:
            include_ok = all(word in full_response for word in case["must_include"])
            results.append(("í•„ìˆ˜ ë‚´ìš© í¬í•¨", include_ok))

        # í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦
        if case.get("check_groundedness"):
            is_honest = "ë„ì¿„" not in full_response or any(
                x in full_response for x in ["ì •ë³´ê°€", "ì•Œ ìˆ˜", "ì œê³µëœ"]
            )
            results.append(("í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€", is_honest))

        # ìµœì¢… ë¦¬í¬íŠ¸
        all_passed = True
        for label, passed in results:
            status = "âœ… PASS" if passed else "âŒ FAIL"
            print(f" - {label}: {status}")
            if not passed:
                all_passed = False

        if not all_passed:
            print(f"âš ï¸ {case['name']} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")


if __name__ == "__main__":
    asyncio.run(test_answer_generation_p0())
