import asyncio
import os
import sys
import time
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

import numpy as np
import torch
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# ê¸°ì¡´ ë¡œì§ ì„í¬íŠ¸ (ìˆ˜ì •ëœ ë²„ì „)
from core.rag_core import _load_pdf_docs, _split_documents


async def benchmark_indexing():
    print("ğŸš€ [Benchmark] ì„ë² ë”© ì¬ì‚¬ìš© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # 1. í™˜ê²½ ì¤€ë¹„
    pdf_path = "tests/data/2201.07520v1.pdf"
    if not os.path.exists(pdf_path):
        print(f"âŒ í…ŒìŠ¤íŠ¸ìš© PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return

    model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    embedder = HuggingFaceEmbeddings(
        model_name=model_name, model_kwargs={"device": device}
    )

    # 2. ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
    print("ğŸ“„ ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹ ì¤‘...")
    docs = _load_pdf_docs(pdf_path, "benchmark.pdf")
    doc_splits = _split_documents(docs, embedder)
    texts = [d.page_content for d in doc_splits]
    print(f"ğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(doc_splits)}")

    # 3. ìµœì í™” ì „ ë°©ì‹ (ì„ë² ë”© 2íšŒ) ì¸¡ì •
    print("\n--- [Test 1] ì´ì „ ë°©ì‹ (ì´ì¤‘ ì„ë² ë”©) ì‹œë®¬ë ˆì´ì…˜ ---")
    start_time = time.time()

    # 1íšŒì°¨: ìµœì í™” ë¡œì§ìš© ì„ë² ë”©
    print("  Step 1: ìµœì í™”ìš© ì„ë² ë”© ìƒì„± ì¤‘...")
    embedder.embed_documents(texts)

    # 2íšŒì°¨: FAISS ìƒì„± (ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ ì„ë² ë”© ìˆ˜í–‰)
    print("  Step 2: FAISS ìƒì„± (ê°•ì œ ì¬ì„ë² ë”©) ì¤‘...")
    _ = FAISS.from_documents(doc_splits, embedder)

    old_method_time = time.time() - start_time
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {old_method_time:.2f}ì´ˆ")

    # 4. ìµœì í™” í›„ ë°©ì‹ (ì„ë² ë”© 1íšŒ + ì¬ì‚¬ìš©) ì¸¡ì •
    print("\n--- [Test 2] í˜„ì¬ ë°©ì‹ (ì„ë² ë”© ì¬ì‚¬ìš©) ì¸¡ì • ---")
    start_time = time.time()

    # 1íšŒì°¨: ì„ë² ë”© ìƒì„±
    print("  Step 1: ì„ë² ë”© ìƒì„± ì¤‘...")
    vectors_2 = embedder.embed_documents(texts)
    vectors_np = [np.array(v) for v in vectors_2]

    # 2íšŒì°¨: ë²¡í„° ì¬ì‚¬ìš©í•˜ì—¬ FAISS ìƒì„±
    print("  Step 2: ë²¡í„° ì¬ì‚¬ìš©í•˜ì—¬ FAISS ìƒì„± ì¤‘...")
    text_embeddings = zip(
        [d.page_content for d in doc_splits], vectors_np, strict=False
    )
    _ = FAISS.from_embeddings(
        text_embeddings, embedder, metadatas=[d.metadata for d in doc_splits]
    )

    new_method_time = time.time() - start_time
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {new_method_time:.2f}ì´ˆ")

    # 5. ê²°ê³¼ ë¹„êµ
    improvement = ((old_method_time - new_method_time) / old_method_time) * 100
    print("\n" + "=" * 40)
    print("ğŸ“ˆ ì„±ëŠ¥ ê°œì„  ê²°ê³¼")
    print(f"  - ì´ì „ ë°©ì‹: {old_method_time:.2f}ì´ˆ")
    print(f"  - í˜„ì¬ ë°©ì‹: {new_method_time:.2f}ì´ˆ")
    print(f"  - ì ˆê° ì‹œê°„: {old_method_time - new_method_time:.2f}ì´ˆ")
    print(f"  - ê°œì„ ìœ¨: {improvement:.1f}%")
    print("=" * 40)


if __name__ == "__main__":
    asyncio.run(benchmark_indexing())
