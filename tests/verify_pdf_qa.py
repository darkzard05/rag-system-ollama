import asyncio
import os
import sys
from pathlib import Path

# Add src directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from core.rag_core import RAGSystem
from core.model_loader import load_llm, load_embedding_model
from core.graph_builder import build_graph
from common.config import OLLAMA_MODEL_NAME, AVAILABLE_EMBEDDING_MODELS
from common.utils import apply_tooltips_to_response
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

async def test_until_success():
    print("ğŸš€ [ìµœì¢… ê²€ì¦] PDF ê¸°ë°˜ ë‹µë³€ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
    pdf_path = "tests/2201.07520v1.pdf"
    session_id = "final_verify_session"
    
    # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    embedding_model_name = AVAILABLE_EMBEDDING_MODELS[0]
    embedder = load_embedding_model(embedding_model_name)
    llm = load_llm(OLLAMA_MODEL_NAME)
    rag_system = RAGSystem(session_id=session_id)
    
    print(f"âš™ï¸ ë¬¸ì„œ ì¸ë±ì‹± ì¤‘: {pdf_path}")
    await asyncio.to_thread(rag_system.load_document, pdf_path, "test.pdf", embedder)
    
    # 2. ì§ˆë¬¸ ì„¤ì •
    question = "What are CM3-Medium and CM3-Large models?"
    print(f"ğŸ¤” ì§ˆë¬¸: {question}")
    
    # 3. ì„±ê³µí•  ë•Œê¹Œì§€ ìµœëŒ€ 3íšŒ ì‹œë„
    max_retries = 3
    full_response = ""
    retrieved_docs = []
    
    for attempt in range(1, max_retries + 1):
        print(f"\nğŸ”„ ì‹œë„ {attempt}/{max_retries}...")
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        retrieved_docs = await rag_system.ensemble_retriever.ainvoke(question)
        if not retrieved_docs:
            print("âŒ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            continue
            
        context_text = "\n".join([d.page_content[:300] for d in retrieved_docs[:3]])
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìµœëŒ€í•œ ë‹¨ìˆœí•˜ê²Œ)
        prompt = ChatPromptTemplate.from_template(
            "Use the context to answer the question briefly.\nContext: {context}\nQuestion: {question}"
        )
        chain = prompt | llm | StrOutputParser()
        
        try:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì • ê°•í™”
            response = await asyncio.wait_for(
                chain.ainvoke({"context": context_text, "question": question}),
                timeout=60
            )
            
            if response and len(response.strip()) > 20:
                full_response = response
                print(f"âœ… ë‹µë³€ ìƒì„± ì„±ê³µ! (ê¸¸ì´: {len(full_response)})")
                break
            else:
                print("âš ï¸ ëª¨ë¸ì´ ë¹ˆ ì‘ë‹µ ë˜ëŠ” ë„ˆë¬´ ì§§ì€ ë‹µë³€ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
        except asyncio.TimeoutError:
            print("âš ï¸ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
        await asyncio.sleep(2) # ì ì‹œ ëŒ€ê¸°

    if not full_response:
        print("\nâŒ ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì§ì ‘ ì§ˆë¬¸ìœ¼ë¡œ ë§ˆì§€ë§‰ ì‹œë„...")
        res = await llm.ainvoke(f"Based on CM3 paper, what is CM3 model? Answer in one sentence.")
        full_response = res.content

    # 4. ê²°ê³¼ ì¶œë ¥ ë° í¬ë§·íŒ… í™•ì¸
    final_content = apply_tooltips_to_response(full_response, retrieved_docs)
    
    print("\n" + "="*50)
    print("ğŸ“‹ [ìµœì¢… ë‹µë³€ ë‚´ìš©]")
    print(full_response)
    print("\nğŸ“‹ [í¬ë§·íŒ… ì ìš© ë‚´ìš© (ì¸ìš©êµ¬ í¬í•¨ ì—¬ë¶€ í™•ì¸)]")
    print(final_content[:500] + "...")
    print("="*50)
    
    if len(full_response) > 0:
        print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ: ì œëŒ€ë¡œ ëœ ë‹µë³€ì„ ìˆ˜ì‹ í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nFAIL: ë‹µë³€ì„ ìˆ˜ì‹ í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(test_until_success())
