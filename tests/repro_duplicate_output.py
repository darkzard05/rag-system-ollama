
import asyncio
import sys
import io
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent / "src"))

from core.graph_builder import build_graph
from core.model_loader import load_llm
from common.config import DEFAULT_OLLAMA_MODEL
from langchain_core.runnables import RunnableConfig

# Windows ì¸ì½”ë”© ëŒ€ì‘
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

async def repro_duplicate_output():
    print("ğŸ§ª [ì¤‘ë³µ ì¶œë ¥ ì¬í˜„ í…ŒìŠ¤íŠ¸] ì´ë²¤íŠ¸ ìˆ˜ì‹  ë¡œì§ ì •ë°€ ë¶„ì„")
    
    llm = load_llm(DEFAULT_OLLAMA_MODEL)
    # ê²€ìƒ‰ ì—†ì´ ë‹µë³€ë§Œ ìƒì„±í•˜ëŠ” ë¹ˆ ë¦¬íŠ¸ë¦¬ë²„ ê·¸ë˜í”„ ìƒì„±
    app = build_graph() 
    config = {"configurable": {"llm": llm}}

    question = "Hello, say 'Test' only."
    
    full_response = ""
    event_log = []

    print(f"ì§ˆë¬¸: {question}")
    print("ì´ë²¤íŠ¸ ìˆ˜ì‹  ì¤‘...")

    # ì‹¤ì œ src/ui/ui.py ì˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì¬í˜„
    async for event in app.astream_events({"input": question}, config=config, version="v2"):
        kind = event["event"]
        name = event.get("name", "Unknown")
        data = event.get("data", {})
        
        chunk_text = None
        if kind == "on_chat_model_stream":
            chunk = data.get("chunk")
            if hasattr(chunk, "content"): chunk_text = chunk.content
            elif isinstance(chunk, dict): chunk_text = chunk.get("content")
            if chunk_text:
                event_log.append(f"[ChatModel] {chunk_text}")
        
        elif kind == "on_parser_stream":
            chunk_text = data.get("chunk")
            if chunk_text:
                event_log.append(f"[Parser] {chunk_text}")
        
        if chunk_text:
            full_response += chunk_text

    print("\n--- ì´ë²¤íŠ¸ ìˆ˜ì‹  ë¡œê·¸ (ìƒìœ„ 10ê°œ) ---")
    for log in event_log[:10]:
        print(log)

    print("\n--- ìµœì¢… ì‘ë‹µ ê²°ê³¼ ---")
    print(f"ê²°ê³¼ê°’: '{full_response}'")

    # ê²€ì¦: ê°™ì€ ë‹¨ì–´ê°€ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸ (ì˜ˆ: 'TTee sst t')
    # ê°„ë‹¨í•˜ê²Œ ê¸€ì ìˆ˜ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë§ì€ì§€ ë˜ëŠ” ë™ì¼ íŒ¨í„´ ë°˜ë³µ í™•ì¸
    is_duplicate = any(event_log[i].split() == event_log[i+1].split() 
                       for i in range(len(event_log)-1) 
                       if "[ChatModel]" in event_log[i] and "[Parser]" in event_log[i+1])
    
    if is_duplicate:
        print("\nâŒ FAIL: ì¤‘ë³µ ì¶œë ¥ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤! ChatModelê³¼ Parser ì´ë²¤íŠ¸ë¥¼ ë™ì‹œì— ìˆ˜ì‹  ì¤‘ì…ë‹ˆë‹¤.")
    else:
        print("\nâœ… PASS: ì¤‘ë³µ ì¶œë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(repro_duplicate_output())
