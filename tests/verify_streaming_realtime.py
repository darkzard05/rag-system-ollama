import asyncio
import sys
import io
import time
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent / "src"))

from core.graph_builder import build_graph
from core.model_loader import load_llm
from common.config import OLLAMA_MODEL_NAME

# Windows ì¸ì½”ë”© ëŒ€ì‘
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

async def verify_streaming_realtime_final():
    print("ğŸ§ª [ìµœì¢… ì‹¤ì‹œê°„ì„± ê²€ì¦] ì»¤ìŠ¤í…€ ì´ë²¤íŠ¸ í†µë¡œë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì „ì†¡ í™•ì¸")
    
    llm = load_llm(OLLAMA_MODEL_NAME)
    app = build_graph() 
    config = {"configurable": {"llm": llm}}

    # ëª¨ë¸ì´ ì—¬ëŸ¬ í† í°ì„ ë‚´ë±‰ë„ë¡ ì§ˆë¬¸ ì„¤ì •
    question = "Count from 1 to 5 slowly."
    
    full_response = ""
    render_steps = []
    start_time = time.time()

    print(f"ì§ˆë¬¸: {question}")
    
    async for event in app.astream_events({"input": question}, config=config, version="v2"):
        kind = event["event"]
        name = event.get("name", "Unknown")
        data = event.get("data", {})
        
        chunk_text = None
        thought_text = None
        # [ì‹¤ì œ ì•± ìµœì‹  ë¡œì§] ì»¤ìŠ¤í…€ ì´ë²¤íŠ¸ë§Œ ìˆ˜ì‹ 
        if kind == "on_custom_event" and name == "response_chunk":
            chunk_text = data.get("chunk")
            thought_text = data.get("thought")
        
        if thought_text:
            elapsed = time.time() - start_time
            print(f"ğŸ§  ì‚¬ê³  ê³¼ì • ìˆ˜ì‹ : '{thought_text}'")
            render_steps.append(f"[{elapsed:.2f}s] THOUGHT: '{thought_text}'")

        if chunk_text:
            full_response += chunk_text
            elapsed = time.time() - start_time
            render_steps.append(f"[{elapsed:.2f}s] Chunk: '{chunk_text}' | Current: '{full_response}'")
            print(f"ğŸ“ ì‹¤ì‹œê°„ ì¡°ê° ìˆ˜ì‹ : '{chunk_text}'")

    print("\n--- ìŠ¤íŠ¸ë¦¬ë° ë Œë”ë§ íƒ€ì„ë¼ì¸ ---")
    for step in render_steps:
        print(step)

    print(f"\nìµœì¢… ê²°ê³¼: '{full_response}'")
    
    # ê²€ì¦: ì¡°ê°ì´ ì—¬ëŸ¬ ë²ˆì— ê±¸ì³ì„œ ì™”ëŠ”ê°€? (ì‹¤ì‹œê°„ì„±)
    is_realtime = len(render_steps) > 1
    # ì¤‘ë³µì´ ì—†ëŠ”ê°€? (ì˜ˆ: '11 22'ê°€ ì•„ë‹Œ '1 2')
    no_duplicate = "11" not in full_response and "22" not in full_response
    
    if is_realtime and no_duplicate:
        print("\nâœ… PASS: ìŠ¤íŠ¸ë¦¬ë°ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¤‘ë³µ ì—†ì´ ì™„ë²½í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤!")
    else:
        if not is_realtime: print("\nâŒ FAIL: ë‹µë³€ì´ í•œêº¼ë²ˆì— ì¶œë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if not no_duplicate: print("\nâŒ FAIL: ì—¬ì „íˆ ì¤‘ë³µ ì¶œë ¥ì´ ë°œìƒí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(verify_streaming_realtime_final())