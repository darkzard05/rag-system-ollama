question,ground_truth,reference_context
"CM3 모델은 어떤 종류의 모델을 기반으로 하고 있으며, 그 핵심 특징은 무엇인가요?","CM3는 구조화된 다중 모달 문서의 대규모 데이터 세트를 기반으로 훈련된 원인 마스킹 생성 모델입니다. 핵심 특징은 왼쪽에서 오른쪽으로 토큰을 생성하면서, 문자열 끝부분에 생성되는 소수의 긴 토큰 스피언을 원래 위치에서 마스킹하지 않고, 그 위치를 제외한 방식으로 처리하는 것입니다.","CM3: A CAUSAL MASKED MULTIMODAL MODEL OF
THE INTERNET
Armen Aghajanyan, Bernie Huang∗, Candace Ross∗, Vlad Karpukhin∗, Hu Xu∗, Naman Goyal,
Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer
Facebook AI Research
{armenag,berniehuang,ccross,vladk,huxu,naman}@fb.com
{oxo,mandarj,gghosh,mikelewis,lsz}@fb.com
ABSTRACT
We introduce CM3, a family of causally masked generative models trained over a
large corpus of structured multi-modal documents that can contain both text and
image tokens. Our new causally masked approach generates tokens left to right
while also masking out a small number of long token spans that are generated at
the end of the string, instead of their original positions. The casual masking ob-
ject provides a type of hybrid of the more common causal and masked language
models, by enabling full generative modeling while also providing bidirectional
context when generating the masked spans. We train causally masked language-
image models on large-scale web and Wikipedia articles, where each document
contains all of the text, hypertext markup, hyperlinks, and image tokens (from a
VQVAE-GAN), provided in the order they appear in the original HTML source
(before masking). The resulting CM3 models can generate rich structured, multi-
modal outputs while conditioning on arbitrary masked document contexts, and
thereby implicitly learn a wide range of text, image, and cross modal tasks. They
can be prompted to recover, in a zero-shot fashion, the functionality of models
such as DALL-E, GENRE, and HTLM (Ramesh et al., 2021; De Cao et al., 2020;
Aghajanyan et al., 2021). We set the new state-of-the-art in zero-shot summa-
rization, entity linking, and entity disambiguation while maintaining competitive
performance in the ﬁne-tuning setting. We can generate images unconditionally,
conditioned on text (like DALL-E) and do captioning all in a zero-shot setting
with a single model.
1
INTRODUCTION
Recent advancements in large-scale generative sequence modeling have signiﬁcantly improved zero-
shot performance on multiple modalities, including text Brown et al. (2020); Fabbri et al. (2020);
Aghajanyan et al. (2021) and images Ramesh et al. (2021). Recent work has also shown how to
use document structure, e.g., as provided by HTML web markup, to enable more effective zero-shot
prompting for text-only tasks (Aghajanyan et al., 2021). In this paper, we show it is possible to learn
multi-modal document-structured generative models, to jointly represent formatted hypertext and
images as they naturally co-occur within full document contexts.
We introduce CM3, a family of causally masked generative models trained over a large corpus of
structured multi-modal documents. Causally masked models generate tokens left to right, just like
a causal language model, but also mask out a small number of long token spans, which are then
generated at the end of the string instead of their original positions. This provides a new hybrid of
causal and masked language models, enabling full generative modeling with bidirectional context.
For example, it can also be used in our setting to inﬁll complete images or larger structured text
sections, conditioned on the rest of the document.
We train CM3 models on close to a terabyte of web-based data following Aghajanyan et al. (2021),
extended to include images through VQVAE-GAN tokens (Esser et al., 2021) and additional hyper-
text link structure. This data is in strong contrast to previous methods that were either uni-modal or
∗Equal Contribution for Second Author
1
arXiv:2201.07520v1  [cs.CL]  19 Jan 2022"
"CM3 모델이 훈련된 데이터 세트는 어떤 내용을 포함하며, 그 데이터의 순서는 어떻게 되나요?","CM3 모델은 대규모 웹 및 위키백과 기사에서 훈련되며, 각 문서는 원래 HTML 소스에서 나타나는 순서대로 텍스트, 하이퍼텍스트 마크업, 하이퍼링크, 그리고 이미지 토큰(비디오-변형-GAN 기반)을 모두 포함합니다.","CM3: A CAUSAL MASKED MULTIMODAL MODEL OF
THE INTERNET
Armen Aghajanyan, Bernie Huang∗, Candace Ross∗, Vlad Karpukhin∗, Hu Xu∗, Naman Goyal,
Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer
Facebook AI Research
{armenag,berniehuang,ccross,vladk,huxu,naman}@fb.com
{oxo,mandarj,gghosh,mikelewis,lsz}@fb.com
ABSTRACT
We introduce CM3, a family of causally masked generative models trained over a
large corpus of structured multi-modal documents that can contain both text and
image tokens. Our new causally masked approach generates tokens left to right
while also masking out a small number of long token spans that are generated at
the end of the string, instead of their original positions. The casual masking ob-
ject provides a type of hybrid of the more common causal and masked language
models, by enabling full generative modeling while also providing bidirectional
context when generating the masked spans. We train causally masked language-
image models on large-scale web and Wikipedia articles, where each document
contains all of the text, hypertext markup, hyperlinks, and image tokens (from a
VQVAE-GAN), provided in the order they appear in the original HTML source
(before masking). The resulting CM3 models can generate rich structured, multi-
modal outputs while conditioning on arbitrary masked document contexts, and
thereby implicitly learn a wide range of text, image, and cross modal tasks. They
can be prompted to recover, in a zero-shot fashion, the functionality of models
such as DALL-E, GENRE, and HTLM (Ramesh et al., 2021; De Cao et al., 2020;
Aghajanyan et al., 2021). We set the new state-of-the-art in zero-shot summa-
rization, entity linking, and entity disambiguation while maintaining competitive
performance in the ﬁne-tuning setting. We can generate images unconditionally,
conditioned on text (like DALL-E) and do captioning all in a zero-shot setting
with a single model.
1
INTRODUCTION
Recent advancements in large-scale generative sequence modeling have signiﬁcantly improved zero-
shot performance on multiple modalities, including text Brown et al. (2020); Fabbri et al. (2020);
Aghajanyan et al. (2021) and images Ramesh et al. (2021). Recent work has also shown how to
use document structure, e.g., as provided by HTML web markup, to enable more effective zero-shot
prompting for text-only tasks (Aghajanyan et al., 2021). In this paper, we show it is possible to learn
multi-modal document-structured generative models, to jointly represent formatted hypertext and
images as they naturally co-occur within full document contexts.
We introduce CM3, a family of causally masked generative models trained over a large corpus of
structured multi-modal documents. Causally masked models generate tokens left to right, just like
a causal language model, but also mask out a small number of long token spans, which are then
generated at the end of the string instead of their original positions. This provides a new hybrid of
causal and masked language models, enabling full generative modeling with bidirectional context.
For example, it can also be used in our setting to inﬁll complete images or larger structured text
sections, conditioned on the rest of the document.
We train CM3 models on close to a terabyte of web-based data following Aghajanyan et al. (2021),
extended to include images through VQVAE-GAN tokens (Esser et al., 2021) and additional hyper-
text link structure. This data is in strong contrast to previous methods that were either uni-modal or
∗Equal Contribution for Second Author
1
arXiv:2201.07520v1  [cs.CL]  19 Jan 2022"
CM3 모델은 어떤 데이터를 기반으로 훈련되었나요?,CM3 모델은 공통 크롤링에서 수집한 거의 테라바이트의 다중 모달 단순화된 HTML 데이터를 기반으로 훈련되었습니다.,"carefully curated the image-text alignment (e.g., for image captioning Radford et al. (2021); Ramesh
et al. (2021)). We train a 2.7 billion and 13 billion causally masked model on this data which we
call CM3-Medium and CM3-Large respectively.
Extensive experiments demonstrate that these models are able to perform a wide range of zero-shot
uni- and cross-modal tasks. We show both qualitatively and quantitatively that CM3 can be prompted
for non-trivial image generation, similar to that of DALL-E. We also show that CM3 models are ca-
pable of improving over state-of-the-art zero-shot summarization, entity linking, entity disambigua-
tion, highlighting the structure that comes from the hypertext during training. Finally, we show that
by ﬁne-tuning CM3 we set the new state-of-the-art for entity linking and entity disambiguation in
general.
To summarize, our contributions include:
• We present the ﬁrst hyper-text language-image model, trained on close to a Terabyte of
multi-modal simpliﬁed HTML data from the common crawl.
• We present the causally masked objective, a hybrid of causal and masked language models
that allows for bidirectional context control during generative mask inﬁlling.
• We demonstrate consistently strong transfer from CM3 to a range of uni-modal and multi-
modal tasks at differing supervision levels, including stating state-of-the-art on entity dis-
ambiguation and zero-shot summarization.
• We release all code and models to support future CM3 research.
2
CAUSALLY MASKED OBJECTIVE
Traditional approaches to pre-training have focused on mixing the architectural choices (i.e.,
encoder-only, decoder-only, encoder-decoder) with objective choices (i.e., masking, causal language
modeling). For example, masked encoder-only models such as BERT (Devlin et al., 2018) and
RoBERTa (Liu et al., 2019) excel in non-generative ﬁne-tuning tasks. Masked encoder-decoder
models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2019) excel in both discriminative
and generative ﬁne-tuning. Brown et al. (2020) on the other hand, showed that causal language mod-
els (de-facto, decoder only) are capable of non-trivial performance without the need of ﬁne-tuning
by simply prompting with appropriate string to control the generated outputs Radford et al. (2019);
Brown et al. (2020); Artetxe et al. (2021).
There are pros and cons to both masked and causal language modeling in the context of prompting.
Masking offers the critical ability to encode bi-directionality within the prompts at the cost of only
decoding roughly 15% of the tokens of the input sequence during training (Devlin et al., 2018;
Liu et al., 2019; Lewis et al., 2019). Conversely, decoder-only causal language models decode every
token in the input sequence in the training but are typically limited to left-only contexts. Empirically,
more work has also been done on scaling causal decoder-only rather than their counterparts.
In an effort to get most of the best of both worlds, we introduce a novel objective that combines
the beneﬁt of per-token generation with optional bi-directionality speciﬁcally tailored to prompting.
For a document of size s we select n ∼Clamp(Poisson(1), 1, 16) masks and for each of those
masks we select span m ∼(Uniform(0, s), Uniform(0, s)) which does not intersect with any
other m. These values are chosen to, on average, select relatively few relatively long spans, which
we expect will allow the model to learn to inﬁll long spans. We then order these masks by the order
that they appear in the source document, replace the span of the mask in the source document with
an enumerated mask token (i.e., <mask:0>, <mask:1>), and move the masked spans to the end
of the document followed by a unique end of document token.
Figure 1 shows the complete process.
We also augment the standard cross-entropy loss to weigh the loss of predicting mask tokens as
0, as they are placed at random locations which carry no information to the underlying sequence
modeling objective.
The complete array of beneﬁts will become more apparent when designing prompts for uni/cross-
modal tasks in § 4. However, at the core, the causally masked objective can do causal language
modeling while optionally allowing for bidirectionality when needed.
2"
CM3 모델의 훈련에 사용된 목적 함수는 무엇인가요?,"CM3 모델의 훈련에 사용된 목적 함수는 인과적 마스킹(causeally masked objective)으로, 인과적 언어 모델과 마스킹 언어 모델의 하이브리드입니다. 이는 생성 마스킹 채우기 동안 양방향 컨텍스트 제어를 가능하게 합니다.","carefully curated the image-text alignment (e.g., for image captioning Radford et al. (2021); Ramesh
et al. (2021)). We train a 2.7 billion and 13 billion causally masked model on this data which we
call CM3-Medium and CM3-Large respectively.
Extensive experiments demonstrate that these models are able to perform a wide range of zero-shot
uni- and cross-modal tasks. We show both qualitatively and quantitatively that CM3 can be prompted
for non-trivial image generation, similar to that of DALL-E. We also show that CM3 models are ca-
pable of improving over state-of-the-art zero-shot summarization, entity linking, entity disambigua-
tion, highlighting the structure that comes from the hypertext during training. Finally, we show that
by ﬁne-tuning CM3 we set the new state-of-the-art for entity linking and entity disambiguation in
general.
To summarize, our contributions include:
• We present the ﬁrst hyper-text language-image model, trained on close to a Terabyte of
multi-modal simpliﬁed HTML data from the common crawl.
• We present the causally masked objective, a hybrid of causal and masked language models
that allows for bidirectional context control during generative mask inﬁlling.
• We demonstrate consistently strong transfer from CM3 to a range of uni-modal and multi-
modal tasks at differing supervision levels, including stating state-of-the-art on entity dis-
ambiguation and zero-shot summarization.
• We release all code and models to support future CM3 research.
2
CAUSALLY MASKED OBJECTIVE
Traditional approaches to pre-training have focused on mixing the architectural choices (i.e.,
encoder-only, decoder-only, encoder-decoder) with objective choices (i.e., masking, causal language
modeling). For example, masked encoder-only models such as BERT (Devlin et al., 2018) and
RoBERTa (Liu et al., 2019) excel in non-generative ﬁne-tuning tasks. Masked encoder-decoder
models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2019) excel in both discriminative
and generative ﬁne-tuning. Brown et al. (2020) on the other hand, showed that causal language mod-
els (de-facto, decoder only) are capable of non-trivial performance without the need of ﬁne-tuning
by simply prompting with appropriate string to control the generated outputs Radford et al. (2019);
Brown et al. (2020); Artetxe et al. (2021).
There are pros and cons to both masked and causal language modeling in the context of prompting.
Masking offers the critical ability to encode bi-directionality within the prompts at the cost of only
decoding roughly 15% of the tokens of the input sequence during training (Devlin et al., 2018;
Liu et al., 2019; Lewis et al., 2019). Conversely, decoder-only causal language models decode every
token in the input sequence in the training but are typically limited to left-only contexts. Empirically,
more work has also been done on scaling causal decoder-only rather than their counterparts.
In an effort to get most of the best of both worlds, we introduce a novel objective that combines
the beneﬁt of per-token generation with optional bi-directionality speciﬁcally tailored to prompting.
For a document of size s we select n ∼Clamp(Poisson(1), 1, 16) masks and for each of those
masks we select span m ∼(Uniform(0, s), Uniform(0, s)) which does not intersect with any
other m. These values are chosen to, on average, select relatively few relatively long spans, which
we expect will allow the model to learn to inﬁll long spans. We then order these masks by the order
that they appear in the source document, replace the span of the mask in the source document with
an enumerated mask token (i.e., <mask:0>, <mask:1>), and move the masked spans to the end
of the document followed by a unique end of document token.
Figure 1 shows the complete process.
We also augment the standard cross-entropy loss to weigh the loss of predicting mask tokens as
0, as they are placed at random locations which carry no information to the underlying sequence
modeling objective.
The complete array of beneﬁts will become more apparent when designing prompts for uni/cross-
modal tasks in § 4. However, at the core, the causally masked objective can do causal language
modeling while optionally allowing for bidirectionality when needed.
2"
Monte Melkonian의 정치적 성향과 정체성은 무엇인가요?,"Monte Melkonian는 좌파이며, 민족주의적 군사적 인물이었습니다.","Monte
Melkonian
was
left-wing
a
nationalist
militant
.
Masked
Language
Model
<a
href=
Armenian
_nationalism
>
<mask>
Monte
Melkonian
was
<a
left-wing
a
href=
Armenian
_nationalism
>
nationalist
militant
.
Language
Model
Monte
Melkonian
was
left-wing
a
nationalist
militant
.
Causally
Masked
Language
Model
<mask:0>
<mask:0>
<a
href=
Armenian
_nationalism
>
Figure 1: A visual representation of various language modeling objectives as well as our proposed
causal language modeling objective with a single mask (n = 1). Given the left-to-right nature of
causal language models (bottom row) we would not be able to generate the Wikipedia entity link
highlighted in orange.
3
CM3
Aghajanyan et al. (2021) used structured documents for text-only pre-training with strong zero-shot
performance. Causally-Masked Multimodal Modeling (CM3) extends this work by modeling full
document structure including images and hypertext links. Furthermore, we move away from the
BART-like objective of Aghajanyan et al. (2021) to use our new causally masked objective with
decoder-only models.
3.1
DATA
Following Aghajanyan et al. (2021) we aim to implement a transform over HTML documents to
extract out to minimal-HTML, i.e., the minimal set of text that is semantically relevant for end tasks.
Birhane et al. (2021) gave in-depth criticisms of Common Crawl based multi-modal datasets and
showed the existence of highly problematic examples (i.e., explicit images and text pairs of rape,
pornography, and ethnic slurs). Given these severe ethical concerns, we opt-out of processing all
of Common Crawl and instead opt into using a subset of the Common Crawl News (CC-NEWS)
dataset and all of English Wikipedia.
Given a valid HTML DOM1 per document, we run several passes to strip down the DOM to the
elements of maximal semantic value. We ﬁrst remove all elements which do not contain textual ele-
ments. We also ﬁlter out all headers, footers, copyrights, forms, dialog boxes and iFrames. We fold
consecutive <div> elements into a singular <div> element with merged attributes. Furthermore
we strip all the attributes from every element which are not derived from structured graphs such as
OpenGraph, Schema and Twitter.
For every <img> tag in the document with a valid src attribute URL, we download the image,
resize to 256x256 pixels with random cropping and then tokenize it with VQVAE-GAN from Esser
et al. (2021). This amounts to 256 tokens for every image. We then insert the string value of the
tokens joined with a space back into the src attribute.
We do not place any restrictions on the number of images or their locations. We present a set of
high-level statistics in Table 1.
Documents (Million)
Size (GB)
Unique Images (Million)
Tokens (Billion)
CC-NEWS
45
460
18
121
En-Wikipedia
16
383
7
102
Total
61
843
25
223
Table 1: High level statistics of the data used to train CM3.
1The DOM or Document Object Model is an interface that treats an HTML document as a tree structure
wherein each node is an object representing a part of the document.
3"
CM3 모델이 Aghajanyan et al. (2021)의 작업에서 어떻게 발전했나요?,"CM3는 Aghajının et al. (2021)이 텍스트만을 대상으로 한 구조화된 문서를 사용한 작업을 확장하여, 이미지와 하이퍼텍스트 링크를 포함한 전체 문서 구조를 모델링합니다. 또한 BART와 유사한 목적을 벗어나, 디코더만 모델을 사용하는 새로운 원인적으로 마스킹된 목적을 도입합니다.","Monte
Melkonian
was
left-wing
a
nationalist
militant
.
Masked
Language
Model
<a
href=
Armenian
_nationalism
>
<mask>
Monte
Melkonian
was
<a
left-wing
a
href=
Armenian
_nationalism
>
nationalist
militant
.
Language
Model
Monte
Melkonian
was
left-wing
a
nationalist
militant
.
Causally
Masked
Language
Model
<mask:0>
<mask:0>
<a
href=
Armenian
_nationalism
>
Figure 1: A visual representation of various language modeling objectives as well as our proposed
causal language modeling objective with a single mask (n = 1). Given the left-to-right nature of
causal language models (bottom row) we would not be able to generate the Wikipedia entity link
highlighted in orange.
3
CM3
Aghajanyan et al. (2021) used structured documents for text-only pre-training with strong zero-shot
performance. Causally-Masked Multimodal Modeling (CM3) extends this work by modeling full
document structure including images and hypertext links. Furthermore, we move away from the
BART-like objective of Aghajanyan et al. (2021) to use our new causally masked objective with
decoder-only models.
3.1
DATA
Following Aghajanyan et al. (2021) we aim to implement a transform over HTML documents to
extract out to minimal-HTML, i.e., the minimal set of text that is semantically relevant for end tasks.
Birhane et al. (2021) gave in-depth criticisms of Common Crawl based multi-modal datasets and
showed the existence of highly problematic examples (i.e., explicit images and text pairs of rape,
pornography, and ethnic slurs). Given these severe ethical concerns, we opt-out of processing all
of Common Crawl and instead opt into using a subset of the Common Crawl News (CC-NEWS)
dataset and all of English Wikipedia.
Given a valid HTML DOM1 per document, we run several passes to strip down the DOM to the
elements of maximal semantic value. We ﬁrst remove all elements which do not contain textual ele-
ments. We also ﬁlter out all headers, footers, copyrights, forms, dialog boxes and iFrames. We fold
consecutive <div> elements into a singular <div> element with merged attributes. Furthermore
we strip all the attributes from every element which are not derived from structured graphs such as
OpenGraph, Schema and Twitter.
For every <img> tag in the document with a valid src attribute URL, we download the image,
resize to 256x256 pixels with random cropping and then tokenize it with VQVAE-GAN from Esser
et al. (2021). This amounts to 256 tokens for every image. We then insert the string value of the
tokens joined with a space back into the src attribute.
We do not place any restrictions on the number of images or their locations. We present a set of
high-level statistics in Table 1.
Documents (Million)
Size (GB)
Unique Images (Million)
Tokens (Billion)
CC-NEWS
45
460
18
121
En-Wikipedia
16
383
7
102
Total
61
843
25
223
Table 1: High level statistics of the data used to train CM3.
1The DOM or Document Object Model is an interface that treats an HTML document as a tree structure
wherein each node is an object representing a part of the document.
3"
실험을 위해 각 데이터 소스에서 테스트 세트를 어떻게 생성했나요?,"실험을 위해 각 데이터 소스에서 각각 10,000개의 고유 문서를 가진 두 개의 테스트 세트를 생성했습니다.","For experimentation, we create two test sets from each data source with 10,000 unique documents
for each. We de-duplicated our test sets to ensure no overlap between test and train sets to the best
of our abilities.
3.2
SIZE HINTS
Aghajanyan et al. (2021) introduced the concept of size hints which allows the user to guide the
model during sample generation through token conditioning. Speciﬁcally, HTLM inserts a prob-
abilistic estimate of the size of the mask as a token post the mask token (e.g., <mask>12 for a
probabilistic size of 12). For CM3, we noticed that size-hints degraded not only end-perplexity but
also the zero-shot performance on a signiﬁcant set of evaluation tests.
We also note that we can implicitly give a size hint during mask generation for a single mask by
asking the model to generate causally max sequence length - size hint tokens before
placing the secondary <mask:0> token.
3.3
TRAINING
We train 4 models; 125M, 800M, 2.7B, and 13B parameters. The purpose of the two smaller models
was to establish basic hyper-parameters that are viable for the causally masked language modeling
objective and therefore were under-trained. However, all downstream tasks will be evaluated with
our 2.7B model (CM3-Medium) and our 13B model (CM3-Large). HTLM-Medium was trained on
240 V100 GPU for 28 days, while HTLM-Large was trained on 384 A100 GPU for 24 days. Our
implementation was in PyTorch (Paszke et al., 2019) using fairseq (Ott et al., 2019) and fairscale
(Baines et al., 2021). For every model, our per GPU batch size was 8, with a maximum token
sequence length of 2048. We use the polynomial decay learning rate scheduler available in Paszke
et al. (2019) with 1500 warmup updates. We clipped the gradient norms to 1.0 and used the Adam
optimizer with β1 = 0.9, β2 = 0.98 (Kingma & Ba, 2014). We defer our model architecture
description to § A.1.
3.4
SCALING LAWS
Our training setting has a couple of new parameters that can impact the traditional scaling laws
of causal language models. The multi-modal nature of our proposed model breaks the standard
assumptions of token distributionality. Traditionally language tokens are said to follow a Zipﬁan
distribution (Piantadosi, 2014), while image tokens are strictly uniform (see § A.2). Furthermore, the
unrestricted locations of the images and text introduce unpredictable complexity. Lastly, although
we are still computing the joint probability of the document, we do so in a roundabout way through
shufﬂing of the document via the causally masked objective. These fundamental differences warrant
a quick look into the scaling laws of CM3.
0
1
2
3
4
Number of Documents
1e8
100
Validation Perplexity
0
50000 100000 150000 200000
Number of Updates
100
0
1
2
3
Training Time (Seconds)
1e6
100
Model
CM3-XSmall
CM3-Small
CM3-Medium
CM3-Large
Perplexity Based Scaling Laws for CM3
Figure 2: Basic perplexity based scaling laws for the proposed CM3 objective and training set-up.
We present the various perplexity curves for the four models of varying sizes we trained. Given that
our models were trained on various hardware set-ups, we normalize the training time by linearly
scaling each experiment timing to 256 GPU. Most importantly, we see healthy scaling, similar to
4"
"사이즈 힌트란 무엇이며, CM3에서 이 힌트의 사용이 어떤 영향을 미쳤나요?",사이즈 힌트는 사용자가 샘플 생성 중 토큰 조건을 통해 모델에 크기를 지시하는 개념입니다. HTLM에서는 마스크 토큰 이후에 마스크 크기의 확률적 추정치를 토큰으로 삽입합니다. CM3에서는 사이즈 힌트가 종결 퍼플렉시티뿐만 아니라 많은 평가 테스트에서 제로샷 성능을 저하시켰습니다.,"For experimentation, we create two test sets from each data source with 10,000 unique documents
for each. We de-duplicated our test sets to ensure no overlap between test and train sets to the best
of our abilities.
3.2
SIZE HINTS
Aghajanyan et al. (2021) introduced the concept of size hints which allows the user to guide the
model during sample generation through token conditioning. Speciﬁcally, HTLM inserts a prob-
abilistic estimate of the size of the mask as a token post the mask token (e.g., <mask>12 for a
probabilistic size of 12). For CM3, we noticed that size-hints degraded not only end-perplexity but
also the zero-shot performance on a signiﬁcant set of evaluation tests.
We also note that we can implicitly give a size hint during mask generation for a single mask by
asking the model to generate causally max sequence length - size hint tokens before
placing the secondary <mask:0> token.
3.3
TRAINING
We train 4 models; 125M, 800M, 2.7B, and 13B parameters. The purpose of the two smaller models
was to establish basic hyper-parameters that are viable for the causally masked language modeling
objective and therefore were under-trained. However, all downstream tasks will be evaluated with
our 2.7B model (CM3-Medium) and our 13B model (CM3-Large). HTLM-Medium was trained on
240 V100 GPU for 28 days, while HTLM-Large was trained on 384 A100 GPU for 24 days. Our
implementation was in PyTorch (Paszke et al., 2019) using fairseq (Ott et al., 2019) and fairscale
(Baines et al., 2021). For every model, our per GPU batch size was 8, with a maximum token
sequence length of 2048. We use the polynomial decay learning rate scheduler available in Paszke
et al. (2019) with 1500 warmup updates. We clipped the gradient norms to 1.0 and used the Adam
optimizer with β1 = 0.9, β2 = 0.98 (Kingma & Ba, 2014). We defer our model architecture
description to § A.1.
3.4
SCALING LAWS
Our training setting has a couple of new parameters that can impact the traditional scaling laws
of causal language models. The multi-modal nature of our proposed model breaks the standard
assumptions of token distributionality. Traditionally language tokens are said to follow a Zipﬁan
distribution (Piantadosi, 2014), while image tokens are strictly uniform (see § A.2). Furthermore, the
unrestricted locations of the images and text introduce unpredictable complexity. Lastly, although
we are still computing the joint probability of the document, we do so in a roundabout way through
shufﬂing of the document via the causally masked objective. These fundamental differences warrant
a quick look into the scaling laws of CM3.
0
1
2
3
4
Number of Documents
1e8
100
Validation Perplexity
0
50000 100000 150000 200000
Number of Updates
100
0
1
2
3
Training Time (Seconds)
1e6
100
Model
CM3-XSmall
CM3-Small
CM3-Medium
CM3-Large
Perplexity Based Scaling Laws for CM3
Figure 2: Basic perplexity based scaling laws for the proposed CM3 objective and training set-up.
We present the various perplexity curves for the four models of varying sizes we trained. Given that
our models were trained on various hardware set-ups, we normalize the training time by linearly
scaling each experiment timing to 256 GPU. Most importantly, we see healthy scaling, similar to
4"
모델 훈련에 사용된 모델의 크기는 무엇인가요?,"모델 훈련에 125M, 800M, 2.7B, 그리고 13B 파라미터를 가진 4개의 모델을 사용했습니다.","For experimentation, we create two test sets from each data source with 10,000 unique documents
for each. We de-duplicated our test sets to ensure no overlap between test and train sets to the best
of our abilities.
3.2
SIZE HINTS
Aghajanyan et al. (2021) introduced the concept of size hints which allows the user to guide the
model during sample generation through token conditioning. Speciﬁcally, HTLM inserts a prob-
abilistic estimate of the size of the mask as a token post the mask token (e.g., <mask>12 for a
probabilistic size of 12). For CM3, we noticed that size-hints degraded not only end-perplexity but
also the zero-shot performance on a signiﬁcant set of evaluation tests.
We also note that we can implicitly give a size hint during mask generation for a single mask by
asking the model to generate causally max sequence length - size hint tokens before
placing the secondary <mask:0> token.
3.3
TRAINING
We train 4 models; 125M, 800M, 2.7B, and 13B parameters. The purpose of the two smaller models
was to establish basic hyper-parameters that are viable for the causally masked language modeling
objective and therefore were under-trained. However, all downstream tasks will be evaluated with
our 2.7B model (CM3-Medium) and our 13B model (CM3-Large). HTLM-Medium was trained on
240 V100 GPU for 28 days, while HTLM-Large was trained on 384 A100 GPU for 24 days. Our
implementation was in PyTorch (Paszke et al., 2019) using fairseq (Ott et al., 2019) and fairscale
(Baines et al., 2021). For every model, our per GPU batch size was 8, with a maximum token
sequence length of 2048. We use the polynomial decay learning rate scheduler available in Paszke
et al. (2019) with 1500 warmup updates. We clipped the gradient norms to 1.0 and used the Adam
optimizer with β1 = 0.9, β2 = 0.98 (Kingma & Ba, 2014). We defer our model architecture
description to § A.1.
3.4
SCALING LAWS
Our training setting has a couple of new parameters that can impact the traditional scaling laws
of causal language models. The multi-modal nature of our proposed model breaks the standard
assumptions of token distributionality. Traditionally language tokens are said to follow a Zipﬁan
distribution (Piantadosi, 2014), while image tokens are strictly uniform (see § A.2). Furthermore, the
unrestricted locations of the images and text introduce unpredictable complexity. Lastly, although
we are still computing the joint probability of the document, we do so in a roundabout way through
shufﬂing of the document via the causally masked objective. These fundamental differences warrant
a quick look into the scaling laws of CM3.
0
1
2
3
4
Number of Documents
1e8
100
Validation Perplexity
0
50000 100000 150000 200000
Number of Updates
100
0
1
2
3
Training Time (Seconds)
1e6
100
Model
CM3-XSmall
CM3-Small
CM3-Medium
CM3-Large
Perplexity Based Scaling Laws for CM3
Figure 2: Basic perplexity based scaling laws for the proposed CM3 objective and training set-up.
We present the various perplexity curves for the four models of varying sizes we trained. Given that
our models were trained on various hardware set-ups, we normalize the training time by linearly
scaling each experiment timing to 256 GPU. Most importantly, we see healthy scaling, similar to
4"
모델 훈련에 사용된 하이퍼파라미터와 최적화 방법은 무엇인가요?,"모든 모델에 대해 GPU당 배치 크기는 8이며, 최대 토큰 시퀀스 길이는 2048입니다. 학습률은 패스케의 다항 감소 스케줄러를 사용하며, 1500번의 워밍업 후 적용됩니다. 기울기 노름을 1.0으로 클리핑하고, Adam 최적화기(β1=0.9, β2=0.98)를 사용했습니다.","For experimentation, we create two test sets from each data source with 10,000 unique documents
for each. We de-duplicated our test sets to ensure no overlap between test and train sets to the best
of our abilities.
3.2
SIZE HINTS
Aghajanyan et al. (2021) introduced the concept of size hints which allows the user to guide the
model during sample generation through token conditioning. Speciﬁcally, HTLM inserts a prob-
abilistic estimate of the size of the mask as a token post the mask token (e.g., <mask>12 for a
probabilistic size of 12). For CM3, we noticed that size-hints degraded not only end-perplexity but
also the zero-shot performance on a signiﬁcant set of evaluation tests.
We also note that we can implicitly give a size hint during mask generation for a single mask by
asking the model to generate causally max sequence length - size hint tokens before
placing the secondary <mask:0> token.
3.3
TRAINING
We train 4 models; 125M, 800M, 2.7B, and 13B parameters. The purpose of the two smaller models
was to establish basic hyper-parameters that are viable for the causally masked language modeling
objective and therefore were under-trained. However, all downstream tasks will be evaluated with
our 2.7B model (CM3-Medium) and our 13B model (CM3-Large). HTLM-Medium was trained on
240 V100 GPU for 28 days, while HTLM-Large was trained on 384 A100 GPU for 24 days. Our
implementation was in PyTorch (Paszke et al., 2019) using fairseq (Ott et al., 2019) and fairscale
(Baines et al., 2021). For every model, our per GPU batch size was 8, with a maximum token
sequence length of 2048. We use the polynomial decay learning rate scheduler available in Paszke
et al. (2019) with 1500 warmup updates. We clipped the gradient norms to 1.0 and used the Adam
optimizer with β1 = 0.9, β2 = 0.98 (Kingma & Ba, 2014). We defer our model architecture
description to § A.1.
3.4
SCALING LAWS
Our training setting has a couple of new parameters that can impact the traditional scaling laws
of causal language models. The multi-modal nature of our proposed model breaks the standard
assumptions of token distributionality. Traditionally language tokens are said to follow a Zipﬁan
distribution (Piantadosi, 2014), while image tokens are strictly uniform (see § A.2). Furthermore, the
unrestricted locations of the images and text introduce unpredictable complexity. Lastly, although
we are still computing the joint probability of the document, we do so in a roundabout way through
shufﬂing of the document via the causally masked objective. These fundamental differences warrant
a quick look into the scaling laws of CM3.
0
1
2
3
4
Number of Documents
1e8
100
Validation Perplexity
0
50000 100000 150000 200000
Number of Updates
100
0
1
2
3
Training Time (Seconds)
1e6
100
Model
CM3-XSmall
CM3-Small
CM3-Medium
CM3-Large
Perplexity Based Scaling Laws for CM3
Figure 2: Basic perplexity based scaling laws for the proposed CM3 objective and training set-up.
We present the various perplexity curves for the four models of varying sizes we trained. Given that
our models were trained on various hardware set-ups, we normalize the training time by linearly
scaling each experiment timing to 256 GPU. Most importantly, we see healthy scaling, similar to
4"
