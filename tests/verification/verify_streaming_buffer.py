import asyncio
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

from langchain_core.runnables import RunnableConfig

from common.config import OLLAMA_MODEL_NAME
from core.graph_builder import build_graph
from core.model_loader import load_llm


async def test_streaming_buffer():
    print(f"ğŸš€ [Test] ìŠ¤íŠ¸ë¦¬ë° ë²„í¼ë§ ê²€ì¦ ì‹œì‘ (ëª¨ë¸: {OLLAMA_MODEL_NAME})")

    # 1. ê·¸ë˜í”„ ì¤€ë¹„ (ë¦¬íŠ¸ë¦¬ë²„ ì—†ì´ ìµœì†Œ ê¸°ëŠ¥ìœ¼ë¡œ ë¹Œë“œ)
    llm = load_llm(OLLAMA_MODEL_NAME)
    graph = build_graph()

    config = RunnableConfig(configurable={"llm": llm}, callbacks=[])

    inputs = {
        "input": "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ 3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ ì„¤ëª…í•´ì¤˜.",
        "context": "ì¸ê³µì§€ëŠ¥ì€ ê³„ì† ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë¯¸ë˜ì—ëŠ” ë” ë§ì€ ì˜ì—­ì—ì„œ í™œìš©ë  ê²ƒì…ë‹ˆë‹¤.",
    }

    chunk_sizes = []
    total_events = 0
    full_response = ""

    print("--- [Streaming Monitoring] ---")

    # graph.astream_eventsë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¤ìŠ¤í…€ ì´ë²¤íŠ¸ ìº¡ì²˜
    async for event in graph.astream_events(inputs, config=config, version="v2"):
        kind = event.get("event")

        # ìš°ë¦¬ê°€ ì •ì˜í•œ 'response_chunk' ì´ë²¤íŠ¸ í•„í„°ë§
        if kind == "on_custom_event" and event.get("name") == "response_chunk":
            chunk_data = event.get("data", {})
            chunk_text = chunk_data.get("chunk", "")

            if chunk_text:
                total_events += 1
                chunk_sizes.append(len(chunk_text))
                full_response += chunk_text
                # ì‹œê°ì ìœ¼ë¡œ ì²­í¬ ë‹¨ìœ„ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ êµ¬ë¶„ì í‘œì‹œ
                print(f"[{len(chunk_text)}]", end="", flush=True)

    print("\n\n--- [Test Result] ---")
    if total_events > 0:
        avg_size = sum(chunk_sizes) / total_events
        print(f"ğŸ“Š ì´ ì´ë²¤íŠ¸ íšŸìˆ˜: {total_events}")
        print(f"ğŸ“Š í‰ê·  ì²­í¬ í¬ê¸°: {avg_size:.1f} ì")
        print(f"ğŸ“Š ì²­í¬ í¬ê¸° ë¶„í¬: {chunk_sizes[:15]} ...")

        # ê²€ì¦: ë²„í¼ë§ì´ ì‘ë™í•œë‹¤ë©´ ëŒ€ë¶€ë¶„ì˜ ì²­í¬ í¬ê¸°ê°€ 1ë³´ë‹¤ ì»¤ì•¼ í•¨
        one_char_chunks = [s for s in chunk_sizes if s <= 1]
        one_char_ratio = (len(one_char_chunks) / total_events) * 100

        if avg_size > 3:
            print(
                f"âœ… ê²°ê³¼: ì„±ê³µ! (í‰ê·  ì²­í¬ í¬ê¸°ê°€ {avg_size:.1f}ìë¡œ ë²„í¼ë§ì´ ì˜ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.)"
            )
        else:
            print(
                f"âš ï¸ ê²°ê³¼: ì£¼ì˜ (í‰ê·  ì²­í¬ í¬ê¸°ê°€ {avg_size:.1f}ìë¡œ ë‚®ìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ í† í° ìƒì„± ë‹¨ìœ„ê°€ í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
            )
    else:
        print("âŒ ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì‹ í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    asyncio.run(test_streaming_buffer())
