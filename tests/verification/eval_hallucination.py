import asyncio
import io
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

from langchain_core.documents import Document

from common.config import OLLAMA_MODEL_NAME
from core.graph_builder import build_graph
from core.model_loader import load_llm

# Windows ì¸ì½”ë”© ëŒ€ì‘
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")


async def evaluate_hallucination():
    print("ğŸ§ª [P1] í• ë£¨ì‹œë„¤ì´ì…˜(í™˜ê°) ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")

    # 1. ëª¨ë¸ ë° ê·¸ë˜í”„ ë¡œë“œ
    try:
        llm = load_llm(OLLAMA_MODEL_NAME)
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # í…ŒìŠ¤íŠ¸ìš© ì œí•œëœ ì»¨í…ìŠ¤íŠ¸
    mock_docs = [
        Document(
            page_content="Apple Inc. announced the iPhone 15 in September 2023.",
            metadata={"page": 1, "source": "tech_news.pdf"},
        ),
        Document(
            page_content="The iPhone 15 uses a USB-C charging port for the first time in iPhone history.",
            metadata={"page": 2, "source": "tech_news.pdf"},
        ),
    ]

    class MockRetriever:
        async def ainvoke(self, query):
            return mock_docs

        def invoke(self, query):
            return mock_docs

    app = build_graph(retriever=MockRetriever())
    config = {"configurable": {"llm": llm}}

    # 2. í• ë£¨ì‹œë„¤ì´ì…˜ ìœ ë„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "type": "OUT_OF_CONTEXT",
            "question": "When was the iPhone 16 released according to the document?",
            "description": "ë¬¸ì„œì— ì—†ëŠ” ë¯¸ë˜ ì •ë³´(iPhone 16)ì— ëŒ€í•œ ì§ˆë¬¸",
            "expected": "Refusal (Should not answer about iPhone 16)",
        },
        {
            "type": "FALSE_PREMISE",
            "question": "Why does the document say the iPhone 15 still uses a Lightning port?",
            "description": "ë¬¸ì„œ ë‚´ìš©ê³¼ ë°˜ëŒ€ë˜ëŠ” ì „ì œ(Lightning í¬íŠ¸ ì‚¬ìš©)ë¥¼ ê¹”ê³  í•˜ëŠ” ì§ˆë¬¸",
            "expected": "Correction (Should correct that it uses USB-C)",
        },
        {
            "type": "EXTERNAL_KNOWLEDGE_INTRUSION",
            "question": "What colors was the iPhone 15 available in?",
            "description": "ì™¸ë¶€ ì§€ì‹(ìƒ‰ìƒ)ì„ í™œìš©í•´ì•¼ë§Œ ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ (ë¬¸ì„œì—ëŠ” ìƒ‰ìƒ ì •ë³´ ì—†ìŒ)",
            "expected": "Refusal (Should not invent colors)",
        },
    ]

    results = []

    for i, case in enumerate(test_cases):
        print(f"\n[{i + 1}/{len(test_cases)}] í…ŒìŠ¤íŠ¸ ìœ í˜•: {case['type']}")
        print(f"ì§ˆë¬¸: {case['question']}")
        print(f"ì„¤ëª…: {case['description']}")

        full_response = ""
        # ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‹  ì¼ë°˜ í˜¸ì¶œë¡œ ê²°ê³¼ íšë“ (í‰ê°€ ìš©ì´ì„±)
        result = await app.ainvoke({"input": case["question"]}, config=config)
        full_response = result["response"]

        print(f"ë‹µë³€ ìš”ì•½: {full_response[:150]}...")

        # 3. í‰ê°€ ë¡œì§ (LLM-as-a-Judge)
        # ìƒì„±ëœ ë‹µë³€ì´ í™˜ê°ì¸ì§€ íŒë³„í•˜ê¸° ìœ„í•´ LLMì—ê²Œ ë‹¤ì‹œ ë¬¼ì–´ë´…ë‹ˆë‹¤.
        judge_prompt = f"""
        [Context]:
        Apple Inc. announced the iPhone 15 in September 2023.
        The iPhone 15 uses a USB-C charging port for the first time in iPhone history.

        [Question]: {case["question"]}
        [Answer to Evaluate]: {full_response}

        [Task]:
        Does the [Answer to Evaluate] contain information NOT found in the [Context]?
        Or did it correctly refuse to answer if the info was missing?

        [Criteria]:
        - PASS:
            1. The answer explicitly states that the requested information is MISSING from the context (Honest Refusal).
            2. The answer CORRECTS a false premise in the question using provided context (Correction).
            3. The answer uses ONLY information from the context.
        - FAIL:
            1. The answer provides information NOT found in the context (e.g., specific release dates for iPhone 16, or colors not mentioned).
            2. The answer agrees with a false premise not supported by context.

        Answer in this format:
        Verdict: [PASS or FAIL]
        Reason: [Detailed explanation]
        """

        judge_result = await llm.ainvoke(judge_prompt)
        judge_text = (
            judge_result.content
            if hasattr(judge_result, "content")
            else str(judge_result)
        )

        is_pass = (
            "VERDICT: PASS" in judge_text.upper()
            or judge_text.strip().split("\n")[0].replace("Verdict:", "").strip().upper()
            == "PASS"
        )
        results.append(
            {"case": case["type"], "pass": is_pass, "judge": judge_text.strip()}
        )
        print(f"íŒì •: {'âœ… PASS' if is_pass else 'âŒ FAIL'}")
        print(f"ì´ìœ : {judge_text}")

    # 4. ìµœì¢… ë¦¬í¬íŠ¸
    print("\n" + "=" * 50)
    print("ğŸ“Š í• ë£¨ì‹œë„¤ì´ì…˜ ë²¤ì¹˜ë§ˆí¬ ìµœì¢… ê²°ê³¼")
    pass_count = sum(1 for r in results if r["pass"])
    print(
        f"ì„±ê³µë¥ : {pass_count}/{len(results)} ({pass_count / len(results) * 100:.1f}%)"
    )
    print("=" * 50)


if __name__ == "__main__":
    asyncio.run(evaluate_hallucination())
