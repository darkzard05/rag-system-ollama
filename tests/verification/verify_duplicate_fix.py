import asyncio
import io
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

from common.config import DEFAULT_OLLAMA_MODEL
from core.graph_builder import build_graph
from core.model_loader import load_llm

# Windows ì¸ì½”ë”© ëŒ€ì‘
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")


async def verify_duplicate_fix_final():
    print("ğŸ§ª [ìµœì¢… ì¤‘ë³µ ì¶œë ¥ ê²€ì¦] í•µì‹¬ ì—”ì§„ ìˆ˜ì • í›„ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    llm = load_llm(DEFAULT_OLLAMA_MODEL)
    app = build_graph()
    config = {"configurable": {"llm": llm}}

    question = "Hello, say 'Test' only."
    full_response = ""

    # í˜„ì¬ ì•±ì˜ ìµœì‹  ë¡œì§ (on_parser_streamë§Œ ì‚¬ìš©)
    async for event in app.astream_events(
        {"input": question}, config=config, version="v2"
    ):
        kind = event["event"]
        event.get("name", "Unknown")
        data = event.get("data", {})

        if kind == "on_parser_stream":
            chunk_text = data.get("chunk")
            if chunk_text:
                full_response += chunk_text

    print(f"\nê²°ê³¼ê°’: '{full_response}'")

    # ëª¨ë¸ì— ë”°ë¼ ê³µë°±ì´ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ strip()ìœ¼ë¡œ ë¹„êµ
    if full_response.strip() == "Test":
        print("\nâœ… PASS: ì¤‘ë³µ ì¶œë ¥ì´ ì™„ì „íˆ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(
            f"\nâŒ FAIL: ê²°ê³¼ê°€ ì—¬ì „íˆ ì¤‘ë³µë˜ê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. (ê²°ê³¼: '{full_response}')"
        )


if __name__ == "__main__":
    asyncio.run(verify_duplicate_fix_final())
