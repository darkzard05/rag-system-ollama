import asyncio
import os
import sys

# Add src directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from common.config import OLLAMA_MODEL_NAME
from core.model_loader import load_llm


async def debug_pure_streaming():
    print(f"ğŸ” ëª¨ë¸ ë¡œë”©: {OLLAMA_MODEL_NAME}")
    llm = load_llm(OLLAMA_MODEL_NAME)

    question = "Hello, how are you? Please give me a long response."
    print(f"ğŸ¤” ì§ˆë¬¸: {question}")
    print("--- ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ---")

    start_time = asyncio.get_event_loop().time()
    first_token_received = False
    full_response = ""

    try:
        # 1. ëª¨ë¸ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
        async for chunk in llm.astream(question):
            if not first_token_received:
                ttft = asyncio.get_event_loop().time() - start_time
                print(f"\nğŸš€ ì²« í† í° ìˆ˜ì‹ ! (TTFT: {ttft:.2f}s)")
                first_token_received = True

            content = chunk.content if hasattr(chunk, "content") else str(chunk)
            if not content:
                print(f"\n[Empty Chunk Detection] {type(chunk)}: {chunk}")
            print(content, end="", flush=True)
            full_response += content

        print("\n\n--- ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ---")
        print(f"ğŸ“Š ìµœì¢… ë‹µë³€ ê¸¸ì´: {len(full_response)}ì")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    asyncio.run(debug_pure_streaming())
