import asyncio
import sys
import time
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

from langchain_core.runnables import RunnableConfig

from common.config import OLLAMA_MODEL_NAME
from core.graph_builder import build_graph
from core.model_loader import load_llm


async def test_prompt_caching():
    print("ğŸš€ [Test] í”„ë¡¬í”„íŠ¸ ìºì‹±(KV Cache) ìµœì í™” ê²€ì¦ ì‹œì‘")

    llm = load_llm(OLLAMA_MODEL_NAME)
    graph = build_graph()

    config = RunnableConfig(configurable={"llm": llm}, callbacks=[])

    # ë™ì¼í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
    shared_context = [
        "ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ê¸°ê³„ë¡œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ, ì‹ ê²½ë§ì„ í†µí•´ ë°ì´í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.",
        "RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì˜ ì•½ìë¡œ, ì™¸ë¶€ ë°ì´í„°ë¥¼ í†µí•´ ë‹µë³€ì˜ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.",
    ]

    async def run_query(query, label):
        print(f"\n--- [{label}] ì‹¤í–‰: '{query}' ---")
        inputs = {
            "input": query,
            "documents": [
                # ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœì„œê°€ ë°”ë€ ìƒí™©ì„ ê°€ì •í•˜ì—¬ ì „ë‹¬í•´ë„ ì •ë ¬ ë¡œì§ì´ í•´ê²°í•´ì•¼ í•¨
                {
                    "page_content": shared_context[2],
                    "metadata": {"source": "doc.pdf", "page": 3, "chunk_index": 2},
                },
                {
                    "page_content": shared_context[0],
                    "metadata": {"source": "doc.pdf", "page": 1, "chunk_index": 0},
                },
                {
                    "page_content": shared_context[1],
                    "metadata": {"source": "doc.pdf", "page": 2, "chunk_index": 1},
                },
            ],
        }

        start_time = time.time()
        ttft = 0

        # ì‹¤ì œ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì—ì„œ ì²« í† í° ì‹œê°„ ì¸¡ì •
        async for event in graph.astream_events(inputs, config=config, version="v2"):
            if (
                event.get("event") == "on_custom_event"
                and event.get("name") == "response_chunk"
            ):
                if ttft == 0:
                    ttft = time.time() - start_time
                    print(f"âš¡ ì²« í† í° ë„ë‹¬ ì‹œê°„ (TTFT): {ttft:.2f}s")

        total_time = time.time() - start_time
        return ttft, total_time

    # 1íšŒì°¨: ìºì‹œ ë¯¸ìŠ¤ (ìµœì´ˆ ë¡œë”©)
    ttft1, total1 = await run_query("ì¸ê³µì§€ëŠ¥ì´ ë­ì•¼?", "1íšŒì°¨ (Cold Start)")

    # 2íšŒì°¨: ìºì‹œ íˆíŠ¸ (ë¬¸ì„œ ìˆœì„œê°€ ë°”ë€Œì–´ì„œ ë“¤ì–´ì™€ë„ ì •ë ¬ë˜ì–´ì•¼ í•¨)
    ttft2, total2 = await run_query("RAGì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.", "2íšŒì°¨ (Warm Start)")

    print("\n" + "=" * 40)
    print("ğŸ“ˆ í”„ë¡¬í”„íŠ¸ ìºì‹± ê²°ê³¼ ë¹„êµ")
    print(f"  - 1íšŒì°¨ TTFT: {ttft1:.2f}s")
    print(f"  - 2íšŒì°¨ TTFT: {ttft2:.2f}s")

    if ttft2 < ttft1:
        improvement = (ttft1 - ttft2) / ttft1 * 100
        print(f"  - ê°œì„ ìœ¨: {improvement:.1f}%")
        print("âœ… ê²°ê³¼: ì„±ê³µ! ìºì‹œ í™œìš©ìœ¼ë¡œ ì¸í•´ ì‘ë‹µ ì†ë„ê°€ ëŒ€í­ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ê²°ê³¼: ì‹¤íŒ¨ ë˜ëŠ” ë¯¸ë¯¸í•¨. Ollama ì„¤ì •ì´ë‚˜ ìì› ìƒí™©ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.")
    print("=" * 40)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ Document ê°ì²´ Mocking í•„ìš” (ê¸°ì¡´ ì½”ë“œê°€ Document ê°ì²´ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ)

    # ëŸ°íƒ€ì„ì— graph_builder ë‚´ì˜ state ì…ë ¥ì„ Document ê°ì²´ë¡œ ë³€í™˜í•˜ë„ë¡ íŒ¨ì¹˜í•˜ê±°ë‚˜
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ ë‚´ì—ì„œ Document ê°ì²´ë¡œ ìƒì„±í•´ì„œ ì „ë‹¬
    asyncio.run(test_prompt_caching())
