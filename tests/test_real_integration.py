import asyncio
import sys
import io
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent / "src"))

from core.rag_core import RAGSystem
from core.model_loader import load_llm
from common.config import OLLAMA_MODEL_NAME, AVAILABLE_EMBEDDING_MODELS

# Windows ì¸ì½”ë”© ëŒ€ì‘
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")


async def test_real_app_condition_flow():
    print("ğŸ§ª [ì‹¤ì œ ì•± ì¡°ê±´ í…ŒìŠ¤íŠ¸] ì‹¤ì œ íŒŒì¼ ì¸ë±ì‹±ë¶€í„° ë‹µë³€ê¹Œì§€ ì „ ê³¼ì • ê²€ì¦")

    # 1. í…ŒìŠ¤íŠ¸ìš© ì‹¤ì œ PDF íŒŒì¼ ìƒì„± (PyMuPDF í™œìš©)
    import fitz

    pdf_path = "tests/real_test_sample.pdf"
    doc = fitz.open()
    page = doc.new_page()
    page.insert_text((50, 50), "RAG-System-Ollama is a local RAG implementation.")
    page.insert_text((50, 100), "It uses LangGraph for workflow orchestration.")
    page.insert_text(
        (50, 150), "The system supports DeepSeek-R1 and other Ollama models."
    )
    doc.save(pdf_path)
    doc.close()
    print(f"âœ… ì‹¤ì œ í…ŒìŠ¤íŠ¸ìš© PDF ìƒì„± ì™„ë£Œ: {pdf_path}")

    try:
        # 2. ì‹¤ì œ ì•±ê³¼ ë™ì¼í•œ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        # (ì„ë² ë”© ëª¨ë¸ì€ ì„¤ì •ëœ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©)
        embedding_model = AVAILABLE_EMBEDDING_MODELS[0]
        rag_system = RAGSystem(embedding_model_name=embedding_model)

        # 3. ì‹¤ì œ ì¸ë±ì‹± (ì´ ê³¼ì •ì—ì„œ ì²­í‚¹, ì„ë² ë”©, ë²¡í„° ì €ì¥ì´ ì‹¤ì œë¡œ ì¼ì–´ë‚¨)
        print("âš™ï¸ ì‹¤ì œ ì¸ë±ì‹± ì‹œì‘ (ì²­í‚¹/ì„ë² ë”©)...")
        with open(pdf_path, "rb") as f:
            pdf_bytes = f.read()
            # ì‹¤ì œ ì•±ì˜ íŒŒì¼ ì²˜ë¦¬ ë¡œì§ê³¼ ë™ì¼
            await rag_system.build_index(pdf_bytes, pdf_path)
        print("âœ… ì‹¤ì œ ì¸ë±ì‹± ë° ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ")

        # 4. ì‹¤ì œ ì•±ê³¼ ë™ì¼í•œ QA Chain íšë“
        llm = load_llm(OLLAMA_MODEL_NAME)
        qa_chain = rag_system.get_qa_chain()
        run_config = {"configurable": {"llm": llm}}

        # 5. ì‹¤ì œ ì§ˆë¬¸ ë˜ì§€ê¸°
        question = "ì´ ì‹œìŠ¤í…œì´ ë¬´ì—‡ì„ ì‚¬ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€ë¦¬í•˜ë‚˜ìš”?"
        print(f"ì§ˆë¬¸: {question}")

        full_response = ""
        async for event in qa_chain.astream_events(
            {"input": question}, config=run_config, version="v2"
        ):
            if event["event"] == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    full_response += content

        print("\n" + "=" * 50)
        print("ì‹¤ì œ ì•± ì—”ì§„ ë‹µë³€:")
        print(full_response)
        print("=" * 50)

        # 6. ê²€ì¦
        has_langgraph = "LangGraph" in full_response
        has_citation = "[p.1]" in full_response

        print("\nê²€ì¦ ê²°ê³¼:")
        print(
            f" - LangGraph í¬í•¨ (ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€): {'âœ… PASS' if has_langgraph else 'âŒ FAIL'}"
        )
        print(f" - ì¸ìš©êµ¬ í¬í•¨ ([p.1]): {'âœ… PASS' if has_citation else 'âŒ FAIL'}")

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
    finally:
        if os.path.exists(pdf_path):
            os.remove(pdf_path)


if __name__ == "__main__":
    asyncio.run(test_real_app_condition_flow())
