import os
import torch
# Streamlitê³¼ Torch ê°„ì˜ ì¶©ëŒ í•´ê²° íŒ¨ì¹˜
torch.classes.__path__ = []

import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

st.title("ğŸ“„ RAG System with Ollama LLM")

uploaded_file = st.file_uploader("Upload your PDF file here", type="pdf")

if uploaded_file:
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.getvalue())
    if os.path.exists("temp.pdf"):
        st.write("âœ… PDF íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        st.write("âŒ PDF íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    loader = PyPDFLoader("temp.pdf")
    docs = loader.load()
    
    # ë¡œë“œëœ ë¬¸ì„œ ìˆ˜ì™€ ì²« í˜ì´ì§€ ë‚´ìš© ì¶œë ¥
    st.write(f"ğŸ“„ ë¡œë“œëœ ë¬¸ì„œ(í˜ì´ì§€) ìˆ˜: {len(docs)}")
    if docs:  # ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆë‹¤ë©´
        st.write("ğŸ“„ PDFì—ì„œ ë¬¸ì„œë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
    else:
        st.write("âŒ PDFì—ì„œ ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    embedder = HuggingFaceEmbeddings(model_name="intfloat/e5-base-v2")
    text_splitter = SemanticChunker(embedder)
    documents = text_splitter.split_documents(docs)
    st.write(f"ğŸ“‘ ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(documents)}")

    vector = FAISS.from_documents(documents, embedder)
    st.write("ğŸ—„ï¸ FAISS ë²¡í„° ì €ì¥ì†Œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    retriever = vector.as_retriever(search_type="similarity", search_kwargs={"k": 3})
    st.write("ğŸ” ê²€ìƒ‰ê¸°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    llm = OllamaLLM(model="deepseek-r1:14b")
    st.write("ğŸ¤– LLMì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    prompt = """
    Use the following context to answer the question.
    Context: {context}
    Question: {question}
    Answer:"""
    QA_PROMPT = PromptTemplate.from_template(prompt)

    combine_documents_chain = create_stuff_documents_chain(llm, QA_PROMPT)
    st.write("ğŸ”— ë¬¸ì„œ ê²°í•© ì²´ì¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    qa = create_retrieval_chain(retriever, combine_documents_chain)
    st.write("ğŸ”— ê²€ìƒ‰ ì²´ì¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    user_input = st.text_input("Ask a question about your document:")

    if user_input:
        response = qa.invoke({"input": user_input, "question": user_input})
        
        st.write("**Response:**")
        st.write(response["answer"])